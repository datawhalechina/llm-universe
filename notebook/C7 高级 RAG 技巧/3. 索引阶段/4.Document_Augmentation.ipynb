{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、理论介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Augmentation技术利用额外的问题生成来改进矢量数据库中的文档检索。通过生成和合并与每个文本片段相关的各种问题，该系统增强了标准检索过程，从而提高了找到可用作生成式问答上下文的相关文档的可能性。\n",
    "\n",
    "通过使用相关问题丰富文本片段，我们的目标是显著提高识别文档中包含用户查询答案的最相关部分的准确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现步骤\n",
    "1. PDF 处理和文本分块：处理 PDF 文档并将其划分为可管理的文本片段。\n",
    "2. 问题增强：使用 OpenAI 的语言模型在文档和片段级别生成相关问题。\n",
    "3. 矢量存储创建：使用嵌入模型计算文档的嵌入并创建向量存储。\n",
    "4. 检索和答案生成：查找最相关的文档，并根据提供的上下文生成答案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法优点\n",
    "1. 增强检索过程：增加为给定查询找到最相关的文档的概率。\n",
    "2. 灵活的上下文调整：允许轻松调整文本文档和片段的上下文窗口大小。\n",
    "3. 高质量语言理解：利用 OpenAI 强大的语言模型进行问题生成和答案生成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 一个简单例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from enum import Enum\n",
    "from langchain.schema import Document\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "class QuestionGeneration(Enum):\n",
    "    \"\"\"\n",
    "    枚举类，用于指定文档处理中的问题生成级别。\n",
    "\n",
    "    属性：\n",
    "        DOCUMENT_LEVEL (int): 表示在整个文档级别生成问题。\n",
    "        FRAGMENT_LEVEL (int): 表示在单个文本片段级别生成问题。\n",
    "    \"\"\"\n",
    "    DOCUMENT_LEVEL = 1\n",
    "    FRAGMENT_LEVEL = 2\n",
    "\n",
    "# 根据模型的不同，Mitral 7B 的最大值为 8000，Llama 3.1 8B 为 128k\n",
    "DOCUMENT_MAX_TOKENS = 2000\n",
    "DOCUMENT_OVERLAP_TOKENS = 400\n",
    "\n",
    "# 嵌入和文本相似度计算基于较短的文本\n",
    "FRAGMENT_MAX_TOKENS = 128\n",
    "FRAGMENT_OVERLAP_TOKENS = 16\n",
    "\n",
    "# 在文档或片段级别生成的问题\n",
    "QUESTION_GENERATION = QuestionGeneration.DOCUMENT_LEVEL\n",
    "# 针对特定文档或片段将生成的问题数量\n",
    "QUESTIONS_PER_DOCUMENT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionList(BaseModel):\n",
    "    question_list: List[str] = Field(..., title=\"为文档或片段生成的问题列表\")\n",
    "\n",
    "\n",
    "class OpenAIEmbeddingsWrapper(OpenAIEmbeddings):\n",
    "    \"\"\"\n",
    "    OpenAI嵌入的包装类，提供与原始OllamaEmbeddings类似的接口。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, query: str) -> List[float]:\n",
    "        \"\"\"\n",
    "        允许实例作为可调用对象生成查询的嵌入。\n",
    "\n",
    "        参数：\n",
    "            query (str): 要嵌入的查询字符串。\n",
    "\n",
    "        返回：\n",
    "            List[float]: 查询的嵌入，作为浮点数列表。\n",
    "        \"\"\"\n",
    "        return self.embed_query(query)\n",
    "\n",
    "def clean_and_filter_questions(questions: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    清理和过滤问题列表。\n",
    "\n",
    "    参数：\n",
    "        questions (List[str]): 要清理和过滤的问题列表。\n",
    "\n",
    "    返回：\n",
    "        List[str]: 一份清理和过滤后的、以问号结尾的问题列表。\n",
    "    \"\"\"\n",
    "    cleaned_questions = []\n",
    "    for question in questions:\n",
    "        cleaned_question = re.sub(r'^\\d+\\.\\s*', '', question.strip())\n",
    "        if cleaned_question.endswith('？'):\n",
    "            cleaned_questions.append(cleaned_question)\n",
    "    return cleaned_questions\n",
    "\n",
    "def generate_questions(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    基于提供的文本生成问题列表，使用OpenAI。\n",
    "\n",
    "    参数：\n",
    "        text (str): 用于生成问题的上下文数据。\n",
    "\n",
    "    返回：\n",
    "        List[str]: 一份独特的、过滤后的问题列表。\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"num_questions\"],\n",
    "        template=\"使用上下文数据: {context}\\n\\n生成至少 {num_questions} 个可能的问题，这些问题可以基于该上下文进行提问。确保问题在上下文中是可以直接回答的，并且不包含任何答案或标题。\"\n",
    "                 \"用换行符分隔问题。\"\n",
    "    )\n",
    "    chain = prompt | llm.with_structured_output(QuestionList)\n",
    "    input_data = {\"context\": text, \"num_questions\": QUESTIONS_PER_DOCUMENT}\n",
    "    result = chain.invoke(input_data)\n",
    "    \n",
    "    # 从QuestionList对象中提取问题列表\n",
    "    questions = result.question_list\n",
    "    \n",
    "    filtered_questions = clean_and_filter_questions(questions)\n",
    "    return list(set(filtered_questions))\n",
    "\n",
    "def generate_answer(content: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    基于提供的上下文生成给定问题的答案，使用OpenAI。\n",
    "\n",
    "    参数：\n",
    "        content (str): 用于生成答案的上下文数据。\n",
    "        question (str): 要生成答案的问题。\n",
    "\n",
    "    返回：\n",
    "        str: 基于提供的上下文的精确答案。\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=\"使用上下文数据: {context}\\n\\n提供对问题: {question} 的简短而精确的答案。\"\n",
    "    )\n",
    "    chain = prompt | llm\n",
    "    input_data = {\"context\": content, \"question\": question}\n",
    "    return chain.invoke(input_data)\n",
    "\n",
    "def split_document(document: str, chunk_size: int, chunk_overlap: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    将文档拆分为较小的文本块。\n",
    "\n",
    "    参数：\n",
    "        document (str): 要拆分的文档文本。\n",
    "        chunk_size (int): 每个块的大小（以标记数量为单位）。\n",
    "        chunk_overlap (int): 连续块之间的重叠标记数量。\n",
    "\n",
    "    返回：\n",
    "        List[str]: 文本块的列表，每个块都是文档内容的字符串。\n",
    "    \"\"\"\n",
    "    tokens = re.findall(r'\\b\\w+\\b', document)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - chunk_overlap):\n",
    "        chunk_tokens = tokens[i:i + chunk_size]\n",
    "        chunks.append(chunk_tokens)\n",
    "        if i + chunk_size >= len(tokens):\n",
    "            break\n",
    "    return [\" \".join(chunk) for chunk in chunks]\n",
    "\n",
    "def print_document(comment: str, document: str) -> None:\n",
    "    \"\"\"\n",
    "    打印评论，后跟文档的内容。\n",
    "\n",
    "    参数：\n",
    "        comment (str): 在文档详细信息之前打印的评论或描述。\n",
    "        document (str): 要打印其内容的文档。\n",
    "\n",
    "    返回：\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(f'{comment} (类型: {document.metadata[\"type\"]}, 索引: {document.metadata[\"index\"]}): {document.page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的问题:\n",
      "- 周志华老师的《机器学习》被称为什么？\n",
      "- 《机器学习》被认为是经典的什么类型的教材？\n",
      "- 西瓜书的全名是什么？\n",
      "- 《机器学习》是哪个领域的教材？\n",
      "- 《机器学习》是否适合想要深入研究机器学习的读者？\n",
      "- 周志华老师在书中对公式推导的细节是如何处理的？\n",
      "- 《机器学习》这本书的作者是谁？\n",
      "- 《机器学习》这本书的出版目的是什么？\n",
      "- 为什么周志华老师没有详述部分公式的推导细节？\n",
      "- 《机器学习》这本书的目标读者是谁？\n",
      "\n",
      "问题: 周志华老师的《机器学习》被称为什么？\n",
      "答案: content='周志华老师的《机器学习》被称为“西瓜书”。' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 115, 'total_tokens': 133, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None} id='run-0328dfac-fb8a-496f-b94f-fe019e99b8f4-0' usage_metadata={'input_tokens': 115, 'output_tokens': 18, 'total_tokens': 133, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "文档片段:\n",
      "片段 1: 周志华老师的 机器学习 西瓜书 是机器学习领域的经典入门教材之一 周老师为了使尽可能多的读者通过西瓜书对机器学习有所了解 所以在书中对部分公式的推导细节没有详述 但是这对那些想深究公式推导细节的读者\n",
      "\n",
      "文档嵌入（前 5 个元素）: [0.002406382996136422, -0.0023081971469888266, 0.029315934351175528, -0.013386545121275248, 0.006230634799979914]\n",
      "查询嵌入（前 5 个元素）: [0.010264923736570593, -0.00900294987581528, 0.0047421608422593894, 0.0032151069428835444, -0.01863038139327201]\n"
     ]
    }
   ],
   "source": [
    "# 初始化 OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddingsWrapper()\n",
    "\n",
    "# 示例文档\n",
    "example_text = '“周志华老师的《机器学习》（西瓜书）是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读者通过西瓜书对机器学习有所了解， 所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推导细节的读者'\n",
    "\n",
    "# 生成问题\n",
    "questions = generate_questions(example_text)\n",
    "print(\"生成的问题:\")\n",
    "for q in questions:\n",
    "    print(f\"- {q}\")\n",
    "\n",
    "# 生成答案\n",
    "sample_question = questions[0] if questions else \"这个文档是关于什么的？\"\n",
    "answer = generate_answer(example_text, sample_question)\n",
    "print(f\"\\n问题: {sample_question}\")\n",
    "print(f\"答案: {answer}\")\n",
    "\n",
    "# 拆分文档\n",
    "chunks = split_document(example_text, chunk_size=10, chunk_overlap=2)\n",
    "print(\"\\n文档片段:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"片段 {i + 1}: {chunk}\")\n",
    "\n",
    "# 使用 OpenAIEmbeddings 的示例\n",
    "doc_embedding = embeddings.embed_documents([example_text])\n",
    "query_embedding = embeddings.embed_query(\"主要主题是什么？\")\n",
    "print(\"\\n文档嵌入（前 5 个元素）:\", doc_embedding[0][:5])\n",
    "print(\"查询嵌入（前 5 个元素）:\", query_embedding[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 具体实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str):\n",
    "    \"\"\"\n",
    "    Implement text cleaning function\n",
    "\n",
    "    Args:\n",
    "        text: Field requiring cleaning\n",
    "\n",
    "    Returns:\n",
    "        Field returned after cleaning is completed\n",
    "    \n",
    "    \"\"\"\n",
    "    # 删除每页开头与结尾标语及链接\n",
    "    text = re.sub(r'→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←', '', text)\n",
    "    text = re.sub(r'→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', '', text)\n",
    "    # 删除字符串开头的空格\n",
    "    text = re.sub(r'\\s+', '', text)\n",
    "    # 删除回车\n",
    "    text = re.sub(r'\\n+', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def DA_retriever(content: str, embedding_model: OpenAIEmbeddings):\n",
    "    \"\"\"\n",
    "    处理文档内容，将其拆分为片段，生成问题，\n",
    "    创建向量存储，并返回检索器。\n",
    "\n",
    "    参数：\n",
    "        content (str): 要处理的文档内容。\n",
    "        embedding_model (OpenAIEmbeddings): 用于向量化的嵌入模型。\n",
    "\n",
    "    返回：\n",
    "        VectorStoreRetriever: 用于检索最相关的文档的检索器。\n",
    "    \"\"\"\n",
    "    # 将整个文本内容拆分为文本文档\n",
    "    text_documents = split_document(content, DOCUMENT_MAX_TOKENS, DOCUMENT_OVERLAP_TOKENS)\n",
    "    print(f'文本内容拆分为: {len(text_documents)} 个文档')\n",
    "\n",
    "    documents = []\n",
    "    counter = 0\n",
    "    for i, text_document in enumerate(text_documents):\n",
    "        text_fragments = split_document(text_document, FRAGMENT_MAX_TOKENS, FRAGMENT_OVERLAP_TOKENS)\n",
    "        print(f'文本文档 {i} - 拆分为: {len(text_fragments)} 个片段')\n",
    "        \n",
    "        for j, text_fragment in enumerate(text_fragments):\n",
    "            documents.append(Document(\n",
    "                page_content=text_fragment,\n",
    "                metadata={\"type\": \"ORIGINAL\", \"index\": counter, \"text\": text_document}\n",
    "            ))\n",
    "            counter += 1\n",
    "            \n",
    "            if QUESTION_GENERATION == QuestionGeneration.FRAGMENT_LEVEL:\n",
    "                questions = generate_questions(text_fragment)\n",
    "                documents.extend([\n",
    "                    Document(page_content=question, metadata={\"type\": \"AUGMENTED\", \"index\": counter + idx, \"text\": text_document})\n",
    "                    for idx, question in enumerate(questions)\n",
    "                ])\n",
    "                counter += len(questions)\n",
    "                print(f'文本文档 {i} 文本片段 {j} - 生成: {len(questions)} 个问题')\n",
    "        \n",
    "        if QUESTION_GENERATION == QuestionGeneration.DOCUMENT_LEVEL:\n",
    "            questions = generate_questions(text_document)\n",
    "            documents.extend([\n",
    "                Document(page_content=question, metadata={\"type\": \"AUGMENTED\", \"index\": counter + idx, \"text\": text_document})\n",
    "                for idx, question in enumerate(questions)\n",
    "            ])\n",
    "            counter += len(questions)\n",
    "            print(f'文本文档 {i} - 生成: {len(questions)} 个问题')\n",
    "\n",
    "    for document in documents:\n",
    "        print_document(\"数据集\", document)\n",
    "\n",
    "    print(f'创建存储，计算 {len(documents)} 个 文档的嵌入')\n",
    "    vectorstore = Chroma.from_documents(documents, embedding_model)\n",
    "\n",
    "    print(\"创建检索器以返回最相关的 Chroma 文档\")\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"data/pumpkin_book.pdf\"\n",
    "qa_path = 'data/train_dataset.json'\n",
    "embedding = HuggingFaceEmbeddings(model_name='BAAI/bge-small-zh-v1.5')\n",
    "\n",
    "with open(qa_path, 'r', encoding='utf-8') as file:\n",
    "    qa_pairs = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个 PyMuPDFLoader Class 实例，输入为待加载的 pdf 文档路径，加载PDF\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "\n",
    "# 调用 PyMuPDFLoader Class 的函数 load 对 pdf 文件进行加载\n",
    "pdf_pages = loader.load()\n",
    "\n",
    "# 第13页为南瓜书第一页正文，因此从13页开始,从倒数13页涉及敏感用语，因此从-13页结束\n",
    "data_pages = pdf_pages[13:-13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了减小token消耗，这里只处理page_num = 13的PDF数据，同时基于已经处理好的qa_pairs中的第一个进行试验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '请根据提供的上下文信息，解释“算法”和“模型”的概念，并说明它们在机器学习中的关系。',\n",
       " 'answer': '“算法”是指从数据中学得“模型”的具体方法，例如后续章节中将会讲述的线性回归、对数几率回归、决策树等。“算法”产出的结果称为“模型”，通常是具体的函数或者可抽象地看作为函数，例如一元线性回归算法产出的模型即为形如f(x)=wx+b的一元一次函数。不过由于严格区分这两者的意义不大，因此多数文献和资料会将其混用，当遇到这两个概念时，其具体指代根据上下文判断即可。',\n",
       " 'page_num': 13}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'第1章绪论本章作为“西瓜书”的开篇，主要讲解什么是机器学习以及机器学习的相关数学符号，为后续内容作铺垫，并未涉及复杂的算法理论，因此阅读本章时只需耐心梳理清楚所有概念和数学符号即可。此外，在阅读本章前建议先阅读西瓜书目录前页的《主要符号表》，它能解答在阅读“西瓜书”过程中产生的大部分对数学符号的疑惑。本章也作为本书的开篇，笔者在此赘述一下本书的撰写初衷，本书旨在以“过来人”的视角陪读者一起阅读“西瓜书”，尽力帮读者消除阅读过程中的“数学恐惧”，只要读者学习过《高等数学》、《线性代数》和《概率论与数理统计》这三门大学必修的数学课，均能看懂本书对西瓜书中的公式所做的解释和推导，同时也能体会到这三门数学课在机器学习上碰撞产生的“数学之美”。1.1引言本节以概念理解为主，在此对“算法”和“模型”作补充说明。“算法”是指从数据中学得“模型”的具体方法，例如后续章节中将会讲述的线性回归、对数几率回归、决策树等。“算法”产出的结果称为“模型”，通常是具体的函数或者可抽象地看作为函数，例如一元线性回归算法产出的模型即为形如f(x)=wx+b的一元一次函数。不过由于严格区分这两者的意义不大，因此多数文献和资料会将其混用，当遇到这两个概念时，其具体指代根据上下文判断即可。1.2基本术语本节涉及的术语较多且很多术语都有多个称呼，下面梳理各个术语，并将最常用的称呼加粗标注。样本：也称为“示例”，是关于一个事件或对象的描述。因为要想让计算机能对现实生活中的事物进行机器学习，必须先将其抽象为计算机能理解的形式，计算机最擅长做的就是进行数学运算，因此考虑如何将其抽象为某种数学形式。显然，线性代数中的向量就很适合，因为任何事物都可以由若干“特征”（或称为“属性”）唯一刻画出来，而向量的各个维度即可用来描述各个特征。例如，如果用色泽、根蒂和敲声这3个特征来刻画西瓜，那么一个“色泽青绿，根蒂蜷缩，敲声清脆”的西瓜用向量来表示即为x=(青绿;蜷缩;清脆)（向量中的元素用分号“;”分隔时表示此向量为列向量，用逗号“,”分隔时表示为行向量），其中青绿、蜷缩和清脆分别对应为相应特征的取值，也称为“属性值”。显然，用中文书写向量的方式不够“数学”，因此需要将属性值进一步数值化，具体例子参见“西瓜书”第3章3.2。此外，仅靠以上3个特征来刻画西瓜显然不够全面细致，因此还需要扩展更多维度的特征，一般称此类与特征处理相关的工作为“特征工程”。样本空间：也称为“输入空间”或“属性空间”。由于样本采用的是标明各个特征取值的“特征向量”来进行表示，根据线性代数的知识可知，有向量便会有向量所在的空间，因此称表示样本的特征向量所在的空间为样本空间，通常用花式大写的X表示。数据集：数据集通常用集合来表示，令集合D={x1,x2,...,xm}表示包含m个样本的数据集，一般同一份数据集中的每个样本都含有相同个数的特征，假设此数据集中的每个样本都含有d个特征，则第i个样本的数学表示为d维向量：xi=(xi1;xi2;...;xid)，其中xij表示样本xi在第j个属性上的取值。模型：机器学习的一般流程如下：首先收集若干样本（假设此时有100个），然后将其分为训练样本（80个）和测试样本（20个），其中80个训练样本构成的集合称为“训练集”，20个测试样本构成的集合称为“测试集”，接着选用某个机器学习算法，让其在训练集上进行“学习”（或称为“训练”），然后产出得到“模型”（或称为“学习器”），最后用测试集来测试模型的效果。执行以上流程时，表示我们已经默认样本的背后是存在某种潜在的规律，我们称这种潜在的规律为“真相”或者“真实”，例如样本是一堆好西瓜和坏西瓜时，我们默认的便是好西瓜和坏西瓜背后必然存在某种规律能将其区分开。当我们应用某个机器学习算法来学习时，产出得到的模型便是该算法所找到的它自己认为的规律，由于该规律通常并不一定'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 只使用page_num = 13\n",
    "data_pages_sample = data_pages[0]\n",
    "\n",
    "content = clean_text(data_pages_sample.page_content)\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本内容拆分为: 1 个文档\n",
      "文本文档 0 - 拆分为: 2 个片段\n",
      "文本文档 0 - 生成: 10 个问题\n",
      "数据集 (类型: ORIGINAL, 索引: 0): 第1章绪论本章作为 西瓜书 的开篇 主要讲解什么是机器学习以及机器学习的相关数学符号 为后续内容作铺垫 并未涉及复杂的算法理论 因此阅读本章时只需耐心梳理清楚所有概念和数学符号即可 此外 在阅读本章前建议先阅读西瓜书目录前页的 主要符号表 它能解答在阅读 西瓜书 过程中产生的大部分对数学符号的疑惑 本章也作为本书的开篇 笔者在此赘述一下本书的撰写初衷 本书旨在以 过来人 的视角陪读者一起阅读 西瓜书 尽力帮读者消除阅读过程中的 数学恐惧 只要读者学习过 高等数学 线性代数 和 概率论与数理统计 这三门大学必修的数学课 均能看懂本书对西瓜书中的公式所做的解释和推导 同时也能体会到这三门数学课在机器学习上碰撞产生的 数学之美 1 1引言本节以概念理解为主 在此对 算法 和 模型 作补充说明 算法 是指从数据中学得 模型 的具体方法 例如后续章节中将会讲述的线性回归 对数几率回归 决策树等 算法 产出的结果称为 模型 通常是具体的函数或者可抽象地看作为函数 例如一元线性回归算法产出的模型即为形如f x wx b的一元一次函数 不过由于严格区分这两者的意义不大 因此多数文献和资料会将其混用 当遇到这两个概念时 其具体指代根据上下文判断即可 1 2基本术语本节涉及的术语较多且很多术语都有多个称呼 下面梳理各个术语 并将最常用的称呼加粗标注 样本 也称为 示例 是关于一个事件或对象的描述 因为要想让计算机能对现实生活中的事物进行机器学习 必须先将其抽象为计算机能理解的形式 计算机最擅长做的就是进行数学运算 因此考虑如何将其抽象为某种数学形式 显然 线性代数中的向量就很适合 因为任何事物都可以由若干 特征 或称为 属性 唯一刻画出来 而向量的各个维度即可用来描述各个特征 例如 如果用色泽 根蒂和敲声这3个特征来刻画西瓜 那么一个 色泽青绿 根蒂蜷缩 敲声清脆 的西瓜用向量来表示即为x 青绿 蜷缩 清脆 向量中的元素用分号 分隔时表示此向量为列向量 用逗号 分隔时表示为行向量 其中青绿 蜷缩和清脆分别对应为相应特征的取值 也称为 属性值 显然 用中文书写向量的方式不够 数学 因此需要将属性值进一步数值化 具体例子参见 西瓜书 第3章3 2 此外 仅靠以上3个特征来刻画西瓜显然不够全面细致 因此还需要扩展更多维度的特征 一般称此类与特征处理相关的工作为 特征工程 样本空间 也称为 输入空间 或 属性空间 由于样本采用的是标明各个特征取值的 特征向量 来进行表示 根据线性代数的知识可知 有向量便会有向量所在的空间 因此称表示样本的特征向量所在的空间为样本空间 通常用花式大写的X表示 数据集 数据集通常用集合来表示 令集合D x1 x2 xm 表示包含m个样本的数据集 一般同一份数据集中的每个样本都含有相同个数的特征\n",
      "数据集 (类型: ORIGINAL, 索引: 1): 属性空间 由于样本采用的是标明各个特征取值的 特征向量 来进行表示 根据线性代数的知识可知 有向量便会有向量所在的空间 因此称表示样本的特征向量所在的空间为样本空间 通常用花式大写的X表示 数据集 数据集通常用集合来表示 令集合D x1 x2 xm 表示包含m个样本的数据集 一般同一份数据集中的每个样本都含有相同个数的特征 假设此数据集中的每个样本都含有d个特征 则第i个样本的数学表示为d维向量 xi xi1 xi2 xid 其中xij表示样本xi在第j个属性上的取值 模型 机器学习的一般流程如下 首先收集若干样本 假设此时有100个 然后将其分为训练样本 80个 和测试样本 20个 其中80个训练样本构成的集合称为 训练集 20个测试样本构成的集合称为 测试集 接着选用某个机器学习算法 让其在训练集上进行 学习 或称为 训练 然后产出得到 模型 或称为 学习器 最后用测试集来测试模型的效果 执行以上流程时 表示我们已经默认样本的背后是存在某种潜在的规律 我们称这种潜在的规律为 真相 或者 真实 例如样本是一堆好西瓜和坏西瓜时 我们默认的便是好西瓜和坏西瓜背后必然存在某种规律能将其区分开 当我们应用某个机器学习算法来学习时 产出得到的模型便是该算法所找到的它自己认为的规律 由于该规律通常并不一定\n",
      "数据集 (类型: AUGMENTED, 索引: 2): 训练集和测试集的区别是什么？\n",
      "数据集 (类型: AUGMENTED, 索引: 3): 数据集通常是如何表示的？\n",
      "数据集 (类型: AUGMENTED, 索引: 4): 本章主要讲解了哪些数学符号？\n",
      "数据集 (类型: AUGMENTED, 索引: 5): 算法和模型之间有什么区别？\n",
      "数据集 (类型: AUGMENTED, 索引: 6): 机器学习的基本流程是什么？\n",
      "数据集 (类型: AUGMENTED, 索引: 7): 样本空间的另一个称呼是什么？\n",
      "数据集 (类型: AUGMENTED, 索引: 8): 阅读本章前需要先了解什么内容？\n",
      "数据集 (类型: AUGMENTED, 索引: 9): 机器学习的定义是什么？\n",
      "数据集 (类型: AUGMENTED, 索引: 10): 样本在机器学习中是如何定义的？\n",
      "数据集 (类型: AUGMENTED, 索引: 11): 特征工程的目的是什么？\n",
      "创建存储，计算 12 个 文档的嵌入\n",
      "创建检索器以返回最相关的 Chroma 文档\n"
     ]
    }
   ],
   "source": [
    "# 处理文档并创建检索器\n",
    "document_query_retriever = DA_retriever(content, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 请根据提供的上下文信息，解释“算法”和“模型”的概念，并说明它们在机器学习中的关系。\n",
      "Retrieved document: 算法和模型之间的区别是什么？\n",
      "Retrieved document original text: 第1章绪论本章作为 西瓜书 的开篇 主要讲解什么是机器学习以及机器学习的相关数学符号 为后续内容作铺垫 并未涉及复杂的算法理论 因此阅读本章时只需耐心梳理清楚所有概念和数学符号即可 此外 在阅读本章前建议先阅读西瓜书目录前页的 主要符号表 它能解答在阅读 西瓜书 过程中产生的大部分对数学符号的疑惑 本章也作为本书的开篇 笔者在此赘述一下本书的撰写初衷 本书旨在以 过来人 的视角陪读者一起阅读 西瓜书 尽力帮读者消除阅读过程中的 数学恐惧 只要读者学习过 高等数学 线性代数 和 概率论与数理统计 这三门大学必修的数学课 均能看懂本书对西瓜书中的公式所做的解释和推导 同时也能体会到这三门数学课在机器学习上碰撞产生的 数学之美 1 1引言本节以概念理解为主 在此对 算法 和 模型 作补充说明 算法 是指从数据中学得 模型 的具体方法 例如后续章节中将会讲述的线性回归 对数几率回归 决策树等 算法 产出的结果称为 模型 通常是具体的函数或者可抽象地看作为函数 例如一元线性回归算法产出的模型即为形如f x wx b的一元一次函数 不过由于严格区分这两者的意义不大 因此多数文献和资料会将其混用 当遇到这两个概念时 其具体指代根据上下文判断即可 1 2基本术语本节涉及的术语较多且很多术语都有多个称呼 下面梳理各个术语 并将最常用的称呼加粗标注 样本 也称为 示例 是关于一个事件或对象的描述 因为要想让计算机能对现实生活中的事物进行机器学习 必须先将其抽象为计算机能理解的形式 计算机最擅长做的就是进行数学运算 因此考虑如何将其抽象为某种数学形式 显然 线性代数中的向量就很适合 因为任何事物都可以由若干 特征 或称为 属性 唯一刻画出来 而向量的各个维度即可用来描述各个特征 例如 如果用色泽 根蒂和敲声这3个特征来刻画西瓜 那么一个 色泽青绿 根蒂蜷缩 敲声清脆 的西瓜用向量来表示即为x 青绿 蜷缩 清脆 向量中的元素用分号 分隔时表示此向量为列向量 用逗号 分隔时表示为行向量 其中青绿 蜷缩和清脆分别对应为相应特征的取值 也称为 属性值 显然 用中文书写向量的方式不够 数学 因此需要将属性值进一步数值化 具体例子参见 西瓜书 第3章3 2 此外 仅靠以上3个特征来刻画西瓜显然不够全面细致 因此还需要扩展更多维度的特征 一般称此类与特征处理相关的工作为 特征工程 样本空间 也称为 输入空间 或 属性空间 由于样本采用的是标明各个特征取值的 特征向量 来进行表示 根据线性代数的知识可知 有向量便会有向量所在的空间 因此称表示样本的特征向量所在的空间为样本空间 通常用花式大写的X表示 数据集 数据集通常用集合来表示 令集合D x1 x2 xm 表示包含m个样本的数据集 一般同一份数据集中的每个样本都含有相同个数的特征 假设此数据集中的每个样本都含有d个特征 则第i个样本的数学表示为d维向量 xi xi1 xi2 xid 其中xij表示样本xi在第j个属性上的取值 模型 机器学习的一般流程如下 首先收集若干样本 假设此时有100个 然后将其分为训练样本 80个 和测试样本 20个 其中80个训练样本构成的集合称为 训练集 20个测试样本构成的集合称为 测试集 接着选用某个机器学习算法 让其在训练集上进行 学习 或称为 训练 然后产出得到 模型 或称为 学习器 最后用测试集来测试模型的效果 执行以上流程时 表示我们已经默认样本的背后是存在某种潜在的规律 我们称这种潜在的规律为 真相 或者 真实 例如样本是一堆好西瓜和坏西瓜时 我们默认的便是好西瓜和坏西瓜背后必然存在某种规律能将其区分开 当我们应用某个机器学习算法来学习时 产出得到的模型便是该算法所找到的它自己认为的规律 由于该规律通常并不一定\n"
     ]
    }
   ],
   "source": [
    "query = qa_pairs[0]['query']\n",
    "retrieved_docs = document_query_retriever.get_relevant_documents(query)\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(f\"Retrieved document: {retrieved_docs[0].page_content}\")\n",
    "print(f\"Retrieved document original text: {retrieved_docs[0].metadata['text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
