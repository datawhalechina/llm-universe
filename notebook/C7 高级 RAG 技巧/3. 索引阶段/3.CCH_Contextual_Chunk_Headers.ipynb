{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Chunk Headers (CCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、理论介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上下文块头 （CCH： Contextual Chunk Headers） 是一种创建包含更高级别上下文（例如文档级或章节级上下文）的块头的方法，并在嵌入这些块头之前将这些块头附加到块中。这为嵌入提供了文本内容和含义的更准确和完整的表示。在我们的测试中，此功能可显著提高检索质量。除了提高检索正确信息的速度外，CCH 还降低了不相关结果在搜索结果中的显示速度。这降低了 LLM 在下游聊天和生成应用程序中误解一段文本的速率。参考论文：https://arxiv.org/abs/2409.04701"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开发人员在使用 RAG 时面临的许多问题都归结为：单个块通常不包含足够的上下文，无法被检索系统或 LLM 正确使用。这导致无法回答问题，更令人担忧的是，还会出现幻觉。具体场景有\n",
    "- Chunk 通常通过隐含的引用和代词来指代其主题。这会导致它们在应该检索的时候没有被检索，或者 LLM 无法正确理解它们。\n",
    "- 单个块通常只在整个部分或文档的上下文中才有意义，并且单独阅读时可能会产生误导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现步骤\n",
    "1. 上下文生成：使用 LLM 为文档生成描述性标题。具体实现为：利用LLM完成简单的prompt模版，对每一个chunk生成描述性标题。如果有足够描述性的文档标题，则可以直接使用这些标题。例如：简明的文档摘要、章节/子章节标题。\n",
    "2. 将生成chunk header 嵌入 chunk\n",
    "3. 在结果中返回chunk header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现逻辑如下所示：\n",
    "- 标准方式：直接对于每个分块embedding后放入向量数据库\n",
    "- CCH的方案为，每一个分块基于LLM构建出一个chunk header，一同放入到向量数据库中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](figures/cch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import tiktoken\n",
    "import os \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY') # OpenAI API key\n",
    "\n",
    "pdf_path = \"data/pumpkin_book.pdf\"\n",
    "qa_path = 'data/train_dataset.json'\n",
    "embedding = HuggingFaceEmbeddings(model_name='BAAI/bge-small-zh-v1.5')\n",
    "\n",
    "with open(qa_path, 'r', encoding='utf-8') as file:\n",
    "    qa_pairs = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DOCUMENT_TITLE_PROMPT = \"\"\"\n",
    "指令\n",
    "总结以下文档内容的标题是什么？\n",
    "\n",
    "您的回答直接输出内容标题且仅此而已。请不要回应其他内容。\n",
    "\n",
    "{document_title_guidance}\n",
    "\n",
    "{truncation_message}\n",
    "\n",
    "文档\n",
    "{document_text}\n",
    "\"\"\".strip()\n",
    "\n",
    "TRUNCATION_MESSAGE = \"\"\"\n",
    "请注意，下面提供的文档文本仅为文档的前~{num_words}个词。这对于此任务来说已经足够。您的回答仍应与整个文档相关，而不仅仅是下面提供的文本。\n",
    "\"\"\".strip()\n",
    "\n",
    "MAX_CONTENT_TOKENS = 4000\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "TOKEN_ENCODER = tiktoken.encoding_for_model('gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 实现示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str):\n",
    "    \"\"\"\n",
    "    实现文本清理函数\n",
    "\n",
    "    参数:\n",
    "        text: 需要清理的字段\n",
    "\n",
    "    返回:\n",
    "        清理完成后返回的字段\n",
    "    \"\"\"\n",
    "    # 删除每页开头与结尾标语及链接\n",
    "    text = re.sub(r'→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←', '', text)\n",
    "    text = re.sub(r'→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', '', text)\n",
    "    # 删除字符串开头的空格\n",
    "    text = re.sub(r'\\s+', '', text)\n",
    "    # 删除回车\n",
    "    text = re.sub(r'\\n+', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def make_llm_call(chat_messages: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    调用 OpenAI 语言模型的 API。\n",
    "\n",
    "    参数:\n",
    "        chat_messages (list[dict]): 用于聊天完成的消息字典列表。\n",
    "\n",
    "    返回:\n",
    "        str: 语言模型生成的响应。\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=chat_messages,\n",
    "        max_tokens=MAX_CONTENT_TOKENS,\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def truncate_content(content: str, max_tokens: int) -> tuple[str, int]:\n",
    "    \"\"\"\n",
    "    将内容截断为指定的最大token数。\n",
    "\n",
    "    参数:\n",
    "        content (str): 需要截断的输入文本。\n",
    "        max_tokens (int): 保留的最大token数。\n",
    "\n",
    "    返回:\n",
    "        tuple[str, int]: 包含截断后内容和令牌数量的元组。\n",
    "    \"\"\"\n",
    "    tokens = TOKEN_ENCODER.encode(content, disallowed_special=())\n",
    "    truncated_tokens = tokens[:max_tokens]\n",
    "    return TOKEN_ENCODER.decode(truncated_tokens), min(len(tokens), max_tokens)\n",
    "\n",
    "def get_document_title(document_text: str, document_title_guidance: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    使用语言模型提取文档标题。\n",
    "\n",
    "    参数:\n",
    "        document_text (str): 文档的文本内容。\n",
    "        document_title_guidance (str, 可选): 提取标题的额外指导。默认为 \"\"。\n",
    "\n",
    "    返回:\n",
    "        str: 提取的文档标题。\n",
    "    \"\"\"\n",
    "    document_text, num_tokens = truncate_content(document_text, MAX_CONTENT_TOKENS)\n",
    "    truncation_message = TRUNCATION_MESSAGE.format(num_words=3000) if num_tokens >= MAX_CONTENT_TOKENS else \"\"\n",
    "\n",
    "    prompt = DOCUMENT_TITLE_PROMPT.format(\n",
    "        document_title_guidance=document_title_guidance,\n",
    "        document_text=document_text,\n",
    "        truncation_message=truncation_message\n",
    "    )\n",
    "    chat_messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    return make_llm_call(chat_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Title: 《机器学习》入门教材总结\n"
     ]
    }
   ],
   "source": [
    "example_text = '“周志华老师的《机器学习》（西瓜书）是机器学习领域的经典入门教材之一，周老师为了使尽可能多的读者通过西瓜书对机器学习有所了解， 所以在书中对部分公式的推导细节没有详述，但是这对那些想深究公式推导细节的读者'\n",
    "\n",
    "document_title = get_document_title(example_text)\n",
    "print(f\"Document Title: {document_title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 功能实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pdf(path, chunk_size=2000, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    使用 OpenAI 嵌入将 PDF 书籍编码为向量存储。\n",
    "\n",
    "    参数:\n",
    "        path: PDF 文件的路径。\n",
    "        chunk_size: 每个文本块的期望大小。\n",
    "        chunk_overlap: 连续块之间的重叠量。\n",
    "\n",
    "    返回:\n",
    "        包含内容的向量存储。\n",
    "    \"\"\"\n",
    "\n",
    "    # 创建一个 PyMuPDFLoader Class 实例，输入为待加载的 pdf 文档路径，加载PDF\n",
    "    loader = PyMuPDFLoader(path)\n",
    "    \n",
    "    # 调用 PyMuPDFLoader Class 的函数 load 对 pdf 文件进行加载\n",
    "    pdf_pages = loader.load()\n",
    "    \n",
    "    # 第13页为南瓜书第一页正文，因此从13页开始,从倒数13页涉及敏感用语，因此从-13页结束\n",
    "    data_pages = pdf_pages[13:-13]\n",
    "\n",
    "    for page in data_pages:\n",
    "        page.page_content = clean_text(page.page_content)\n",
    "\n",
    "    # 文档分块\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, separator='')\n",
    "\n",
    "    # 运用metadata，增加是否有cch标签\n",
    "    split_docs_wo_cch = text_splitter.split_documents(data_pages)\n",
    "    for split_doc in tqdm(split_docs_wo_cch):\n",
    "        # 加入chunk_size和chunk_overlap\n",
    "        split_doc.metadata['chunk_size'] = chunk_size\n",
    "        split_doc.metadata['chunk_overlap'] = chunk_overlap\n",
    "        # 加入数据访问权限\n",
    "        split_doc.metadata['cch_type'] = 0\n",
    "\n",
    "    # 给每一个分块增加header\n",
    "    split_docs_w_cch = text_splitter.split_documents(data_pages)\n",
    "    for split_doc in tqdm(split_docs_w_cch):\n",
    "        document_title = get_document_title(split_doc.page_content)\n",
    "        split_doc.page_content = f\"文章标题: {document_title}\\n\\n{split_doc.page_content}\"\n",
    "        # 加入chunk_size和chunk_overlap\n",
    "        split_doc.metadata['chunk_size'] = chunk_size\n",
    "        split_doc.metadata['chunk_overlap'] = chunk_overlap\n",
    "        # 加入数据访问权限\n",
    "        split_doc.metadata['cch_type'] = 1\n",
    "\n",
    "    # 构建向量库\n",
    "    vectordb = Chroma.from_documents(documents=split_docs_w_cch + split_docs_wo_cch, embedding=embedding)\n",
    "\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCHRetriever:\n",
    "    def __init__(self, chunk_size=2000, chunk_overlap=400):\n",
    "        self.llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", max_tokens=4000)\n",
    "        self.embeddings = embedding\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    # 基于pdf构建向量数据库   \n",
    "    def encode_pdf(self, files_path):\n",
    "        self.vectorstore = encode_pdf(files_path, chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:00<00:00, 2983396.15it/s]\n",
      "100%|██████████| 170/170 [04:52<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "retriever = CCHRetriever()\n",
    "retriever.encode_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qa_pairs为第一部分研究不同分块参数效果过程中构建的问答对，这里我们将沿用这一数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '请根据提供的上下文信息，解释什么是“泛化”能力，并给出一个具体的例子说明为何泛化能力是衡量机器学习模型好坏的关键。',\n",
       " 'answer': '泛化：由于机器学习的目标是根据已知来对未知做出尽可能准确的判断，因此对未知事物判断的准确与否才是衡量一个模型好坏的关键，我们称此为“泛化”能力。例如学习西瓜好坏时，假设训练集中共有3个样本：{(x1=(青绿;蜷缩),y1=好瓜),(x2=(乌黑;蜷缩),y2=好瓜),(x3=(浅白;蜷缩),y3=好瓜)}，同时假设判断西瓜好坏的真相是“只要根蒂蜷缩就是好瓜”，如果应用算法A在此训练集上训练得到模型fa(x)，模型a学到的规律是“色泽等于青绿、乌黑或者浅白时，同时根蒂蜷缩即为好瓜，否则便是坏瓜”，再应用算法B在此训练集上训练得到模型fb(x)，模型fb(x)学到的规律是“只要根蒂蜷缩就是好瓜”，因此对于一个未见过的西瓜样本x=(金黄;蜷缩)来说，模型fa(x)给出的预测结果为“坏瓜”，模型fb(x)给出的预测结果为“好瓜”，此时我们称模型fb(x)的泛化能力优于模型fa(x)。通过以上举例可知，尽管模型fa(x)和模型fb(x)对训练集学得一样好，即两个模型对训练集中每个样本的判断都对，但是其所学到的规律是不同的。导致此现象最直接的原因是算法的不同，但是算法通常是有限的，可穷举的，尤其是在特定任务场景下可使用的算法更是有限，因此，数据便是导致此现象的另一重要原因，这也就是机器学习领域常说的“数据决定模型的上限，而算法则是让模型无限逼近上限”。',\n",
       " 'page_num': 14}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc = 1\n",
    "\n",
    "qa_pairs[test_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_cch_filter_condition = {\"cch_type\": 1} # 使用了cch的向量记录\n",
    "wo_cch_filter_condition = {\"cch_type\": 0} # 没有使用了cch的向量记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'author': '', 'cch_type': 1, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 17, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='文章标题: 模型评估与选择\\n\\n第2章模型评估与选择如“西瓜书”前言所述，本章仍属于机器学习基础知识，如果说第1章介绍了什么是机器学习及机器学习的相关数学符号，那么本章则进一步介绍机器学习的相关概念。具体来说，介绍内容正如本章名称“模型评估与选择”所述，讲述的是如何评估模型的优劣和选择最适合自己业务场景的模型。由于“模型评估与选择”是在模型产出以后进行的下游工作，要想完全吸收本章内容需要读者对模型有一些基本的认知，因此零基础的读者直接看本章会很吃力，实属正常，在此建议零基础的读者可以简单泛读本章，仅看能看懂的部分即可，或者直接跳过本章从第3章开始看，直至看完第6章以后再回头来看本章便会轻松许多。2.1经验误差与过拟合梳理本节的几个概念。错误率：E=am，其中m为样本个数，a为分类错误样本个数。精度：精度=1-错误率。误差：学习器的实际预测输出与样本的真实输出之间的差异。经验误差：学习器在训练集上的误差，又称为“训练误差”。泛化误差：学习器在新样本上的误差。经验误差和泛化误差用于分类问题的定义式可参见“西瓜书”第12章的式(12.1)和式(12.2)，接下来辨析一下以上几个概念。错误率和精度很容易理解，而且很明显是针对分类问题的。误差的概念更适用于回归问题，但是，根据“西瓜书”第12章的式(12.1)和式(12.2)的定义可以看出，在分类问题中也会使用误差的概念，此时的“差异”指的是学习器的实际预测输出的类别与样本真实的类别是否一致，若一致则“差异”为0，若不一致则“差异”为1，训练误差是在训练集上差异的平均值，而泛化误差则是在新样本（训练集中未出现过的样本）上差异的平均值。过拟合是由于模型的学习能力相对于数据来说过于强大，反过来说，欠拟合是因为模型的学习能力相对于数据来说过于低下。暂且抛开“没有免费的午餐”定理不谈，例如对于“西瓜书”第1章图1.4中的训练样本（黑点）来说，用类似于抛物线的曲线A去拟合则较为合理，而比较崎岖的曲线B相对于训练样本来说学习能力过于强大，但若仅用一条直线去训练则相对于训练样本来说直线的学习能力过于低下。2.2评估方法本节介绍了3种模型评估方法：留出法、交叉验证法、自助法。留出法由于操作简单，因此最常用；交叉验证法常用于对比同一算法的不同参数配置之间的效果，以及对比不同算法之间的效果；自助法常用于集成学习（详见“西瓜书”第8章的8.2节和8.3节）产生基分类器。留出法和自助法简单易懂，在此不再赘述，下面举例说明交叉验证法的常用方式。对比同一算法的不同参数配置之间的效果：假设现有数据集D，且有一个被评估认为适合用于数据集D的算法L，该算法有可配置的参数，假设备选的参数配置方案有两套：方案a，方案b。下面通过交叉验证法为算法L筛选出在数据集D上效果最好的参数配置方案。以3折交叉验证为例，首先按照“西瓜书”中所说的方法，通过分层采样将数据集D划分为3个大小相似的互斥子集：D1,D2,D3，然后分别用其中1个子集作为测试集，其他子集作为训练集，这样就可获得3组训练集和测试集：训练集1：D1∪D2，测试集1:D3训练集2：D1∪D3，测试集2:D2训练集3：D2∪D3，测试集3:D1接下来用算法L搭配方案a在训练集1上进行训练，训练结束后将训练得到的模型在测试集1上进行测试，得到测试结果1，依此方法再分别通过训练集2和测试集2、训练集3和测试集3得到测试结果'),\n",
       " Document(metadata={'author': '', 'cch_type': 1, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 151, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='文章标题: 机器学习中的泛化误差与样本数量的关系\\n\\nE(h)>ϵ且bE(h)=0是互斥的，因此总的概率P(h∈H:E(h)>ϵ∧bE(h)=0)就是这些互斥事件之和，即P\\x10h∈H:E(h)>ϵ∧bE(h)=0\\x11=|H|XiP\\x10E(hi)>ϵ∧bE(hi)=0\\x11<|H|(1−ϵ)m小于号依据公式(12.11)。第二个小于号实际上是要证明|H|(1−ϵ)m<|H|e−mϵ，即证明(1−ϵ)m<e−mϵ，其中ϵ∈(0,1]，m是正整数，推导如下：当ϵ=1时，显然成立，当ϵ∈(0,1)时，因为左式和右式的值域均大于0，所以可以左右两边同时取对数，又因为对数函数是单调递增函数，所以即证明mln(1−ϵ)<−mϵ，即证明ln(1−ϵ)<−ϵ，这个式子很容易证明：令f(ϵ)=ln(1−ϵ)+ϵ，其中ϵ∈(0,1)，f′(ϵ)=1−11−ϵ=0⇒ϵ=0取极大值0，因此ln(1−ϵ)<−ϵ也即|H|(1−ϵ)m<|H|e−mϵ成立。12.3.4式(12.13)的解释回到我们要回答的问题：到底需要多少样例才能学得目标概念c的有效近似。只要训练集D的规模能使学习算法L以概率1−δ找到目标假设的ϵ近似即可。根据式12.12，学习算法L生成的假设大于目标假设的ϵ近似的概率为P\\x10h∈H:E(h)>ϵ∧bE(h)=0\\x11<|H|e−mϵ，因此学习算法L生成的假设落在目标假设的ϵ近似的概率为1−P\\x10h∈H:E(h)>ϵ∧bE(h)=0\\x11≥1−|H|e−mϵ，这个概率我们希望至少是1−δ，因此1−δ⩽1−|H|e−mϵ⇒|H|e−mϵ⩽δ12.3.5式(12.14)的推导|H|e−mϵ⩽δe−mϵ⩽δ|H|−mϵ⩽lnδ−ln|H|m⩾1ϵ\\x12ln|H|+ln1δ\\x13这个式子告诉我们，在假设空间H是PAC可学习的情况下，输出假设h的泛化误差ϵ随样本数目m增大而收敛到0，收敛速率为O(1m)。这也是我们在机器学习中的一个共识，即可供模型训练的观测集样本数量越多，机器学习模型的泛化性能越好。12.3.6引理12.1的解释根据式(12.2),bE(h)=1mPmi=1I(h(xi)̸=yi),而指示函数I(·)取值非0即1,也就是说0≤I(h(xi)̸=yi)≤1;对于式(12.1)的E(h)实际上表示I(h(xi)̸=yi)为1的期望E(I(h(xi)̸=yi))(泛化误差表示样本空间中任取一个样本,其预测类别不等于真实类别的概率),当假设h确定时,泛化误差固定不变,因此可记为E(h)=1mPmi=1E(I(h(xi)̸=yi))。此时,将bE(h)和E(h)代入式(12.15)到式(12.17),对比式(12.5)和式(12.6)的Hoeffding不等式可知,式(12.15)对应式(12.5),式(12.16)与式(12.15)对称,式(12.17)对应式(12.6)。'),\n",
       " Document(metadata={'author': '', 'cch_type': 1, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 150, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='文章标题: 学习算法的有效性与样本需求分析\\n\\n•研究某任务在什么样的条件下可学得较好的模型？（定义12.2）•某算法在什么样的条件下可进行有效的学习?（定义12.3）•需多少训练样例才能获得较好的模型？（定义12.4）有限假设空间指H中包含的假设个数是有限的,反之则为无限假设空间;无限假设空间更为常见,例如能够将图5.4(a)(b)(c)中的正例和反例样本分开的线性超平面个数是无限多的。12.2.1式(12.9)的解释PAC辨识的定义：E(h)表示算法L在用观测集D训练后输出的假设函数h，它的泛化误差(见公式12.1)。这个概率定义指出，如果h的泛化误差不大于ϵ的概率不小于1−δ，那么我们称学习算法L能从假设空间H中PAC辨识概念类C。12.3有限假设空间本节内容分两部分,第1部分“可分情形”时,可以达到经验误差bE(h)=0,做的事情是以1−δ概率学得目标概念的ϵ近似,即式(12.12);第2部分“不可分情形”时,无法达到经验误差bE(h)=0,做的事情是以1−δ概率学得minh∈HE(h)的ϵ近似,即式(12.20)。无论哪种情形,对于h∈H,可以得到该假设的泛化误差E(h)与经验误差bE(h)的关系,即“当样例数目m较大时,h的经验误差是泛化误差很好的近似”,即式(12.18);实际研究中经常需要推导类似的泛化误差上下界。从式12.10到式12.14的公式是为了回答一个问题：到底需要多少样例才能学得目标概念c的有效近似。只要训练集D的规模能使学习算法L以概率1−δ找到目标假设的ϵ近似即可。下面就是用数学公式进行抽象。12.3.1式(12.10)的解释P(h(x)=y)=1−P(h(x)̸=y)因为它们是对立事件，P(h(x)̸=y)=E(h)是泛化误差的定义(见12.1)，由于我们假定了泛化误差E(h)>ϵ，因此有1−E(h)<1−ϵ。12.3.2式(12.11)的解释先解释什么是h与D“表现一致”，12.2节开头阐述了这样的概念，如果h能将D中所有样本按与真实标记一致的方式完全分开，我们称问题对学习算法是一致的。即(h(x1)=y1)∧...∧(h(xm)=ym)为True。因为每个事件是独立的，所以上式可以写成P((h(x1)=y1)∧...∧(h(xm)=ym))=Qmi=1P(h(xi)=yi)。根据对立事件的定义有：Qmi=1P(h(xi)=yi)=Qmi=1(1−P(h(xi)̸=yi))，又根据公式(12.10)，有mYi=1(1−P(h(xi)̸=yi))<mYi=1(1−ϵ)=(1−ϵ)m12.3.3式(12.12)的推导首先解释为什么”我们事先并不知道学习算法L会输出H中的哪个假设“，因为一些学习算法对用一个观察集D的输出结果是非确定的，比如感知机就是个典型的例子，训练样本的顺序也会影响感知机学习到的假设h参数的值。泛化误差大于ϵ且经验误差为0的假设(即在训练集上表现完美的假设)出现的概率可以表示为P(h∈H:E(h)>ϵ∧bE(h)=0)，根据式12.11，每一个这样的假设h都满足P(E(h)>ϵ∧bE(h)=0)<(1−ϵ)m，假设一共有|H|这么多个这样的假设h，因为每个假设h满足'),\n",
       " Document(metadata={'author': '', 'cch_type': 1, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 176, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='文章标题: A survey on multi-view learning\\n\\n[3]ChangXu,DachengTao,andChaoXu.Asurveyonmulti-viewlearning.arXivpreprintarXiv:1304.5634,2013.'),\n",
       " Document(metadata={'author': '', 'cch_type': 1, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 53, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='文章标题: 第5章神经网络\\n\\n第5章神经网络神经网络类算法可以堪称当今最主流的一类机器学习算法，其本质上和前几章讲到的线性回归、对数几率回归、决策树等算法一样均属于机器学习算法，也是被发明用来完成分类和回归等任务。不过由于神经网络类算法在如今超强算力的加持下效果表现极其出色，且从理论角度来说神经网络层堆叠得越深其效果越好，因此也单独称用深层神经网络类算法所做的机器学习为深度学习，属于机器学习的子集。5.1神经元模型本节对神经元模型的介绍通俗易懂，在此不再赘述。本节第2段提到“阈值”(threshold)的概念时，“西瓜书”左侧边注特意强调是“阈(yù)”而不是“阀(fá)”，这是因为该字确实很容易认错，读者注意一下即可。图5.1所示的M-P神经元模型，其中的“M-P”便是两位作者McCulloch和Pitts的首字母简写。5.2感知机与多层网络5.2.1式(5.1)和式(5.2)的推导此式是感知机学习算法中的参数更新公式，下面依次给出感知机模型、学习策略和学习算法的具体介绍[1]：感知机模型：已知感知机由两层神经元组成，故感知机模型的公式可表示为y=fnXi=1wixi−θ!=f(wTx−θ)其中，x∈Rn，为样本的特征向量，是感知机模型的输入；w,θ是感知机模型的参数，w∈Rn，为权重，θ为阈值。假定f为阶跃函数，那么感知机模型的公式可进一步表示为（用ε(·)代表阶跃函数）y=ε(wTx−θ)=(1,wTx−θ⩾0;0,wTx−θ<0.由于n维空间中的超平面方程为w1x1+w2x2+···+wnxn+b=wTx+b=0所以此时感知机模型公式中的wTx−θ可以看作是n维空间中的一个超平面，将n维空间划分为wTx−θ⩾0和wTx−θ<0两个子空间，落在前一个子空间的样本对应的模型输出值为1，落在后一个子空间的样本对应的模型输出值为0，如此便实现了分类功能。感知机学习策略：给定一个数据集T={(x1,y1),(x2,y2),···,(xN,yN)}其中xi∈Rn,yi∈{0,1},i=1,2,···,N。如果存在某个超平面wTx+b=0能将数据集T中的正样本和负样本完全正确地划分到超平面两侧，即对所有yi=1的样本xi有wTxi+b⩾0，对所有yi=0的样本xi有wTxi+b<0，则称数据集T线性可分，否则称数据集T线性不可分。现给定一个线性可分的数据集T，感知机的学习目标是求得能对数据集T中的正负样本完全正确划分的分离超平面wTx−θ=0'),\n",
       " Document(metadata={'author': '', 'cch_type': 1, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 148, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='文章标题: 计算学习理论\\n\\n第12章计算学习理论正如本章开篇所述，计算学习理论研究目的是分析学习任务的困难本质，为学习算法提供理论保证，并根据分析结果指导算法设计。例如，“西瓜书”定理12.1、定理12.3、定理12.6所表达意思的共同点是，泛化误差与经验误差之差的绝对值以很大概率(1−δ)很小，且这个差的绝对值随着训练样本个数(m)的增加而减小，随着模型复杂度（定理12.1为假设空间包含的假设个数|H|，定理12.3中为假设空间的VC维，定理12.6中为(经验)Rademacher复杂度）的减小而减小。因此，若想要得到一个泛化误差很小的模型，足够的训练样本是前提，最小化经验误差是实现途径，另外还要选择性能相同的模型中模型复杂度最低的那一个；“最小化经验误差”即常说的经验风险最小化，“选择模型复杂度最低的那一个”即结构风险最小化，可以参见“西瓜书”6.4节最后一段的描述，尤其是式(6.42)所表达的含义。12.1基础知识统计学中有总体集合和样本集合之分,比如要统计国内本科生对机器学习的掌握情况,此时全国所有的本科生就是总体集合,但总体集合往往太大而不具有实际可操作性,一般都是取总体集合的一部分,比如从双一流A类、双一流B类、一流学科建设高校、普通高校中各找一部分学生(即样本集合)进行调研,以此来了解国内本科生对机器学习的掌握情况。在机器学习中,样本空间(参见1.2节)对应总体集合,而我们手头上的样例集D对应样本集合,样例集D是从样本空间中采样而得,分布D可理解为当从样本空间采样获得样例集D时每个样本被采到的概率,我们用D(t)表示样本空间第t个样本被采到的概率。12.1.1式(12.1)的解释该式为泛化误差的定义式，所谓泛化误差，是指当样本x从真实的样本分布D中采样后其预测值h(x)不等于真实值y的概率。在现实世界中，我们很难获得样本分布D，我们拿到的数据集可以看做是从样本分布D中独立同分布采样得到的。在西瓜书中，我们拿到的数据集，称为样例集D[也叫观测集、样本集，注意与花体D的区别]。12.1.2式(12.2)的解释该式为经验误差的定义式，所谓经验误差，是指观测集D中的样本xi,i=1,2,···,m的预测值h(xi)和真实值yi的期望误差。12.1.3式(12.3)的解释假设我们有两个模型h1和h2，将它们同时作用于样本x上，那么他们的”不合“度定义为这两个模型预测值不相同的概率。12.1.4式(12.4)的解释Jensen不等式：这个式子可以做很直观的理解，比如说在二维空间上，凸函数可以想象成开口向上的抛物线，假如我们有两个点x1,x2，那么f(E(x))表示的是两个点的均值的纵坐标，而E(f(x))表示的是两个点纵坐标的均值，因为两个点的均值落在抛物线的凹处，所以均值的纵坐标会小一些。12.1.5式(12.5)的解释随机变量的观测值是随机的,进一步地,随机过程的每个时刻都是一个随机变量。式中,1mPmi=1xi表示m个独立随机变量各自的某次观测值的平均,1mPmi=1E(xi)表示m个独立随机变量各自的期望的平均。'),\n",
       " Document(metadata={'author': '', 'cch_type': 1, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 156, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='文章标题: PAC可学习的定义与性质分析\\n\\n12.4.7式(12.30)的解释这个是经验风险最小化的定义式。即从假设空间中找出能使经验风险最小的假设。12.4.8定理12.4的解释首先回忆PAC可学习的概念，见定义12.2，而可知/不可知PAC可学习之间的区别仅仅在于概念类c是否包含于假设空间H中。令δ′=δ2r(ln2/δ′)2m=ϵ2结合这两个标记的转换，由推论12.1可知：bE(g)−ϵ2⩽E(g)⩽bE(g)+ϵ2至少以1−δ/2的概率成立。写成概率的形式即：P\\x10|E(g)−bE(g)|⩽ϵ2\\x11⩾1−δ/2即P\\x10\\x10E(g)−bE(g)⩽ϵ2\\x11∧\\x10E(g)−bE(g)⩾−ϵ2\\x11\\x11⩾1−δ/2，因此P\\x10E(g)−bE(g)⩽ϵ2\\x11⩾1−δ/2且P\\x10E(g)−bE(g)⩾−ϵ2\\x11⩾1−δ/2成立。再令s8dln2emd+8ln4δ′m=ϵ2由式12.29可知P\\x10E(h)−bE(h)⩽ϵ2\\x11⩾1−δ2同理，P\\x10E(h)−bE(h)⩽ϵ2\\x11⩾1−δ/2且P\\x10E(h)−bE(h)⩾−ϵ2\\x11⩾1−δ/2成立。由P\\x10E(g)−bE(g)⩾−ϵ2\\x11⩾1−δ/2和P\\x10E(h)−bE(h)⩽ϵ2\\x11⩾1−δ/2均成立可知则事件E(g)−bE(g)⩾−ϵ2和事件E(h)−bE(h)⩽ϵ2同时成立的概率为：P\\x10\\x10E(g)−bE(g)⩾−ϵ2\\x11∧\\x10E(h)−bE(h)⩽ϵ2\\x11\\x11=P\\x10E(g)−bE(g)⩾−ϵ2\\x11+P\\x10E(h)−bE(h)⩽ϵ2\\x11−P\\x10\\x10E(g)−bE(g)⩾−ϵ2\\x11∨\\x10E(h)−bE(h)⩽ϵ2\\x11\\x11⩾1−δ/2+1−δ/2−1=1−δ即P\\x10\\x10E(g)−bE(g)⩾−ϵ2\\x11∧\\x10E(h)−bE(h)⩽ϵ2\\x11\\x11⩾1−δ因此P\\x10bE(g)−E(g)+E(h)−bE(h)⩽ϵ2+ϵ2\\x11=P\\x10E(h)−E(g)⩽bE(h)−bE(g)+ϵ\\x11⩾1−δ再由h和g的定义，h表示假设空间中经验误差最小的假设，g表示泛化误差最小的假设，将这两个假设共用作用于样本集D，则一定有bE(h)⩽bE(g)，因此上式可以简化为：P(E(h)−E(g)⩽ϵ)⩾1−δ根据式12.32和式12.34，可以求出m为关于(1/ϵ,1/δ,size(x),size(c))的多项式，因此根据定理12.2，定理12.5，得到结论任何VC维有限的假设空间H都是(不可知)PAC可学习的。'),\n",
       " Document(metadata={'author': '', 'cch_type': 1, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 155, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='文章标题: 泛化误差界与样本数量的关系\\n\\n注：最后一步依据组合公式，推导如下：m−1i!+m−1i−1!=(m−1)!(m−1−i)!i!+(m−1)!(m−1−i+1)!(i−1)!=(m−1)!(m−i)(m−i)(m−1−i)!i!+(m−1)!i(m−i)!(i−1)!i=(m−1)!(m−i)+(m−1)!i(m−i)!i!=(m−1)!(m−i+i)(m−i)!i!=(m−1)!m(m−i)!i!=m!(m−i)!i!=mi!12.4.5式(12.28)的解释ΠH(m)⩽dXi=0mi!⩽dXi=0mi!\\x10md\\x11d−i=\\x10md\\x11ddXi=0mi!\\x12dm\\x13i⩽\\x10md\\x11dmXi=0mi!\\x12dm\\x13i=\\x10md\\x11d\\x121+dm\\x13m<\\x10e·md\\x11d第一步到第二步和第三步到第四步均因为m⩾d，第四步到第五步是由于二项式定理[3]：(x+y)n=Pnk=0nk!xn−kyk，其中令k=i,n=m,x=1,y=dm得\\x00md\\x01dPmi=0mi!\\x00dm\\x01i=\\x00md\\x01d(1+dm)m，最后一步的不等式即需证明\\x001+dm\\x01m⩽ed，因为\\x001+dm\\x01m=\\x001+dm\\x01mdd，根据自然对数底数e的定义[4]，\\x001+dm\\x01mdd<ed，注意原文中用的是⩽，但是由于e=limdm→0\\x001+dm\\x01md的定义是一个极限，所以应该是用<。12.4.6式(12.29)的解释这里应该是作者的笔误，根据式12.22，E(h)−bE(h)应当被绝对值符号包裹。将式12.28带入式12.22得P\\x10|E(h)−bE(h)|>ϵ\\x11⩽4\\x122emd\\x13dexp\\x12−mϵ28\\x13令4\\x002emd\\x01dexp\\x10−mϵ28\\x11=δ可解得δ=s8dln2emd+8ln4δm带入式12.22，则定理得证。这个式子是用VC维表示泛化界，可以看出，泛化误差界只与样本数量m有关，收敛速率为qlnmm(书上简化为1√m)。'),\n",
       " Document(metadata={'author': '', 'cch_type': 1, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 54, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='文章标题: 感知机学习算法及其损失函数优化\\n\\n假设此时误分类样本集合为M⊆T，对任意一个误分类样本(x,y)∈M来说，当wTx−θ⩾0时，模型输出值为ˆy=1，样本真实标记为y=0；反之，当wTx−θ<0时，模型输出值为ˆy=0，样本真实标记为y=1。综合两种情形可知，以下公式恒成立：(ˆy−y)\\x00wTx−θ\\x01⩾0所以，给定数据集T，其损失函数可以定义为L(w,θ)=Xx∈M(ˆy−y)\\x00wTx−θ\\x01显然，此损失函数是非负的。如果没有误分类点，则损失函数值为0。而且，误分类点越少，误分类点离超平面越近，损失函数值就越小。因此，给定数据集T，损失函数L(w,θ)是关于w,θ的连续可导函数。感知机学习算法：感知机模型的学习问题可以转化为求解损失函数的最优化问题，具体地，给定数据集T={(x1,y1),(x2,y2),···,(xN,yN)}其中xi∈Rn,yi∈{0,1}，求参数w,θ，使其为极小化损失函数的解：minw,θL(w,θ)=minw,θXxi∈M(ˆyi−yi)(wTxi−θ)其中M⊆T为误分类样本集合。若将阈值θ看作一个固定输入为−1的“哑节点”，即−θ=−1·wn+1=xn+1·wn+1那么wTxi−θ可化简为wTxi−θ=nXj=1wjxj+xn+1·wn+1=n+1Xj=1wjxj=wTxi其中xi∈Rn+1,w∈Rn+1。根据该公式，可将要求解的极小化问题进一步简化为minwL(w)=minwXxi∈M(ˆyi−yi)wTxi假设误分类样本集合M固定，那么可以求得损失函数L(w)的梯度∇wL(w)=Xxi∈M(ˆyi−yi)xi感知机的学习算法具体采用的是随机梯度下降法，即在极小化过程中，不是一次使M中所有误分类点的梯度下降，而是一次随机选取一个误分类点并使其梯度下降。所以权重w的更新公式为w←w+∆w∆w=−η(ˆyi−yi)xi=η(yi−ˆyi)xi相应地，w中的某个分量wi的更新公式即式(5.2)。5.2.2图5.5的解释图5.5中(0,0),(0,1),(1,0),(1,1)这4个样本点实现“异或”计算的过程如下：(x1,x2)→h1=ε(x1−x2−0.5),h2=ε(x2−x1−0.5)→y=ε(h1+h2−0.5)以(0,1)为例，首先求得h1=ε(0−1−0.5)=0,h2=ε(1−0−0.5)=1，然后求得y=ε(0+1−0.5)=1。'),\n",
       " Document(metadata={'author': '', 'cch_type': 1, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 161, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='文章标题: 算法的稳定性与经验损失最小化分析\\n\\n析可以获得与算法有关的分析结果。算法的“稳定性”考察的是算法在输入发生变化时，输出是否会随之发生较大的变化。12.7.1泛化/经验/留一损失的解释根据式(12.54)上方关于损失函数的描述：“刻画了假设的预测标记与真实标记之间的差别”，这里针对的是二分类，预测标记和真实标记均只能取和两个值，它们之间的“差别”又能是什么呢？因此，当“差别”取为时，式(12.54)的泛化损失就是式(12.1)的泛化误差，式(12.55)的经验损失就是式(12.2)的经验误差，如果类似于式(12.1)和式(12.2)继续定义留一误差，那么式(12.56)就对应于留一误差。12.7.2式(12.57)的解释根据三角不等式[7]，有|a+b|≤|a|+|b|，将a=ℓ(LD,z)−ℓ(LDi)，b=ℓ(LDi,z)−ℓ\\x00LD\\\\i,z\\x01带入即可得出第一个不等式，根据D\\\\i表示移除D中第i个样本，Di表示替换D中第i个样本，那么a,b的变动均为一个样本，根据式12.57，a⩽β,b⩽β，因此a+b⩽2β。12.7.3定理12.8的解释西瓜书在该定理下方已明确给出该定理的意义,即“定理12.8给出了基于稳定性分析推导出的学习算法L学得假设的泛化误差界”,式(12.58)和式(12.59)分别基于经验损失和留一损失给出了泛化损失的上界。接下来讨论两个相关问题:(1)定理12.8的条件包括损失函数有界,即0⩽ℓ(LD,z)⩽M;如本节第1条注解“泛化/经验/留一损失的解释”中所述,若“差别”取为I(LD(x),y),则泛化损失对应于泛化误差,此时上限M=1。(2)在前面泛化误差上界的推导中（例如定理12.1、定理12.3、定理12.6、定理12.7),上界中与样本数m有关的项收玫率均为O(1/√m),但在该定理中却是O(β√m);一般来讲,随着样本数m的增加,经验误差/损失应该收玫于泛化误差/损失,因此这里假设β=1/m(书中式(12.59)下方第3行写为β=O(1/m)),而在第2条注解“定义12.10的解释”中已经提到β的取值的确会随着样本数m的增多会变小,虽然书中并没有严格去讨论β随m增多的变化规律,但至少直觉上是对的。12.7.4式(12.60)的推导将β=1m带入至式(12.58)即得证。12.7.5经验损失最小化顾名思义,“经验损失最小化”指通过最小化经验损失来求得假设函数。这里,“对于损失函数ℓ,若学习算法L所输出的假设满足经验损失最小化,则称算法L满足经验风险最小化原则,简称算法是ERM的”。在”西瓜书”第278页,若学习算法L输出的假设h满足式(12.30),则也称L为满足经验风险最小化原则的算法。而很明显,式(12.30)是在最小化经验误差。那么最小化经验误差和最小化经验损失有什么区别么?在”西瓜书“第286页左下角边注中提到,“最小化经验误差和最小化经验损失有时并不相同,这是由于存在某些病态的损失函数ℓ使得最小化经验损失并不是最小化经验误差”。对于“误差”、“损失”、“风险”等概念的辨析，参见“西瓜书”第2章2.1节的注解。')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检索使用了cch技术的的记录，返回top10\n",
    "test_query = qa_pairs[test_doc]['query']\n",
    "results = retriever.vectorstore.similarity_search(test_query, k=10, filter=w_cch_filter_condition)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'author': '', 'cch_type': 0, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 17, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='第2章模型评估与选择如“西瓜书”前言所述，本章仍属于机器学习基础知识，如果说第1章介绍了什么是机器学习及机器学习的相关数学符号，那么本章则进一步介绍机器学习的相关概念。具体来说，介绍内容正如本章名称“模型评估与选择”所述，讲述的是如何评估模型的优劣和选择最适合自己业务场景的模型。由于“模型评估与选择”是在模型产出以后进行的下游工作，要想完全吸收本章内容需要读者对模型有一些基本的认知，因此零基础的读者直接看本章会很吃力，实属正常，在此建议零基础的读者可以简单泛读本章，仅看能看懂的部分即可，或者直接跳过本章从第3章开始看，直至看完第6章以后再回头来看本章便会轻松许多。2.1经验误差与过拟合梳理本节的几个概念。错误率：E=am，其中m为样本个数，a为分类错误样本个数。精度：精度=1-错误率。误差：学习器的实际预测输出与样本的真实输出之间的差异。经验误差：学习器在训练集上的误差，又称为“训练误差”。泛化误差：学习器在新样本上的误差。经验误差和泛化误差用于分类问题的定义式可参见“西瓜书”第12章的式(12.1)和式(12.2)，接下来辨析一下以上几个概念。错误率和精度很容易理解，而且很明显是针对分类问题的。误差的概念更适用于回归问题，但是，根据“西瓜书”第12章的式(12.1)和式(12.2)的定义可以看出，在分类问题中也会使用误差的概念，此时的“差异”指的是学习器的实际预测输出的类别与样本真实的类别是否一致，若一致则“差异”为0，若不一致则“差异”为1，训练误差是在训练集上差异的平均值，而泛化误差则是在新样本（训练集中未出现过的样本）上差异的平均值。过拟合是由于模型的学习能力相对于数据来说过于强大，反过来说，欠拟合是因为模型的学习能力相对于数据来说过于低下。暂且抛开“没有免费的午餐”定理不谈，例如对于“西瓜书”第1章图1.4中的训练样本（黑点）来说，用类似于抛物线的曲线A去拟合则较为合理，而比较崎岖的曲线B相对于训练样本来说学习能力过于强大，但若仅用一条直线去训练则相对于训练样本来说直线的学习能力过于低下。2.2评估方法本节介绍了3种模型评估方法：留出法、交叉验证法、自助法。留出法由于操作简单，因此最常用；交叉验证法常用于对比同一算法的不同参数配置之间的效果，以及对比不同算法之间的效果；自助法常用于集成学习（详见“西瓜书”第8章的8.2节和8.3节）产生基分类器。留出法和自助法简单易懂，在此不再赘述，下面举例说明交叉验证法的常用方式。对比同一算法的不同参数配置之间的效果：假设现有数据集D，且有一个被评估认为适合用于数据集D的算法L，该算法有可配置的参数，假设备选的参数配置方案有两套：方案a，方案b。下面通过交叉验证法为算法L筛选出在数据集D上效果最好的参数配置方案。以3折交叉验证为例，首先按照“西瓜书”中所说的方法，通过分层采样将数据集D划分为3个大小相似的互斥子集：D1,D2,D3，然后分别用其中1个子集作为测试集，其他子集作为训练集，这样就可获得3组训练集和测试集：训练集1：D1∪D2，测试集1:D3训练集2：D1∪D3，测试集2:D2训练集3：D2∪D3，测试集3:D1接下来用算法L搭配方案a在训练集1上进行训练，训练结束后将训练得到的模型在测试集1上进行测试，得到测试结果1，依此方法再分别通过训练集2和测试集2、训练集3和测试集3得到测试结果'),\n",
       " Document(metadata={'author': '', 'cch_type': 0, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 150, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='•研究某任务在什么样的条件下可学得较好的模型？（定义12.2）•某算法在什么样的条件下可进行有效的学习?（定义12.3）•需多少训练样例才能获得较好的模型？（定义12.4）有限假设空间指H中包含的假设个数是有限的,反之则为无限假设空间;无限假设空间更为常见,例如能够将图5.4(a)(b)(c)中的正例和反例样本分开的线性超平面个数是无限多的。12.2.1式(12.9)的解释PAC辨识的定义：E(h)表示算法L在用观测集D训练后输出的假设函数h，它的泛化误差(见公式12.1)。这个概率定义指出，如果h的泛化误差不大于ϵ的概率不小于1−δ，那么我们称学习算法L能从假设空间H中PAC辨识概念类C。12.3有限假设空间本节内容分两部分,第1部分“可分情形”时,可以达到经验误差bE(h)=0,做的事情是以1−δ概率学得目标概念的ϵ近似,即式(12.12);第2部分“不可分情形”时,无法达到经验误差bE(h)=0,做的事情是以1−δ概率学得minh∈HE(h)的ϵ近似,即式(12.20)。无论哪种情形,对于h∈H,可以得到该假设的泛化误差E(h)与经验误差bE(h)的关系,即“当样例数目m较大时,h的经验误差是泛化误差很好的近似”,即式(12.18);实际研究中经常需要推导类似的泛化误差上下界。从式12.10到式12.14的公式是为了回答一个问题：到底需要多少样例才能学得目标概念c的有效近似。只要训练集D的规模能使学习算法L以概率1−δ找到目标假设的ϵ近似即可。下面就是用数学公式进行抽象。12.3.1式(12.10)的解释P(h(x)=y)=1−P(h(x)̸=y)因为它们是对立事件，P(h(x)̸=y)=E(h)是泛化误差的定义(见12.1)，由于我们假定了泛化误差E(h)>ϵ，因此有1−E(h)<1−ϵ。12.3.2式(12.11)的解释先解释什么是h与D“表现一致”，12.2节开头阐述了这样的概念，如果h能将D中所有样本按与真实标记一致的方式完全分开，我们称问题对学习算法是一致的。即(h(x1)=y1)∧...∧(h(xm)=ym)为True。因为每个事件是独立的，所以上式可以写成P((h(x1)=y1)∧...∧(h(xm)=ym))=Qmi=1P(h(xi)=yi)。根据对立事件的定义有：Qmi=1P(h(xi)=yi)=Qmi=1(1−P(h(xi)̸=yi))，又根据公式(12.10)，有mYi=1(1−P(h(xi)̸=yi))<mYi=1(1−ϵ)=(1−ϵ)m12.3.3式(12.12)的推导首先解释为什么”我们事先并不知道学习算法L会输出H中的哪个假设“，因为一些学习算法对用一个观察集D的输出结果是非确定的，比如感知机就是个典型的例子，训练样本的顺序也会影响感知机学习到的假设h参数的值。泛化误差大于ϵ且经验误差为0的假设(即在训练集上表现完美的假设)出现的概率可以表示为P(h∈H:E(h)>ϵ∧bE(h)=0)，根据式12.11，每一个这样的假设h都满足P(E(h)>ϵ∧bE(h)=0)<(1−ϵ)m，假设一共有|H|这么多个这样的假设h，因为每个假设h满足'),\n",
       " Document(metadata={'author': '', 'cch_type': 0, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 53, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='第5章神经网络神经网络类算法可以堪称当今最主流的一类机器学习算法，其本质上和前几章讲到的线性回归、对数几率回归、决策树等算法一样均属于机器学习算法，也是被发明用来完成分类和回归等任务。不过由于神经网络类算法在如今超强算力的加持下效果表现极其出色，且从理论角度来说神经网络层堆叠得越深其效果越好，因此也单独称用深层神经网络类算法所做的机器学习为深度学习，属于机器学习的子集。5.1神经元模型本节对神经元模型的介绍通俗易懂，在此不再赘述。本节第2段提到“阈值”(threshold)的概念时，“西瓜书”左侧边注特意强调是“阈(yù)”而不是“阀(fá)”，这是因为该字确实很容易认错，读者注意一下即可。图5.1所示的M-P神经元模型，其中的“M-P”便是两位作者McCulloch和Pitts的首字母简写。5.2感知机与多层网络5.2.1式(5.1)和式(5.2)的推导此式是感知机学习算法中的参数更新公式，下面依次给出感知机模型、学习策略和学习算法的具体介绍[1]：感知机模型：已知感知机由两层神经元组成，故感知机模型的公式可表示为y=fnXi=1wixi−θ!=f(wTx−θ)其中，x∈Rn，为样本的特征向量，是感知机模型的输入；w,θ是感知机模型的参数，w∈Rn，为权重，θ为阈值。假定f为阶跃函数，那么感知机模型的公式可进一步表示为（用ε(·)代表阶跃函数）y=ε(wTx−θ)=(1,wTx−θ⩾0;0,wTx−θ<0.由于n维空间中的超平面方程为w1x1+w2x2+···+wnxn+b=wTx+b=0所以此时感知机模型公式中的wTx−θ可以看作是n维空间中的一个超平面，将n维空间划分为wTx−θ⩾0和wTx−θ<0两个子空间，落在前一个子空间的样本对应的模型输出值为1，落在后一个子空间的样本对应的模型输出值为0，如此便实现了分类功能。感知机学习策略：给定一个数据集T={(x1,y1),(x2,y2),···,(xN,yN)}其中xi∈Rn,yi∈{0,1},i=1,2,···,N。如果存在某个超平面wTx+b=0能将数据集T中的正样本和负样本完全正确地划分到超平面两侧，即对所有yi=1的样本xi有wTxi+b⩾0，对所有yi=0的样本xi有wTxi+b<0，则称数据集T线性可分，否则称数据集T线性不可分。现给定一个线性可分的数据集T，感知机的学习目标是求得能对数据集T中的正负样本完全正确划分的分离超平面wTx−θ=0'),\n",
       " Document(metadata={'author': '', 'cch_type': 0, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 148, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='第12章计算学习理论正如本章开篇所述，计算学习理论研究目的是分析学习任务的困难本质，为学习算法提供理论保证，并根据分析结果指导算法设计。例如，“西瓜书”定理12.1、定理12.3、定理12.6所表达意思的共同点是，泛化误差与经验误差之差的绝对值以很大概率(1−δ)很小，且这个差的绝对值随着训练样本个数(m)的增加而减小，随着模型复杂度（定理12.1为假设空间包含的假设个数|H|，定理12.3中为假设空间的VC维，定理12.6中为(经验)Rademacher复杂度）的减小而减小。因此，若想要得到一个泛化误差很小的模型，足够的训练样本是前提，最小化经验误差是实现途径，另外还要选择性能相同的模型中模型复杂度最低的那一个；“最小化经验误差”即常说的经验风险最小化，“选择模型复杂度最低的那一个”即结构风险最小化，可以参见“西瓜书”6.4节最后一段的描述，尤其是式(6.42)所表达的含义。12.1基础知识统计学中有总体集合和样本集合之分,比如要统计国内本科生对机器学习的掌握情况,此时全国所有的本科生就是总体集合,但总体集合往往太大而不具有实际可操作性,一般都是取总体集合的一部分,比如从双一流A类、双一流B类、一流学科建设高校、普通高校中各找一部分学生(即样本集合)进行调研,以此来了解国内本科生对机器学习的掌握情况。在机器学习中,样本空间(参见1.2节)对应总体集合,而我们手头上的样例集D对应样本集合,样例集D是从样本空间中采样而得,分布D可理解为当从样本空间采样获得样例集D时每个样本被采到的概率,我们用D(t)表示样本空间第t个样本被采到的概率。12.1.1式(12.1)的解释该式为泛化误差的定义式，所谓泛化误差，是指当样本x从真实的样本分布D中采样后其预测值h(x)不等于真实值y的概率。在现实世界中，我们很难获得样本分布D，我们拿到的数据集可以看做是从样本分布D中独立同分布采样得到的。在西瓜书中，我们拿到的数据集，称为样例集D[也叫观测集、样本集，注意与花体D的区别]。12.1.2式(12.2)的解释该式为经验误差的定义式，所谓经验误差，是指观测集D中的样本xi,i=1,2,···,m的预测值h(xi)和真实值yi的期望误差。12.1.3式(12.3)的解释假设我们有两个模型h1和h2，将它们同时作用于样本x上，那么他们的”不合“度定义为这两个模型预测值不相同的概率。12.1.4式(12.4)的解释Jensen不等式：这个式子可以做很直观的理解，比如说在二维空间上，凸函数可以想象成开口向上的抛物线，假如我们有两个点x1,x2，那么f(E(x))表示的是两个点的均值的纵坐标，而E(f(x))表示的是两个点纵坐标的均值，因为两个点的均值落在抛物线的凹处，所以均值的纵坐标会小一些。12.1.5式(12.5)的解释随机变量的观测值是随机的,进一步地,随机过程的每个时刻都是一个随机变量。式中,1mPmi=1xi表示m个独立随机变量各自的某次观测值的平均,1mPmi=1E(xi)表示m个独立随机变量各自的期望的平均。'),\n",
       " Document(metadata={'author': '', 'cch_type': 0, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 14, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='就是所谓的真相，所以也将其称为“假设”。通常机器学习算法都有可配置的参数，同一个机器学习算法，使用不同的参数配置或者不同的训练集，训练得到的模型通常都不同。标记：上文提到机器学习的本质就是在学习样本在某个方面的表现是否存在潜在的规律，我们称该方面的信息为“标记”。例如在学习西瓜的好坏时，“好瓜”和“坏瓜”便是样本的标记。一般第i个样本的标记的数学表示为yi，标记所在的空间称为“标记空间”或“输出空间”，数学表示为花式大写的Y。标记通常也看作为样本的一部分，因此，一个完整的样本通常表示为(x,y)。根据标记的取值类型不同，可将机器学习任务分为以下两类：•当标记取值为离散型时，称此类任务为“分类”，例如学习西瓜是好瓜还是坏瓜、学习猫的图片是白猫还是黑猫等。当分类的类别只有两个时，称此类任务为“二分类”，通常称其中一个为“正类”，另一个为“反类”或“负类”；当分类的类别超过两个时，称此类任务为“多分类”。由于标记也属于样本的一部分，通常也需要参与运算，因此也需要将其数值化，例如对于二分类任务，通常将正类记为1，反类记为0，即Y={0,1}。这只是一般默认的做法，具体标记该如何数值化可根据具体机器学习算法进行相应地调整，例如第6章的支持向量机算法则采用的是Y={−1,+1}；•当标记取值为连续型时，称此类任务为“回归”，例如学习预测西瓜的成熟度、学习预测未来的房价等。由于是连续型，因此标记的所有可能取值无法直接罗列，通常只有取值范围，回归任务的标记取值范围通常是整个实数域R，即Y=R。无论是分类还是回归，机器学习算法最终学得的模型都可以抽象地看作为以样本x为自变量，标记y为因变量的函数y=f(x)，即一个从输入空间X到输出空间Y的映射。例如在学习西瓜的好坏时，机器学习算法学得的模型可看作为一个函数f(x)，给定任意一个西瓜样本xi=(青绿;蜷缩;清脆)，将其输入进函数即可计算得到一个输出yi=f(xi)，此时得到的yi便是模型给出的预测结果，当yi取值为1时表明模型认为西瓜xi是好瓜，当yi取值为0时表明模型认为西瓜xi是坏瓜。根据是否有用到标记信息，可将机器学习任务分为以下两类：•在模型训练阶段有用到标记信息时，称此类任务为“监督学习”，例如第3章的线性模型；•在模型训练阶段没用到标记信息时，称此类任务为“无监督学习”，例如第9章的聚类。泛化：由于机器学习的目标是根据已知来对未知做出尽可能准确的判断，因此对未知事物判断的准确与否才是衡量一个模型好坏的关键，我们称此为“泛化”能力。例如学习西瓜好坏时，假设训练集中共有3个样本：{(x1=(青绿;蜷缩),y1=好瓜),(x2=(乌黑;蜷缩),y2=好瓜),(x3=(浅白;蜷缩),y3=好瓜)}，同时假设判断西瓜好坏的真相是“只要根蒂蜷缩就是好瓜”，如果应用算法A在此训练集上训练得到模型fa(x)，模型a学到的规律是“色泽等于青绿、乌黑或者浅白时，同时根蒂蜷缩即为好瓜，否则便是坏瓜”，再应用算法B在此训练集上训练得到模型fb(x)，模型fb(x)学到的规律是“只要根蒂蜷缩就是好瓜”，因此对于一个未见过的西瓜样本x=(金黄;蜷缩)来说，模型fa(x)给出的预测结果为“坏瓜”，模型fb(x)给出的预测结果为“好瓜”，此时我们称模型fb(x)的泛化能力优于模型fa(x)。通过以上举例可知，尽管模型fa(x)和模型fb(x)对训练集学得一样好，即两个模型对训练集中每个样本的判断都对，但是其所学到的规律是不同的。导致此现象最直接的原因是算法的不同，但是算法通常是有限的，可穷举的，尤其是在特定任务场景下可使用的算法更是有限，因此，数据便是导致此现象的另一重要原因，这也就是机器学习领域常说的“数据决定模型的上限，而算法则是让模型无限逼近上限”,下面详细解释此话的含义。先解释“数据决定模型效果的上限”，其中数据是指从数据量和特征工程两个角度考虑。从数据量的角度来说，通常数据量越大模型效果越好，因为数据量大即表示累计的经验多，因此模型学习到的经验也多，自然表现效果越好。例如以上举例中如果训练集中含有相同颜色但根蒂不蜷缩的坏瓜，模型a学到真相的概率则也会增大；从特征工程的角度来说，通常对特征数值化越合理，特征收集越全越细致，模型效果通常越好，因为此时模型更易学得样本之间潜在的规律。例如学习区分亚洲人和非洲人时，此时样本即'),\n",
       " Document(metadata={'author': '', 'cch_type': 0, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 161, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='析可以获得与算法有关的分析结果。算法的“稳定性”考察的是算法在输入发生变化时，输出是否会随之发生较大的变化。12.7.1泛化/经验/留一损失的解释根据式(12.54)上方关于损失函数的描述：“刻画了假设的预测标记与真实标记之间的差别”，这里针对的是二分类，预测标记和真实标记均只能取和两个值，它们之间的“差别”又能是什么呢？因此，当“差别”取为时，式(12.54)的泛化损失就是式(12.1)的泛化误差，式(12.55)的经验损失就是式(12.2)的经验误差，如果类似于式(12.1)和式(12.2)继续定义留一误差，那么式(12.56)就对应于留一误差。12.7.2式(12.57)的解释根据三角不等式[7]，有|a+b|≤|a|+|b|，将a=ℓ(LD,z)−ℓ(LDi)，b=ℓ(LDi,z)−ℓ\\x00LD\\\\i,z\\x01带入即可得出第一个不等式，根据D\\\\i表示移除D中第i个样本，Di表示替换D中第i个样本，那么a,b的变动均为一个样本，根据式12.57，a⩽β,b⩽β，因此a+b⩽2β。12.7.3定理12.8的解释西瓜书在该定理下方已明确给出该定理的意义,即“定理12.8给出了基于稳定性分析推导出的学习算法L学得假设的泛化误差界”,式(12.58)和式(12.59)分别基于经验损失和留一损失给出了泛化损失的上界。接下来讨论两个相关问题:(1)定理12.8的条件包括损失函数有界,即0⩽ℓ(LD,z)⩽M;如本节第1条注解“泛化/经验/留一损失的解释”中所述,若“差别”取为I(LD(x),y),则泛化损失对应于泛化误差,此时上限M=1。(2)在前面泛化误差上界的推导中（例如定理12.1、定理12.3、定理12.6、定理12.7),上界中与样本数m有关的项收玫率均为O(1/√m),但在该定理中却是O(β√m);一般来讲,随着样本数m的增加,经验误差/损失应该收玫于泛化误差/损失,因此这里假设β=1/m(书中式(12.59)下方第3行写为β=O(1/m)),而在第2条注解“定义12.10的解释”中已经提到β的取值的确会随着样本数m的增多会变小,虽然书中并没有严格去讨论β随m增多的变化规律,但至少直觉上是对的。12.7.4式(12.60)的推导将β=1m带入至式(12.58)即得证。12.7.5经验损失最小化顾名思义,“经验损失最小化”指通过最小化经验损失来求得假设函数。这里,“对于损失函数ℓ,若学习算法L所输出的假设满足经验损失最小化,则称算法L满足经验风险最小化原则,简称算法是ERM的”。在”西瓜书”第278页,若学习算法L输出的假设h满足式(12.30),则也称L为满足经验风险最小化原则的算法。而很明显,式(12.30)是在最小化经验误差。那么最小化经验误差和最小化经验损失有什么区别么?在”西瓜书“第286页左下角边注中提到,“最小化经验误差和最小化经验损失有时并不相同,这是由于存在某些病态的损失函数ℓ使得最小化经验损失并不是最小化经验误差”。对于“误差”、“损失”、“风险”等概念的辨析，参见“西瓜书”第2章2.1节的注解。'),\n",
       " Document(metadata={'author': '', 'cch_type': 0, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 74, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='第7章贝叶斯分类器本章是从概率框架下的贝叶斯视角给出机器学习问题的建模方法，不同于前几章着重于算法具体实现，本章的理论性会更强。朴素贝叶斯算法常用于文本分类，例如用于广告邮件检测，贝叶斯网和EM算法均属于概率图模型的范畴，因此可合并至第14章一起学习。7.1贝叶斯决策论7.1.1式(7.5)的推导由式(7.1)和式(7.4)可得R(ci|x)=1∗P(c1|x)+...+1∗P(ci−1|x)+0∗P(ci|x)+1∗P(ci+1|x)+...+1∗P(cN|x)又PNj=1P(cj|x)=1，则R(ci|x)=1−P(ci|x)此即式(7.5）。7.1.2式(7.6)的推导将式(7.5)代入式(7.3)即可推得此式7.1.3判别式模型与生成式模型对于判别式模型来说，就是在已知x的条件下判别其类别标记c，即求后验概率P(c|x)，前几章介绍的模型都属于判别式模型的范畴，尤其是对数几率回归最为直接明了，式(3.23)和式(3.24)直接就是后验概率的形式。对于生成式模型来说，理解起来比较抽象，但是可通过思考以下两个问题来理解。(1)对于数据集来说，其中的样本是如何生成的？通常假设数据集中的样本服从独立同分布，即每个样本都是按照联合概率分布P(x,c)采样而得，也可以描述为根据P(x,c)生成的。(2)若已知样本x和联合概率分布P(x,c)，如何预测类别呢？若样本x和联合概率分布P(x,c)已知，则可以分别求出x属于各个类别的概率，即P(x,c1),P(x,c2),...,P(x,cN)，然后选择概率最大的类别作为样本x的预测结果。因此，之所以称为“生成式”模型，是因为所求的概率P(x,c)是生成样本x的概率。7.2极大似然估计7.2.1式(7.12)和(7.13)的推导根据式(7.11)和式(7.10)可知参数求解式为ˆθc=argmaxθcLL(θc)=argminθc−LL(θc)=argminθc−Xx∈DclogP(x|θc)由“西瓜书”上下文可知，此时假设概率密度函数p(x|c)∼N(µc,σ2c)，其等价于假设P(x|θc)=P\\x00x|µc,σ2c\\x01=1p(2π)d|Σc|exp\\x12−12(x−µc)TΣ−1c(x−µc)\\x13'),\n",
       " Document(metadata={'author': '', 'cch_type': 0, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 58, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='5.6深度学习“西瓜书”在本节并未对如今深度学习领域的诸多经典神经网络作展开介绍，而是从更宏观的角度详细解释了应该如何理解深度学习。因此，本书也顺着“西瓜书”的思路对深度学习相关概念作进一步说明，对深度学习的经典神经网络感兴趣的读者可查阅其他相关书籍进行系统性学习。5.6.1什么是深度学习深度学习就是很深层的神经网络，而神经网络属于机器学习算法的范畴，因此深度学习是机器学习的子集。5.6.2深度学习的起源深度学习中的经典神经网络以及用于训练神经网络的BP算法其实在很早就已经被提出，例如卷积神经网络[2]是在1989提出，BP算法[3]是在1986年提出，但是在当时的计算机算力水平下，其他非神经网络类算法（例如当时红极一时的支持向量机算法）的效果优于神经网络类算法，因此神经网络类算法进入瓶颈期。随着计算机算力的不断提升，以及2012年Hinton和他的学生提出了AlexNet并在ImageNet评测中以明显优于第二名的成绩夺冠后，引起了学术界和工业界的广泛关注，紧接着三位深度学习之父LeCun、Bengio和Hinton在2015年正式提出深度学习的概念，自此深度学习开始成为机器学习的主流研究方向。5.6.3怎么理解特征学习举例来说，用非深度学习算法做西瓜分类时，首先需要人工设计西瓜的各个特征，比如根蒂、色泽等，然后将其表示为数学向量，这些过程统称为“特征工程”，完成特征工程后用算法分类即可，其分类效果很大程度上取决于特征工程做得是否够好。而对于深度学习算法来说，只需将西瓜的图片表示为数学向量输入，输出层设置为想要的分类结果即可（例如二分类通常设置为对数几率回归），之前的“特征工程”交由神经网络来自动完成，即让神经网络进行“特征学习”，通过在输出层约束分类结果，神经网络会自动从西瓜的图片上提取出有助于西瓜分类的特征。因此，如果分别用对数几率回归和卷积神经网络来做西瓜分类，其算法运行流程分别是“人工特征工程→对数几率回归分类”和“卷积神经网络特征学习→对数几率回归分类”。参考文献[1]李航.统计学习方法.清华大学出版社,2012.[2]YannLeCun,BernhardBoser,JohnSDenker,DonnieHenderson,RichardEHoward,WayneHub-bard,andLawrenceDJackel.Backpropagationappliedtohandwrittenzipcoderecognition.Neuralcomputation,1(4):541–551,1989.[3]DavidERumelhart,GeoffreyEHinton,andRonaldJWilliams.Learningrepresentationsbyback-propagatingerrors.nature,323(6088):533–536,1986.'),\n",
       " Document(metadata={'author': '', 'cch_type': 0, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 30, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='第3章线性模型作为“西瓜书”介绍机器学习模型的开篇，线性模型也是机器学习中最为基础的模型，很多复杂模型均可认为由线性模型衍生而得，无论是曾经红极一时的支持向量机还是如今万众瞩目的神经网络，其中都有线性模型的影子。本章的线性回归和对数几率回归分别是回归和分类任务上常用的算法，因此属于重点内容，线性判别分析不常用，但是其核心思路和后续第10章将会讲到的经典降维算法主成分分析相同，因此也属于重点内容，且两者结合在一起看理解会更深刻。3.1基本形式第1章的1.2基本术语中讲述样本的定义时，我们说明了“西瓜书”和本书中向量的写法，当向量中的元素用分号“;”分隔时表示此向量为列向量，用逗号“,”分隔时表示为行向量。因此，式(3.2)中w=(w1;w2;...;wd)和x=(x1;x2;...;xd)均为d行1列的列向量。3.2线性回归3.2.1属性数值化为了能进行数学运算，样本中的非数值类属性都需要进行数值化。对于存在“序”关系的属性，可通过连续化将其转化为带有相对大小关系的连续值；对于不存在“序”关系的属性，可根据属性取值将其拆解为多个属性，例如“西瓜书”中所说的“瓜类”属性，可将其拆解为“是否是西瓜”、“是否是南瓜”、“是否是黄瓜”3个属性，其中每个属性的取值为1或0，1表示“是”，0表示“否”。具体地，假如现有3个瓜类样本：x1=(甜度=高;瓜类=西瓜),x2=(甜度=中;瓜类=南瓜),x3=(甜度=低;瓜类=黄瓜)，其中“甜度”属性存在序关系，因此可将“高”、“中”、“低”转化为{1.0,0.5,0.0}，“瓜类”属性不存在序关系，则按照上述方法进行拆解，3个瓜类样本数值化后的结果为：x1=(1.0;1;0;0),x1=(0.5;0;1;0),x1=(0.0;0;0;1)。以上针对样本属性所进行的处理工作便是第1章1.2基本术语中提到的“特征工程”范畴，完成属性数值化以后通常还会进行缺失值处理、规范化、降维等一系列处理工作。由于特征工程属于算法实践过程中需要掌握的内容，待学完机器学习算法以后，再进一步学习特征工程相关知识即可，在此先不展开。3.2.2式(3.4)的解释下面仅针对式(3.4)中的数学符号进行解释。首先解释一下符号“argmin”，其中“arg”是“argument”（参数）的前三个字母，“min”是“minimum”（最小值）的前三个字母，该符号表示求使目标函数达到最小值的参数取值。例如式(3.4)表示求出使目标函数Pmi=1(yi−wxi−b)2达到最小值的参数取值(w∗,b∗)，注意目标函数是以(w,b)为自变量的函数，(xi,yi)均是已知常量，即训练集中的样本数据。类似的符号还有“min”，例如将式(3.4)改为min(w,b)mXi=1(yi−wxi−b)2则表示求目标函数的最小值。对比知道，“min”和“argmin”的区别在于，前者输出目标函数的最小值，而后者输出使得目标函数达到最小值时的参数取值。若进一步修改式(3.4)为min(w,b)mXi=1(yi−wxi−b)2s.t.w>0,b<0.'),\n",
       " Document(metadata={'author': '', 'cch_type': 0, 'chunk_overlap': 400, 'chunk_size': 2000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 89, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='8.2Boosting注意8.1节最后一段提到：根据个体学习器的生成方式，目前的集成学习方法大致可分为两大类，即个体学习器间存在强依赖关系、必须串行生成的序列化方法，以及个体学习器间不存在强依赖关系、可同时生成的并行化方法。本节Boosting为前者的代表，Adaboost又是Boosting族算法的代表。8.2.1式(8.4)的解释这个式子是集成学习的加性模型，加性模型不采用梯度下降的思想，而是H(x)=PT−1t=1αtht(x)+αThT(x)，共迭代T次，每次更新求解一个理论上最优的hT和αT。hT和αT的定义参见式(8.18)和式(8.11)8.2.2式(8.5)的解释先考虑指数损失函数e−f(x)H(x)的含义参见“西瓜书”图6.5：f为真实函数，对于样本x来说，f(x)∈{+1,−1}只能取+1和−1，而H(x)是一个实数。当H(x)的符号与f(x)一致时，f(x)H(x)>0，因此e−f(x)H(x)=e−|H(x)|<1，且|H(x)|越大指数损失函数e−f(x)H(x)越小。这很合理：此时|H(x)|越大意味着分类器本身对预测结果的信心越大，损失应该越小；若|H(x)|在零附近，虽然预测正确，但表示分类器本身对预测结果信心很小，损失应该较大；当H(x)的符号与f(x)不一致时，f(x)H(x)<0，因此e−f(x)H(x)=e|H(x)|>1，且|H(x)|越大指数损失函数越大。这很合理：此时|H(x)|越大意味着分类器本身对预测结果的信心越大，但预测结果是错的，因此损失应该越大；若|H(x)|在零附近，虽然预测错误，但表示分类器本身对预测结果信心很小，虽然错了，损失应该较小。再解释符号Ex∼D[·]的含义：D为概率分布，可简单理解为在数据集D中进行一次随机抽样，每个样本被取到的概率；E[·]为经典的期望，则综合起来Ex∼D[·]表示在概率分布D上的期望，可简单理解为对数据集D以概率D进行加权后的期望。综上所述,若数据集D中样本x的权值分布为D(x),则式(8.5)可写为:ℓexp(H|D)=Ex∼D\\x02e−f(x)H(x)\\x03=Xx∈DD(x)e−f(x)H(x)=Xx∈DD(x)\\x00e−H(x)I(f(x)=1)+eH(x)I(f(x)=−1)\\x01特别地,若针对任意样本x,若分布D(x)=1|D|,其中|D|为数据集D样本个数,则ℓexp(H|D)=Ex∼D\\x02e−f(x)H(x)\\x03=1|D|Xx∈De−f(x)H(x)而这就是在求传统平均值。')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检索未使用了cch技术的的记录，返回top10\n",
    "test_query = qa_pairs[test_doc]['query']\n",
    "results = retriever.vectorstore.similarity_search(test_query, k=10, filter=wo_cch_filter_condition)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 效果测评"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def test_wo_cch(k=10):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for qa_pair in tqdm(qa_pairs):\n",
    "        if len(qa_pair['query']) > 10:\n",
    "            query = qa_pair['query']\n",
    "            sim_docs = retriever.vectorstore.similarity_search(query, k=k, filter=wo_cch_filter_condition)\n",
    "            page_nums = [doc.metadata['page'] for doc in sim_docs]\n",
    "            if qa_pair['page_num'] in page_nums: i += 1\n",
    "            j += 1\n",
    "    return i/j * 100\n",
    "\n",
    "def test_w_cch(k=10):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for qa_pair in tqdm(qa_pairs):\n",
    "        if len(qa_pair['query']) > 10:\n",
    "            query = qa_pair['query']\n",
    "            sim_docs = retriever.vectorstore.similarity_search(query, k=k, filter=w_cch_filter_condition)\n",
    "            page_nums = [doc.metadata['page'] for doc in sim_docs]\n",
    "            if qa_pair['page_num'] in page_nums: i += 1\n",
    "            j += 1\n",
    "    return i/j * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:02<00:00, 41.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84.54545454545455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:02<00:00, 44.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.27272727272727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 不使用CCH\n",
    "print(test_wo_cch())\n",
    "\n",
    "# 使用CCH\n",
    "print(test_w_cch())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从返回的top10的命中上来看，使用CCH能一定程度提升召回率。下面我们将更加系统测试该技术带来的提升，使用k为1、3、5、7、10分别进行统计召回率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:02<00:00, 44.86it/s]\n",
      "100%|██████████| 119/119 [00:02<00:00, 47.11it/s]\n",
      "100%|██████████| 119/119 [00:02<00:00, 46.54it/s]\n",
      "100%|██████████| 119/119 [00:02<00:00, 47.09it/s]\n",
      "100%|██████████| 119/119 [00:02<00:00, 47.07it/s]\n",
      "100%|██████████| 119/119 [00:02<00:00, 47.27it/s]\n",
      "100%|██████████| 119/119 [00:02<00:00, 47.25it/s]\n",
      "100%|██████████| 119/119 [00:02<00:00, 47.23it/s]\n",
      "100%|██████████| 119/119 [00:02<00:00, 46.00it/s]\n",
      "100%|██████████| 119/119 [00:02<00:00, 47.80it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "k = [1, 3, 5, 7, 10]\n",
    "results = {'k': [], 'wo_cch': [], 'w_cch': []}\n",
    "\n",
    "for k_ in k:\n",
    "    wo_cch_recall = test_wo_cch(k=k_)\n",
    "    w_cch_recall = test_w_cch(k=k_)\n",
    "    results['k'].append(k_)\n",
    "    results['wo_cch'].append(wo_cch_recall)\n",
    "    results['w_cch'].append(w_cch_recall)\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>wo_cch</th>\n",
       "      <th>w_cch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60.909091</td>\n",
       "      <td>62.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>72.727273</td>\n",
       "      <td>78.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>77.272727</td>\n",
       "      <td>81.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>82.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>84.545455</td>\n",
       "      <td>87.272727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    k     wo_cch      w_cch\n",
       "0   1  60.909091  62.727273\n",
       "1   3  72.727273  78.181818\n",
       "2   5  77.272727  81.818182\n",
       "3   7  80.000000  82.727273\n",
       "4  10  84.545455  87.272727"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
