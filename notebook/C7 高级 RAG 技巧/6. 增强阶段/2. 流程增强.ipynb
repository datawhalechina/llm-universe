{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea471d41",
   "metadata": {},
   "source": [
    "# 流程增强"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38077160",
   "metadata": {},
   "source": [
    "简单的 RAG 流程就像一条单行道，只能进行一次检索和生成。然而，现实世界的问题往往是复杂多变的，需要我们不断地探索和迭代。为了解决这个问题，我们需要对 RAG 流程进行增强，使其能够更灵活地适应不同的查询和场景。流程增强的目标是打破单行道的限制，建立一个动态的、可迭代的 RAG 流程，从而提高 RAG 系统的整体性能。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63183e45-e087-47ee-afd3-f718cb9037ae",
   "metadata": {},
   "source": [
    "## 一、Self-RAG高级检索\n",
    "\n",
    "Self-RAG代表了一种创新的框架，它利用自我反思的标记来对语言模型（LM）进行训练和调控。这个框架的核心流程可以划分为三个主要环节：信息检索、内容生成和内容评估。\n",
    "\n",
    "![selfrag](./figures/selfrag.png)\n",
    "\n",
    "\n",
    "1. 在信息检索环节，Self-RAG首先对检索标记进行解码，以判断当前情境下是否有必要进行信息的检索，并据此控制检索机制。如果确定需要检索，LM便会激活外部的检索模块，以便搜寻与当前任务相关的文档资料。\n",
    "\n",
    "2. 进入内容生成阶段，若无需额外检索，LM将直接预测接下来的文本片段。若检索是必要的，LM会先生成一系列批评标记，这些标记的作用是评估已检索到的文档资料的相关性，并基于这些资料来构建后续的内容。\n",
    "\n",
    "3. 在内容评估环节，LM进一步评价检索到的文档对生成内容的支持程度。最终，通过一个新的批评标记来对整体的响应效果进行评估。\n",
    "\n",
    "![selftoken](./figures/selftoken.jpg)\n",
    "\n",
    "在训练Self-RAG的过程中，涉及到三个关键的模型组件：检索器、批评家和生成器。首先，批评家模型会被训练，它利用检索器找到的文档和增强的反思标记来训练。接着，生成器LM通过标准的下一个标记预测任务进行训练，目的是掌握如何生成自然连贯的文本以及如何使用特殊的标记来检索或评估自身的生成内容。\n",
    "\n",
    "在推理应用中，Self-RAG通过掌握生成反思标记的能力，能够为多种不同的下游任务或特定需求定制模型的行为，而无需对LMs进行额外的训练。它能够根据检索标记灵活地决定何时进行检索，让模型自行判断是否需要进行信息检索。此外，Self-RAG引入了多种细致的批评标记，用于评估生成内容的各种质量维度。在生成过程中，研究者采用了基于期望批评标记概率的线性插值方法，进行分段级别的束搜索（beam search），以便在每个时间节点确定最佳的续写方案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c100738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"sk-你的sk\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3385830b",
   "metadata": {},
   "source": [
    "首先构建一个字典，包含文件的八个标准问答对，key是问题，value是期望rag的答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d90f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_dict = {\n",
    "        \"介绍下SVM算法\": \"是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机\",\n",
    "        \"用通俗的语言介绍下强化学习\": \"监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略\",\n",
    "        \"BN 和 LN 区别\": \"Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。\",\n",
    "        \"讲讲self attention\": \"Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系\",\n",
    "        \"Bert 的预训练过程\": \"Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。\",\n",
    "        \"GPT 与 Bert 的区别\": \"GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。\",\n",
    "        \"pca 属于有监督还是无监督\": \"PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。\",\n",
    "        \"介绍 transformer 算法\": \"Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc1a2d1",
   "metadata": {},
   "source": [
    "构建评估prompt和评估函数：\n",
    "\n",
    "\n",
    "意在让大模型判断生成的结果和期望的结果是否含义一致，在下面使用各种advanecd-rag方法时会经常调用， 主要使用Llama Packs，Llama Packs是对llamaindex的更加高级的封装，几行代码实现各种高级RAG，如果想看具体的流程也可以直接进入github查看源码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb4b67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "import re\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "eval_template = PromptTemplate(\"\"\"\n",
    "            输入:\n",
    "            Question: {question}\n",
    "            LLM-Answer: {llm_answer}\n",
    "            Expected-Answer: {expected_answer}\n",
    "\n",
    "            任务:\n",
    "            比较LLM-Answer和Expected-Answer，确定它们是否传达类似的含义。\n",
    "\n",
    "            输出:\n",
    "            意思一致：如果LLM-Answer和Expected-Answer传达类似的含义。 \n",
    "            意思不同：如果LLM-Answer和Expected-Answer传达完全不同的含义。\n",
    "\n",
    "            示例： \n",
    "            Question：法国的首都是什么？ \n",
    "            LLM-Answer: 巴黎 \n",
    "            Expected-Answer: 光之城\n",
    "\n",
    "            输出:\n",
    "            意思一致\"\"\")\n",
    "    \n",
    "def eval_llm_output(row, llm):\n",
    "    ''' Eval llm_answer and expected_answer for the question with llm '''\n",
    "    question = row['question']\n",
    "    llm_answer = row['llm_answer']\n",
    "    expected_answer = row['expected_answer']\n",
    "    prompt_message = eval_template.format(question=question, llm_answer=llm_answer, expected_answer=expected_answer)\n",
    "    answer = llm.complete(prompt_message).text\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5266d7",
   "metadata": {},
   "source": [
    "初始化 llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0acdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7defe255",
   "metadata": {},
   "source": [
    "载入需要查询的数据文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f629bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PDFReader\n",
    "from pathlib import Path\n",
    "\n",
    "loader = PDFReader()\n",
    "documents = loader.load_data(\n",
    "        file=Path(f\"./data/face.pdf\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9472f390",
   "metadata": {},
   "source": [
    "下载Self-RAG pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7529b87-4918-47f4-a5bd-3ab8de4e56cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "\n",
    "# download and install dependencies\n",
    "SelfRAGPack=download_llama_pack(\n",
    "    \"SelfRAGPack\",\n",
    "    \"D:/AAAAAAAAAAAIGC/self_rag_pack\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b979583-859b-42aa-9c13-b76d6ec86bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document, VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Setup a simple retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43de756f",
   "metadata": {},
   "source": [
    "下载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22e76989-8a97-4caa-86ab-5f71d1f7d144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tmpmodel\\selfrag_llama2_7b.q4_k_m.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
      "downloading https://huggingface.co/m4r1/selfrag_llama2_7b-GGUF/resolve/main/selfrag_llama2_7b.q4_k_m.gguf to C:\\Users\\Alienware\\.cache\\huggingface\\hub\\tmpx3m0nk9s\n"
     ]
    }
   ],
   "source": [
    "download_dir = \"./tmpmodel\"  # Replace\n",
    "!pip3 install -q huggingface-hub\n",
    "!huggingface-cli download m4r1/selfrag_llama2_7b-GGUF selfrag_llama2_7b.q4_k_m.gguf --local-dir {download_dir} --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3816f8fc",
   "metadata": {},
   "source": [
    "初始化 self-Rag 检索器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3efe0e4-e8cc-45aa-a99f-f7edf570a35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./tmpmodel/selfrag_llama2_7b.q4_k_m.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 275/32016 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32016\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3891.33 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_path = \"./tmpmodel/selfrag_llama2_7b.q4_k_m.gguf\"\n",
    "agent_pack = SelfRAGPack(\n",
    "    model_path=model_path, retriever=retriever, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9fa648",
   "metadata": {},
   "source": [
    "用 self-Rag 检索器进行检索。因为 self-Rag 会进行多次判断，所以整个检索的链路相对较长，但是效果会更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e1788f3-5c00-4d0a-8951-c7fc6937fa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      16.82 ms /    40 runs   (    0.42 ms per token,  2378.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2904.54 ms /    22 tokens (  132.02 ms per token,     7.57 tokens per second)\n",
      "llama_print_timings:        eval time =    8948.68 ms /    39 runs   (  229.45 ms per token,     4.36 tokens per second)\n",
      "llama_print_timings:       total time =   12115.75 ms /    61 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieval required\n",
      "\u001b[0m\u001b[1;3;34mReceived: 10 documents\n",
      "\u001b[0m\u001b[1;3;34mStart evaluation\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      21.36 ms /    50 runs   (    0.43 ms per token,  2341.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =   79415.59 ms /   743 tokens (  106.89 ms per token,     9.36 tokens per second)\n",
      "llama_print_timings:        eval time =   11974.64 ms /    49 runs   (  244.38 ms per token,     4.09 tokens per second)\n",
      "llama_print_timings:       total time =   94319.48 ms /   792 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第4页共46页第十七篇：2022年4月10日百度机器学习方向暑期实习面试题6道\n",
      "1、介绍下SVM算法\n",
      "是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感\n",
      "知机。\n",
      "SVM可分为三种：\n",
      "线性可分SVM\n",
      "当训练数据线性可分时，通过最大化硬间隔（hardmargin）可以学习得到一个线性分类器，即硬间隔\n",
      "SVM。\n",
      "线性SVM\n",
      "当训练数据不能线性可分但是近似线性可分时，通过最大化软间隔（softmargin）也可以学习到一个线\n",
      "性分类器，即软间隔SVM。\n",
      "非线性SVM\n",
      "当训练数据线性不可分时，通过使用核技巧（kerneltrick）和最大化软间隔，可以学习到一个非线性\n",
      "SVM。\n",
      "SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损\n",
      "失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。\n",
      "SVM如何选择核函数\n",
      "Linear核：主要用于线性可分的情形。参数少，速度快，对于一般数据，分类效果已经很理想了。\n",
      "RBF核：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交\n",
      "叉验证来寻找合适的参数，不过这个过程比较耗时。\n",
      "2、介绍下逻辑回归算法\n",
      "逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度下降法来求解参数，从而达\n",
      "到将数据二分类的目的。\n",
      "逻辑回归的优缺点</paragraph>\n",
      "Prediction: [Relevant]逻辑回归是一种常见的二分类模型，它的基本模型是定义在特征空间上的一个最大的\n",
      "Score: 0.9285435719763134\n",
      "\u001b[0m\u001b[1;3;34m1/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       4.18 ms /    19 runs   (    0.22 ms per token,  4547.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   89390.15 ms /   861 tokens (  103.82 ms per token,     9.63 tokens per second)\n",
      "llama_print_timings:        eval time =    3332.36 ms /    18 runs   (  185.13 ms per token,     5.40 tokens per second)\n",
      "llama_print_timings:       total time =   95194.60 ms /   879 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第18页共46页第二十一篇：2022年4月14日美团计算机视觉算法暑期实习面试题6道\n",
      "1、简单介绍gbdt算法的原理\n",
      "GBDT是梯度提升决策树，是一种基于Boosting的算法，采用以决策树为基学习器的加法模型，通过不\n",
      "断拟合上一个弱学习器的残差，最终实现分类或回归的模型。关键在于利用损失函数的负梯度在当前模型的值\n",
      "作为残差的近似值，从而拟合一个回归树。对于分类问题：常使用指数损失函数；对于回归问题：常使用平方\n",
      "误差损失函数（此时，其负梯度就是通常意义的残差），对于一般损失函数来说就是残差的近似。\n",
      "无论损失函数是什么形式，每个决策树拟合的都是负梯度。准确的说，不是用负梯度代替残差，而是当损\n",
      "失函数是均方损失时，负梯度刚好是残差，残差只是特例。\n",
      "2、pca属于有监督还是无监督\n",
      "PCA按有监督和无监督划分应该属于无监督学习，所以数据集有无y并不重要，只是改变样本X的属性\n",
      "(特征)维度。\n",
      "3、介绍svm算法\n",
      "是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感\n",
      "知机。\n",
      "SVM可分为三种：\n",
      "线性可分SVM\n",
      "当训练数据线性可分时，通过最大化硬间隔（hardmargin）可以学习得到一个线性分类器，即硬间隔\n",
      "SVM。\n",
      "线性SVM\n",
      "当训练数据不能线性可分但是近似线性可分时，通过最大化软间隔（softmargin）也可以学习到一个线\n",
      "性分类器，即软间隔SVM。\n",
      "非线性SVM\n",
      "当训练数据线性不可分时，通过使用核技巧（kerneltrick）和最大化软间隔，可以学习到一个非线性\n",
      "SVM。</paragraph>\n",
      "Prediction: [Relevant]这里介绍的是线性可分SVM。[Fully supported][Utility:5]\n",
      "Score: 2.3546355045543526\n",
      "\u001b[0m\u001b[1;3;34m2/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       0.92 ms /     4 runs   (    0.23 ms per token,  4362.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   60326.36 ms /   837 tokens (   72.07 ms per token,    13.87 tokens per second)\n",
      "llama_print_timings:        eval time =     576.27 ms /     3 runs   (  192.09 ms per token,     5.21 tokens per second)\n",
      "llama_print_timings:       total time =   62555.82 ms /   840 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第19页共46页SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损\n",
      "失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。\n",
      "SVM如何选择核函数\n",
      "Linear核：主要用于线性可分的情形。参数少，速度快，对于一般数据，分类效果已经很理想了。\n",
      "RBF核：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交\n",
      "叉验证来寻找合适的参数，不过这个过程比较耗时。\n",
      "4、介绍transformer算法\n",
      "Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个Block，\n",
      "Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder端的\n",
      "Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一个前\n",
      "馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和LayerNormalization\n",
      "层。\n",
      "5、layernorm和batchnorm的比较\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "6、Leetcode—两数之和\n",
      "该题较为简单。\n",
      "1.classSolution(object):\n",
      "2. deftwoSum(self,nums,target):\n",
      "3. \"\"\"\n",
      "4. :typenums:List[int]\n",
      "5. :typetarget:int\n",
      "6. :rtype:List[int]\n",
      "7. \"\"\"\n",
      "8. dic={}\n",
      "9. fori,numinenumerate(nums):\n",
      "10. iftarget-numindic:</paragraph>\n",
      "Prediction: [Irrelevant]​[Utility:5]\n",
      "Score: 0.8689226437672928\n",
      "\u001b[0m\u001b[1;3;34m3/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      13.74 ms /    52 runs   (    0.26 ms per token,  3785.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   88326.36 ms /  1148 tokens (   76.94 ms per token,    13.00 tokens per second)\n",
      "llama_print_timings:        eval time =   10331.39 ms /    51 runs   (  202.58 ms per token,     4.94 tokens per second)\n",
      "llama_print_timings:       total time =  101213.51 ms /  1199 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第1页共46页目录\n",
      "第十七篇：2022年4月10日百度机器学习方向暑期实习面试题6道.4\n",
      "1、介绍下SVM算法.4\n",
      "2、介绍下逻辑回归算法.4\n",
      "3、介绍下决策树算法.5\n",
      "4、用通俗的语言介绍下强化学习（ReinforcementLearning）.5\n",
      "5、Leetcode34在排序数组中查找元素的第一个和最后一个位置.5\n",
      "6、Letcode102层序遍历.6\n",
      "第十八篇：2022年4月18日字节跳动机器学习AILab一面面试题6道.8\n",
      "1、BN和LN区别.8\n",
      "2、讲讲selfattention.8\n",
      "3、Bert的预训练过程.8\n",
      "4、PreNorm与PostNorm的区别？8\n",
      "5、GPT与Bert的区别.9\n",
      "6、如何加速Bert模型的训练.9\n",
      "第十九篇：2022年4月8日字节跳动机抖音APP推荐实习面试题8道.11\n",
      "1、AUC是什么？如何计算AUC？11\n",
      "2、AUC线上线下不一致怎么办.11\n",
      "3、召回阶段的负采样是怎么做的？12\n",
      "4、FM，DeepFM跟FFM的对比.12\n",
      "5、手撕FM的训练过程.12\n",
      "6、Leetcode—64.最小路径和.13\n",
      "7、剑指Offer10-I.斐波那契数列.14\n",
      "8、Leetcode—215.数组中的第K个最大元素.14\n",
      "第二十篇：2022年4月携程暑期实习搜索推荐算法岗面试题8道.15\n",
      "1、你所理解的推荐系统是什么样的？大致流程？所用模型？15\n",
      "2、双塔模型优势，缺点，如何改进？15\n",
      "3、粗排的目的是什么？15\n",
      "4、wide&deep模型为什么要有wide层结构，优缺点，如何改进？16\n",
      "5、推荐领域GBDT+LR的做法了解吗？16\n",
      "6、粗排有哪些指标？NDCG了解吗？16\n",
      "7、ROC，PR曲线含义，坐标轴代表什么？17\n",
      "8、AUC怎么求，实际意义？17\n",
      "第二十一篇：2022年4月14日美团计算机视觉算法暑期实习面试题6道.18\n",
      "1、简单介绍gbdt算法的原理.18\n",
      "2、pca属于有监督还是无监督.18\n",
      "3、介绍svm算法.18\n",
      "4、介绍transformer算法.19\n",
      "5、layernorm和batchnorm的比较.19\n",
      "6、Leetcode—两数之和.19\n",
      "第二十二篇：2022年5月18日2023届广联达提前批面试题5道.21\n",
      "1、Transformer中encoder和decoder的区别（结构和功能上）.21\n",
      "2、有哪些防止过拟合的方法.21\n",
      "3、L1和L2正则化为什么可以防止过拟合.21\n",
      "4、传统机器学习方法了解多少.21</paragraph>\n",
      "Prediction: [Relevant]这些问题都是在知识点上相对较广的，尤其是在SVM和transformer算法上，可以通过在线资料进行阅\n",
      "Score: 0.851191696652873\n",
      "\u001b[0m\u001b[1;3;34m4/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.64 ms /    50 runs   (    0.25 ms per token,  3954.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =   59962.88 ms /   800 tokens (   74.95 ms per token,    13.34 tokens per second)\n",
      "llama_print_timings:        eval time =    9394.84 ms /    49 runs   (  191.73 ms per token,     5.22 tokens per second)\n",
      "llama_print_timings:       total time =   71143.90 ms /   849 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第22页共46页形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权\n",
      "重值比较高，那么这个特征最后对结果的影响会比较大。\n",
      "模型效果不错。在工程上是可以接受的（作为baseline），如果特征工程做的好，效果不会太差，并且\n",
      "特征工程可以并行开发，大大加快开发的速度。\n",
      "训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。\n",
      "缺点：\n",
      "准确率欠佳。因为形式非常的简单，而现实中的数据非常复杂，因此，很难达到很高的准确性。很难处理\n",
      "数据不平衡的问题。\n",
      "5、生成式模型和判别式模型的区别并举一些例子\n",
      "生成模型：学习得到联合概率分布P(x,y)，即特征x，共同出现的概率。\n",
      "常见的生成模型：朴素贝叶斯模型，混合高斯模型，HMM模型。\n",
      "判别模型：学习得到条件概率分布P(y|x)，即在特征x出现的情况下标记y出现的概率。\n",
      "常见的判别模型：感知机，决策树，逻辑回归，SVM，CRF等。\n",
      "判别式模型：要确定一个羊是山羊还是绵羊，用判别式模型的方法是从历史数据中学习到模型，然后通过\n",
      "提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。\n",
      "生成式模型：是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模\n",
      "型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，再放到绵羊模型中看概率是多少，哪个大就是\n",
      "哪个。</paragraph>\n",
      "Prediction: [Relevant]这种模型是一种判别模型，因为它能够判断出一个样本是否属于山羊或者是绵羊。[Partially supported]\n",
      "Score: 1.085246139101128\n",
      "\u001b[0m\u001b[1;3;34m5/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      10.65 ms /    42 runs   (    0.25 ms per token,  3942.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =   77830.12 ms /  1038 tokens (   74.98 ms per token,    13.34 tokens per second)\n",
      "llama_print_timings:        eval time =    8314.95 ms /    41 runs   (  202.80 ms per token,     4.93 tokens per second)\n",
      "llama_print_timings:       total time =   88345.31 ms /  1079 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第27页共46页第二十四篇：2022年5月10日美团搜索推荐算法面试题10道\n",
      "1、介绍下推荐系统的流程\n",
      "推荐系统的流程主要包含一下几个阶段。\n",
      "索引&特征:会根据内容特性提前建立若干种类型的索引。\n",
      "召回阶段:用户请求时会从各种索引种取出千/万条item。\n",
      "粗排阶段:针对这上千/万条item，进行第一遍打分，再筛选出几百条或者千条。这个阶段的排序模型一\n",
      "般都比较简单，能够过滤掉一些与用户兴趣明显不相关的。\n",
      "精排阶段:得到几百条item后，精排阶段会建立相对精细的模型，根据用户的画像，偏好，上下文，结\n",
      "合业务目标进行排序。一般精排后返回50-100条给到engine侧。\n",
      "重排阶段:engine侧拿到精排的50条item。还会做很多的人工干预和产品逻辑，比如item之间的多样\n",
      "性，产品策略逻辑，比如热门，置顶，多种内容之间的位置混合等等。最终会返回5-10条左右的item，曝\n",
      "光给客户端。根据业务特性，在线流程还有许多比较细的模块，比如去重服务，避免给用户推荐重复的内容。\n",
      "特征预处理，特征抽取等模块。\n",
      "2、召回和排序的差异？\n",
      "召回的目的在于减少候选的数量（尽量控制在1000以内），方便后续排序环节使用复杂模型精准排序；\n",
      "因为在短时间内评估海量候选，所以召回的关键点是个快字，受限与此与排序相比，召回的算法模型相对简\n",
      "单，使用的特征比较少。而排序模型相对更加复杂，更追求准确性，使用的特征也会较多。\n",
      "3、结果f1提升的1%怎么保证有效性，如何保证置信呢？\n",
      "实验过程中固定随机种子、多次实验取平均\n",
      "4、固定随机种子后，多次实验结果相同吗？\n",
      "还是会有细微差别，因为在梯度传播过程，梯度（浮点数）精度有差异，随着神经网络层数的增加，梯度\n",
      "差异会从小数后面的位置往前跑。只能设置浮点数精度增加来缓解这个问题。</paragraph>\n",
      "Prediction: [Relevant]因此，在实验过程中，需要多次实验取平均，以避免这个问题。[No support / Contradictory][Utility:5]\n",
      "Score: 1.4039067481529837\n",
      "\u001b[0m\u001b[1;3;34m6/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.47 ms /    51 runs   (    0.24 ms per token,  4088.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =   75346.32 ms /  1013 tokens (   74.38 ms per token,    13.44 tokens per second)\n",
      "llama_print_timings:        eval time =    9905.67 ms /    50 runs   (  198.11 ms per token,     5.05 tokens per second)\n",
      "llama_print_timings:       total time =   87609.43 ms /  1063 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第15页共46页第二十篇：2022年4月携程暑期实习搜索推荐算法岗面试题8道\n",
      "1、你所理解的推荐系统是什么样的？大致流程？所用模型？\n",
      "推荐系统一方面是为了帮助消费者发现对自己有价值的商品，另一方面是帮助生产者把用户可能感兴趣的\n",
      "商品展现给用户，实现生产者和消费者的双赢。\n",
      "大致流程主要包括：获取用户特征，召回过程，排序过程（粗排、精排）\n",
      "召回模型：\n",
      "·规则召回（兴趣标签top，热门top，新品top等）\n",
      "·协同召回（基于用户的协同过滤，基于商品的协同过滤）\n",
      "·向量召回（FM召回，Item2vec,YoutubeDNN向量召回，GraphEmbedding召回，DSSM双塔召\n",
      "回）\n",
      "排序模型：GBDT+LR、Xgboost、FM/FFM、Wide&Deep、DeepFM、Deep&Cross、DIN、BST\n",
      "等\n",
      "2、双塔模型优势，缺点，如何改进？\n",
      "双塔模型的优势是速度快，但模型精度还有待提升。\n",
      "速度快是因为将所有Item转化成Embedding，并存储进ANN检索系统，比如FAISS，以供查询。类\n",
      "似FAISS这种ANN检索系统对海量数据的查询效率高。\n",
      "而双塔模型为了速度快，必须对用户和Item进行特征分离，而特征分离，又必然导致上述两个原因产生\n",
      "的效果损失。\n",
      "改进：SENet双塔模型，把SENet放在Embedding层之上，目的是通过SENet网络，动态地学习这些\n",
      "特征的重要性：对于每个特征学会一个特征权重，然后再把学习到的权重乘到对应特征的Embedding里，这\n",
      "样就可以动态学习特征权重，通过小权重抑制噪音或者无效低频特征，通过大权重放大重要特征影响的目的。\n",
      "3、粗排的目的是什么？\n",
      "粗排是用来帮精排模型找到那些它本来就要打高分的item，只不过范围更广一些。.按照上面的例子，如\n",
      "果没有粗排，精排模型自己找出来的某top10的item。而粗排的任务就是要找到包含这10个item的一个更\n",
      "小的候选集，既保证了效果，又减少线上预测的负担。</paragraph>\n",
      "Prediction: [Relevant]4、如何理解协同召回？\n",
      "协同召回的思想是基于用户的协同过滤，基于商品的协\n",
      "Score: 0.719972554975759\n",
      "\u001b[0m\u001b[1;3;34m7/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.60 ms /    50 runs   (    0.25 ms per token,  3968.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45592.07 ms /   616 tokens (   74.01 ms per token,    13.51 tokens per second)\n",
      "llama_print_timings:        eval time =    9832.24 ms /    49 runs   (  200.66 ms per token,     4.98 tokens per second)\n",
      "llama_print_timings:       total time =   56904.70 ms /   665 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第35页共46页beamsearch是介于全局搜索和贪婪搜索之间。使用beamsize参数来限制在每一步保留下来的可能性\n",
      "词的数量。beamsearch是在测试阶段为了获得更好准确性而采取的一种策略，在训练阶段无需使用。\n",
      "相比暴力搜索的全局最优解，降低时间复杂度的方法就是寻找次优解，具体就是把搜索空间中的N减下\n",
      "来，每一步计算完只保留K个(beamsize)最大的取值路径，这样时间复杂度降为O(K*N*T)，K取值一般比\n",
      "N小很多。这样得到的虽然不是最优解，但是在seq2seq模型的推理预测中可以兼顾时间和效果。\n",
      "5、你了解的文本表示方法有哪些\n",
      "基于one-hot、tf-idf、textrank等的bag-of-words；\n",
      "主题模型：LSA（SVD）、pLSA、LDA；\n",
      "基于词向量的固定表征：word2vec、fastText、glove\n",
      "基于词向量的动态表征：elmo、GPT、bert\n",
      "对比如下：\n",
      "One-hot表示：维度灾难、语义鸿沟；\n",
      "矩阵分解（LSA）：利用全局语料特征，但SVD求解计算复杂度大；\n",
      "基于NNLM/RNNLM的词向量：词向量为副产物，存在效率不高等问题；\n",
      "word2vec、fastText：优化效率高，但是基于局部语料；\n",
      "glove：基于全局预料，结合了LSA和word2vec的优点；\n",
      "elmo、GPT、bert：动态特征。</paragraph>\n",
      "Prediction: [Relevant]这里的“N”是一个大的数字，例如10000，“T”也是一个大的数字，例如100，“K”也是一个大的\n",
      "Score: 0.5379866103651348\n",
      "\u001b[0m\u001b[1;3;34m8/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.02 ms /    50 runs   (    0.24 ms per token,  4161.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   66940.73 ms /   879 tokens (   76.16 ms per token,    13.13 tokens per second)\n",
      "llama_print_timings:        eval time =   10033.26 ms /    49 runs   (  204.76 ms per token,     4.88 tokens per second)\n",
      "llama_print_timings:       total time =   79092.65 ms /   928 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第43页共46页好处是这样做大大节约了参数量。\n",
      "5、如何防止过拟合\n",
      "数据的角度：获取和使用更多的数据（数据集增强）；\n",
      "模型角度：降低模型复杂度、L1\\L2\\Dropout正则化、Earlystopping（提前终止）\n",
      "模型融合的角度：使用bagging等模型融合方法。\n",
      "6、BN在训练和测试的时候的区别？可以防止过拟合吗？\n",
      "对于BN，在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。\n",
      "而在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全\n",
      "量训练数据的均值和方差，这个可以通过移动平均法求得。\n",
      "对于BN，当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，gamma和bata。\n",
      "BN算法防止过拟合：在网络的训练中，BN的使用使得一个minibatch中所有样本都被关联在了一起，\n",
      "因此网络不会从某一个训练样本中生成确定的结果，即同样一个样本的输出不再仅仅取决于样本的本身，也取\n",
      "决于跟这个样本同属一个batch的其他样本，而每次网络都是随机取batch，这样就会使得整个网络不会朝\n",
      "这一个方向使劲学习。一定程度上避免了过拟合。\n",
      "7、什么是AUC？\n",
      "AUC：AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。AUC反映的是分\n",
      "类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。\n",
      "8、卷积核计算公式\n",
      "卷积层计算公式如下：\n",
      "其中，W为输入大小，K为卷积核大小，P为padding大小，S为步幅。\n",
      "如果，想保持卷积前后的特征图大小相同，通常会设定padding为：</paragraph>\n",
      "Prediction: [Relevant]输出大小为W-K+1，因此可以得出：\n",
      "W=K+1\n",
      "\n",
      "其中，K为卷积核大小，W为输出大小\n",
      "Score: 0.7874755543086321\n",
      "\u001b[0m\u001b[1;3;34m9/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.30 ms /    50 runs   (    0.25 ms per token,  4064.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45178.53 ms /   603 tokens (   74.92 ms per token,    13.35 tokens per second)\n",
      "llama_print_timings:        eval time =    9311.76 ms /    49 runs   (  190.04 ms per token,     5.26 tokens per second)\n",
      "llama_print_timings:       total time =   55997.96 ms /   652 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第41页共46页8、LightGBM对于Xgboost有什么改进\n",
      "模型精度：XGBoost和LightGBM相当。\n",
      "训练速度：LightGBM远快于XGBoost。(快百倍以上，跟数据集有关系)\n",
      "内存消耗：LightGBM远小于XGBoost。(大约是xgb的五分之一)\n",
      "缺失值特征：XGBoost和LightGBM都可以自动处理特征缺失值。\n",
      "分类特征：XGBoost不支持类别特征，需要OneHot编码预处理。LightGBM直接支持类别特征。\n",
      "LightGBM在XGBoost上主要有3方面的优化。\n",
      "1，Histogram算法:直方图算法。\n",
      "2，GOSS算法:基于梯度的单边采样算法。\n",
      "3，EFB算法:互斥特征捆绑算法。\n",
      "9、防止过拟合的方式\n",
      "降低模型复杂度\n",
      "增加更多的训练数据：使用更大的数据集训练模型\n",
      "数据增强\n",
      "正则化：L1、L2、添加BN层\n",
      "添加Dropout策略\n",
      "EarlyStopping\n",
      "10、Adam讲一下\n",
      "Adam算法即自适应时刻估计方法（AdaptiveMomentEstimation），能计算每个参数的自适应学习\n",
      "率。这个方法不仅存储了AdaDelta先前平方梯度的指数衰减平均值，而且保持了先前梯度M(t)的指数衰减\n",
      "平均值，这一点与动量类似。\n",
      "Adam实际上就是将Momentum和RMSprop集合在一起，把一阶动量和二阶动量都使用起来了。</paragraph>\n",
      "Prediction: [Relevant]这里是一个示例代码，使用Adam算法训练一个二阶梯度中心偏移的神经网络模型。[No support / Contradictory]\n",
      "Score: 0.5490881291001676\n",
      "\u001b[0m\u001b[1;3;34m10/10 paragraphs done\n",
      "\n",
      "\u001b[0m\u001b[1;3;34mEnd evaluation\n",
      "\u001b[0m\u001b[1;3;34mSelected the best answer: [Relevant]这里介绍的是线性可分SVM。[Fully supported][Utility:5]\n",
      "\u001b[0m\u001b[1;3;32mFinal answer: 这里介绍的是线性可分SVM。\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       1.03 ms /     4 runs   (    0.26 ms per token,  3868.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2147.54 ms /    28 tokens (   76.70 ms per token,    13.04 tokens per second)\n",
      "llama_print_timings:        eval time =     509.85 ms /     3 runs   (  169.95 ms per token,     5.88 tokens per second)\n",
      "llama_print_timings:       total time =    2722.33 ms /    31 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;32mFinal answer: 好的\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       8.15 ms /    32 runs   (    0.25 ms per token,  3928.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1402.07 ms /    18 tokens (   77.89 ms per token,    12.84 tokens per second)\n",
      "llama_print_timings:        eval time =    5417.76 ms /    31 runs   (  174.77 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:       total time =    6950.20 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieval required\n",
      "\u001b[0m\u001b[1;3;34mReceived: 10 documents\n",
      "\u001b[0m\u001b[1;3;34mStart evaluation\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       1.38 ms /     5 runs   (    0.28 ms per token,  3636.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   79081.09 ms /  1073 tokens (   73.70 ms per token,    13.57 tokens per second)\n",
      "llama_print_timings:        eval time =     850.66 ms /     4 runs   (  212.66 ms per token,     4.70 tokens per second)\n",
      "llama_print_timings:       total time =   82007.95 ms /  1077 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第8页共46页第十八篇：2022年4月18日字节跳动机器学习AILab一面面试题6道\n",
      "1、BN和LN区别\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "2、讲讲selfattention\n",
      "SelfAttention与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的\n",
      "隐变量（hiddenstate）计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但\n",
      "SelfAttention不同，它分别在source端和target端进行，仅与sourceinput或者targetinput自身相关\n",
      "的SelfAttention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的\n",
      "selfAttention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系。\n",
      "因此，selfAttentionAttention比传统的Attentionmechanism效果要好，主要原因之一是，传统的\n",
      "Attention机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，selfAttention可以不仅可以得\n",
      "到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系。\n",
      "3、Bert的预训练过程\n",
      "Bert的预训练主要包含两个任务，MLM和NSP，MaskedLanguageModel任务可以理解为完形填\n",
      "空，随机mask每一个句子中15%的词，用其上下文来做预测；NextSentencePrediction任务选择一些句\n",
      "子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中\n",
      "的相关性。BERT预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的Loss相加。\n",
      "4、PreNorm与PostNorm的区别？\n",
      "参考：https://kexue.fm/archives/9009\n",
      "在同一设置下，PreNorm（也就是Normandadd）的效果是要优于PostNorm（AddandNorm）\n",
      "的，但是单独调整的话，PostNorm的效果是更好的，PreNorm结构无形地增加了模型的宽度而降低了模</paragraph>\n",
      "Prediction: [Relevant]​[No support / Contradictory][Utility:5]\n",
      "Score: 1.1204651873525742\n",
      "\u001b[0m\u001b[1;3;34m1/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.57 ms /    50 runs   (    0.25 ms per token,  3978.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   72783.27 ms /   865 tokens (   84.14 ms per token,    11.88 tokens per second)\n",
      "llama_print_timings:        eval time =    9716.23 ms /    49 runs   (  198.29 ms per token,     5.04 tokens per second)\n",
      "llama_print_timings:       total time =   84667.85 ms /   914 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第39页共46页第二十八篇：京东科技NLP实习面试题10道\n",
      "1、Bert里面为什么用layernormalization，而不用batchnormalization，分别讲一下这\n",
      "两个啥意思。\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "2.、Bert里面为什么Q，K，V要用三个不同的矩阵，用一个不是也行吗\n",
      "如果使用相同的矩阵，相同量级的情况下，q和k进行点积的值会是最大的，进行softmax的加权平均\n",
      "后，该词所占的比重会最大，使得其他词的比重很少，无法有效利用上下文信息来增强当前词的语义表示，而\n",
      "使用不同的QKV后，会很大程度减轻上述的影响。\n",
      "3、Bert和transformer讲一下\n",
      "1bert只有transformer的encode结构，是生成语言模型\n",
      "2bert加入了输入句子的mask机制，在输入的时候会随机mask\n",
      "3模型接收两个句子作为输入，并且预测其中第二个句子是否在原始文档中也是后续句子可以做对话机\n",
      "制的应答。\n",
      "4在训练BERT模型时，MaskedLM和NextSentencePrediction是一起训练的，目标就是要最小化\n",
      "两种策略的组合损失函数。\n",
      "4、AUC指标讲一下\n",
      "AUC：AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。AUC反映的是分\n",
      "类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。</paragraph>\n",
      "Prediction: [Relevant]AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "Score: 0.8590345020754301\n",
      "\u001b[0m\u001b[1;3;34m2/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      13.02 ms /    52 runs   (    0.25 ms per token,  3994.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   49882.44 ms /   663 tokens (   75.24 ms per token,    13.29 tokens per second)\n",
      "llama_print_timings:        eval time =   10088.16 ms /    51 runs   (  197.81 ms per token,     5.06 tokens per second)\n",
      "llama_print_timings:       total time =   61603.50 ms /   714 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第9页共46页型的深度，PostNorm每Norm一次就削弱一次恒等分支的权重，所以PostNorm反而是更突出残差分支\n",
      "的。\n",
      "参考：https://zhuanlan.zhihu.com/p/474988236\n",
      "post-norm在残差之后做归一化，对参数正则化的效果更强，进而模型的鲁棒性也会更好；pre-norm\n",
      "相对于post-norm，因为有一部分参数直接加在了后面，不需要对这部分参数进行正则化，正好可以防止模\n",
      "型的梯度爆炸或者梯度消失，因此，如果层数少post-norm的效果其实要好一些，如果要把层数加大，为了\n",
      "保证模型的训练，pre-norm显然更好一些。\n",
      "5、GPT与Bert的区别\n",
      "1）GPT是单向模型，无法利用上下文信息，只能利用上文；而BERT是双向模型。\n",
      "2）GPT是基于自回归模型，可以应用在NLU和NLG两大任务，而原生的BERT采用的基于自编码模\n",
      "型，只能完成NLU任务，无法直接应用在文本生成上面。\n",
      "6、如何加速Bert模型的训练\n",
      "BERT基线模型的训练使用Adamwithweightdecay（Adam优化器的变体）作为优化器，LAMB是一\n",
      "款通用优化器，它适用于小批量和大批量，且除了学习率以外其他超参数均无需调整。LAMB优化器支持自\n",
      "适应元素级更新（adaptiveelement-wiseupdating）和准确的逐层修正（layer-wisecorrection）。\n",
      "LAMB可将BERT预训练的批量大小扩展到64K，且不会造成准确率损失，76分钟就可以完成BERT的训\n",
      "练。</paragraph>\n",
      "Prediction: [Relevant]参考：https://zhuanlan.zhihu.com/p/474988236\n",
      "LAMB优化器可以将BERT预训练\n",
      "Score: 0.7307614573140608\n",
      "\u001b[0m\u001b[1;3;34m3/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       1.12 ms /     5 runs   (    0.22 ms per token,  4460.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =   75082.76 ms /   978 tokens (   76.77 ms per token,    13.03 tokens per second)\n",
      "llama_print_timings:        eval time =     789.96 ms /     4 runs   (  197.49 ms per token,     5.06 tokens per second)\n",
      "llama_print_timings:       total time =   77871.34 ms /   982 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第36页共46页第二十七篇：京东广告NLP实习面试题7道\n",
      "1、BeamSearch生成的句子基本都一样，是否有方法扩展生成句子的多样性\n",
      "解决方法：通过分组加入相似性惩罚，具体可以参考论文Diversebeamsearch：\n",
      "https://arxiv.org/pdf/1610.02424.pdf。\n",
      "具体方法：选择Beamsize为B，然后将其分为G组，每一组就有B/G个beam，每个单独的组内跟\n",
      "beamsearch很像，不断延展序列，同时引入一个dissimilarity项来保证组与组之间有差异。\n",
      "组内与beamsearch很像：从t-1到t时刻，不断的减少搜索空间（如同beamsearch一样）。\n",
      "组间差异：对于t=4时刻，我们先对第一组输出y（t=4），然后我们开始对第二组输出y（t=4），但\n",
      "是第二组y（t=4）的score不仅取决于第二组之前的y（t=3），也取决于其与第一组的相似程度。以此类\n",
      "推，在t=4时刻对于第三组的输出，我们从上图可以看到其score的打分标准。这儿对于其dissimilarity项\n",
      "的计算采用的办法是hammingdiversity，这个理解起来很简单，比如这个时刻可能输出的词在上面的组出\n",
      "现过，我们就对这个词的分数-1，如果这个时刻可能输出的词在上面组没有出现过，我们就对这个词的分数不\n",
      "惩罚。\n",
      "2、LayerNormalization和BatchNormalization的区别，padding对这两者有影响吗，\n",
      "对哪一维有影响\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样\n",
      "本的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "padding会对BatchNormalization的seq_len这个维度有影响，计算的时候会把padding也算进去。\n",
      "3、pytorch.Dataloader报错出现数据维度不一样怎么解决。\n",
      "在构建dataset重写的__getitem__方法中要返回相同长度的tensor。\n",
      "可以使用向量补全的方法来解决这个问题，把不同长度的向量补全成等长的。</paragraph>\n",
      "Prediction: [Relevant]​[No support / Contradictory][Utility:5]\n",
      "Score: 1.2103178809228488\n",
      "\u001b[0m\u001b[1;3;34m4/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      13.63 ms /    50 runs   (    0.27 ms per token,  3668.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =   73166.46 ms /   957 tokens (   76.45 ms per token,    13.08 tokens per second)\n",
      "llama_print_timings:        eval time =   10614.21 ms /    49 runs   (  216.62 ms per token,     4.62 tokens per second)\n",
      "llama_print_timings:       total time =   85914.47 ms /  1006 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第34页共46页第二十六篇：京东健康NLP实习面试题5道\n",
      "1、RNN为什么会出现梯度消失的现象\n",
      "梯度消失的原因：很难捕捉到长期的依赖关系，因为乘法梯度可以随着层的数量呈指数递减/递增。但需\n",
      "要注意的是RNN中总的梯度是不会消失的。即便梯度越传越弱，那也只是远距离的梯度消失，由于近距离的\n",
      "梯度不会消失，所有梯度之和便不会消失。RNN所谓梯度消失的真正含义是，梯度被近距离梯度主导，导致\n",
      "模型难以学到远距离的依赖关系。\n",
      "2、RNN和LSTM的区别\n",
      "RNN没有细胞状态；LSTM通过细胞状态记忆信息。\n",
      "RNN激活函数只有tanh；LSTM通过输入门、遗忘门、输出门引入sigmoid函数并结合tanh函数，添\n",
      "加求和操作，减少梯度消失和梯度爆炸的可能性。\n",
      "RNN只能够处理短期依赖问题；LSTM既能够处理短期依赖问题，又能够处理长期依赖问题。\n",
      "3、word2vec有几种方式\n",
      "word2vec有CBOW模型和Skip-Gram模型。\n",
      "CBOW模型：输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向\n",
      "量。（ContinuousBag-of-Word）\n",
      "Skip-Gram模型：输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。\n",
      "有两种改进方式：\n",
      "一种是基于HierarchicalSoftmax的，另一种是基于NegativeSampling的。\n",
      "4、greedysearch和beamsearch的区别\n",
      "greedysearch比较简单，就是贪婪式的搜索，每一步都选择概率最大的单词输出，最后组成整个句子输\n",
      "出。这种方法给出的结果一般情况结果比较差，因为只考虑了每一步的最优解，往往里全局最优解差距很大。\n",
      "优点：计算速度快，每次取概率最大的词。缺点：局部最优并不等于全局最好的，而且一旦选错了，后\n",
      "续生成的内容很可能也是错误的，具有错误的累加效果。</paragraph>\n",
      "Prediction: [Relevant]LN 是一种近邻模型，其中每个观测都是一个特征向量，观测之间的相关性由一个特定的函\n",
      "Score: 0.7838164176298077\n",
      "\u001b[0m\u001b[1;3;34m5/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.82 ms /    50 runs   (    0.26 ms per token,  3899.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   65672.09 ms /   879 tokens (   74.71 ms per token,    13.38 tokens per second)\n",
      "llama_print_timings:        eval time =    9847.03 ms /    49 runs   (  200.96 ms per token,     4.98 tokens per second)\n",
      "llama_print_timings:       total time =   77468.04 ms /   928 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第43页共46页好处是这样做大大节约了参数量。\n",
      "5、如何防止过拟合\n",
      "数据的角度：获取和使用更多的数据（数据集增强）；\n",
      "模型角度：降低模型复杂度、L1\\L2\\Dropout正则化、Earlystopping（提前终止）\n",
      "模型融合的角度：使用bagging等模型融合方法。\n",
      "6、BN在训练和测试的时候的区别？可以防止过拟合吗？\n",
      "对于BN，在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。\n",
      "而在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全\n",
      "量训练数据的均值和方差，这个可以通过移动平均法求得。\n",
      "对于BN，当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，gamma和bata。\n",
      "BN算法防止过拟合：在网络的训练中，BN的使用使得一个minibatch中所有样本都被关联在了一起，\n",
      "因此网络不会从某一个训练样本中生成确定的结果，即同样一个样本的输出不再仅仅取决于样本的本身，也取\n",
      "决于跟这个样本同属一个batch的其他样本，而每次网络都是随机取batch，这样就会使得整个网络不会朝\n",
      "这一个方向使劲学习。一定程度上避免了过拟合。\n",
      "7、什么是AUC？\n",
      "AUC：AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。AUC反映的是分\n",
      "类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。\n",
      "8、卷积核计算公式\n",
      "卷积层计算公式如下：\n",
      "其中，W为输入大小，K为卷积核大小，P为padding大小，S为步幅。\n",
      "如果，想保持卷积前后的特征图大小相同，通常会设定padding为：</paragraph>\n",
      "Prediction: [Relevant]卷积核计算公式如下：\n",
      "其中，W为输入大小，K为卷积核大小，P为padding大小，S为\n",
      "Score: 0.6867971081852774\n",
      "\u001b[0m\u001b[1;3;34m6/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.81 ms /    50 runs   (    0.26 ms per token,  3902.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   57682.46 ms /   757 tokens (   76.20 ms per token,    13.12 tokens per second)\n",
      "llama_print_timings:        eval time =    9765.61 ms /    49 runs   (  199.30 ms per token,     5.02 tokens per second)\n",
      "llama_print_timings:       total time =   69237.36 ms /   806 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第21页共46页第二十二篇：2022年5月18日2023届广联达提前批面试题5道\n",
      "1、Transformer中encoder和decoder的区别（结构和功能上）\n",
      "结构上：Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个\n",
      "Block，Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder\n",
      "端的Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一\n",
      "个前馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和Layer\n",
      "Normalization层。\n",
      "功能上：Transformer中encoder是双向的，是编码器；decoder是单向的，是解码器，可以用于生成\n",
      "任务。\n",
      "2、有哪些防止过拟合的方法\n",
      "数据的角度：获取和使用更多的数据（数据集增强）；\n",
      "模型角度：降低模型复杂度、L1\\L2\\Dropout正则化、Earlystopping（提前终止）\n",
      "模型融合的角度：使用bagging等模型融合方法。\n",
      "3、L1和L2正则化为什么可以防止过拟合\n",
      "正规化是防止过拟合的一种重要技巧。正则化通过降低模型的复杂性，达到避免过拟合的问题。这里的\n",
      "降低模型的复杂性可以理解为：\n",
      "L1将很多权重变成0，这样起到作用的因素就会减少。\n",
      "L2使权重都趋于0，这样就不会有某个权重占比特别大。\n",
      "4、传统机器学习方法了解多少\n",
      "这个可以说逻辑回归模型，逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度\n",
      "下降法来求解参数，从而达到将数据二分类的目的。\n",
      "逻辑回归的优缺点\n",
      "优点：</paragraph>\n",
      "Prediction: [Relevant]逻辑回归模型是一种基本的机器学习模型，其中包括了一些基本的机器学习术语\n",
      "Score: 0.6283624230223188\n",
      "\u001b[0m\u001b[1;3;34m7/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.80 ms /    52 runs   (    0.25 ms per token,  4062.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   97929.24 ms /  1244 tokens (   78.72 ms per token,    12.70 tokens per second)\n",
      "llama_print_timings:        eval time =   10602.27 ms /    51 runs   (  207.89 ms per token,     4.81 tokens per second)\n",
      "llama_print_timings:       total time =  111368.23 ms /  1295 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第2页共46页5、生成式模型和判别式模型的区别并举一些例子.22\n",
      "第二十三篇：2022年4月大厂常考Leetcode面试题6道.23\n",
      "1、剑指offer29：顺时针打印矩阵.23\n",
      "2、剑指offer11：旋转数组的最小数字.24\n",
      "3、LeetCode69：x的平方根.24\n",
      "4、LeetCode22：括号生成.24\n",
      "5、LeetCode123：买卖股票的最佳时机III.25\n",
      "6、LeetCode3：无重复字符的最长子串.25\n",
      "第二十四篇：2022年5月10日美团搜索推荐算法面试题10道.27\n",
      "1、介绍下推荐系统的流程.27\n",
      "2、召回和排序的差异？27\n",
      "3、结果f1提升的1%怎么保证有效性，如何保证置信呢？27\n",
      "4、固定随机种子后，多次实验结果相同吗？27\n",
      "5、召回主流的做法.28\n",
      "6、介绍下embedding召回.28\n",
      "7、推荐系统冷启动问题，怎么解决.28\n",
      "8、LeetCode101—对称二叉树.28\n",
      "9、LeetCode3—无重复字符的最长子串.29\n",
      "10、LeetcCode130—被围绕的区域.29\n",
      "第二十五篇：2022年4月字节视频架构暑期实习面试题5道.31\n",
      "1、图像处理的基本知识：直方图均衡化、维纳滤波、锐化的操作.31\n",
      "2、Leetcode912—排序数组.31\n",
      "3、Leetcode102—二叉树层序遍历.32\n",
      "4、Leetcode200—岛屿数量.32\n",
      "5、Leetcode48—旋转图像.33\n",
      "第二十六篇：京东健康NLP实习面试题5道.34\n",
      "1、RNN为什么会出现梯度消失的现象.34\n",
      "2、RNN和LSTM的区别.34\n",
      "3、word2vec有几种方式.34\n",
      "4、greedysearch和beamsearch的区别.34\n",
      "5、你了解的文本表示方法有哪些.35\n",
      "第二十七篇：京东广告NLP实习面试题7道.36\n",
      "1、BeamSearch生成的句子基本都一样，是否有方法扩展生成句子的多样性.36\n",
      "2、LayerNormalization和BatchNormalization的区别，padding对这两者有影响吗，对哪一维有影响.36\n",
      "3、pytorch.Dataloader报错出现数据维度不一样怎么解决。36\n",
      "4、无序数组，找topK，要求比快排快.37\n",
      "5、Bert里面mask的用处。38\n",
      "6、对于两个词怎么算他们的相似度，用基于wordembedding的方式。38\n",
      "7、Leetcode—最大子序列和.38\n",
      "第二十八篇：京东科技NLP实习面试题10道.39\n",
      "1、Bert里面为什么用layernormalization，而不用batchnormalization，分别讲一下这两个啥意思。39\n",
      "2.、Bert里面为什么Q，K，V要用三个不同的矩阵，用一个不是也行吗.39\n",
      "3、Bert和transformer讲一下.39\n",
      "4、AUC指标讲一下.39\n",
      "5、Precision和Recall讲一下.40\n",
      "6、GBDT和Xgboost的区别.40</paragraph>\n",
      "Prediction: [Relevant]LN 是一种常见的训练损失函数，它是一种损失函数，用于训练机器学习\n",
      "Score: 0.7488686516654387\n",
      "\u001b[0m\u001b[1;3;34m8/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.54 ms /    50 runs   (    0.25 ms per token,  3985.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =   62242.99 ms /   831 tokens (   74.90 ms per token,    13.35 tokens per second)\n",
      "llama_print_timings:        eval time =    9565.36 ms /    49 runs   (  195.21 ms per token,     5.12 tokens per second)\n",
      "llama_print_timings:       total time =   73627.74 ms /   880 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第16页共46页4、wide&deep模型为什么要有wide层结构，优缺点，如何改进？\n",
      "wide&deep模型中的wide部分可以通过利用交叉特征引入非线性高效的实现记忆能力，但需要依赖人\n",
      "工特征工程。\n",
      "改进：DeepFM在Wide&Deep的基础上进行改进，不需要预训练FM得到隐向量，不需要人工特征工\n",
      "程，能同时学习低阶和高阶的组合特征；FM模块和Deep模块共享FeatureEmbedding部分，可以更快的\n",
      "训练，以及更精确的训练学习。\n",
      "5、推荐领域GBDT+LR的做法了解吗？\n",
      "GBDT+LR由两部分组成，其中GBDT用来对训练集提取特征作为新的训练输入数据，LR作为新训练输\n",
      "入数据的分类器。GBDT+LR的提出意味着特征工程可以完全交由一个独立的模型来完成，模型的输入可以是\n",
      "原始的特征向量，不必在特征工程上投入过多的人工筛选和模型设计的精力，真正实现了端到端的训练。\n",
      "6、粗排有哪些指标？NDCG了解吗？\n",
      "(1)NDCG，排序相似性的指标，看精排的排序结果和粗排有多相似\n",
      "(2)粗排的召回率/重叠率，粗排的topk和精排的topk有多大占比。\n",
      "计算DCG，计算公式如下：\n",
      "其中，rel为这个排序list，结果i的一个等级得分；i是指结果i的当前位置序号；\n",
      "​在搜索引擎中，rel等级得分，是由人工抽样数据，并且根据一定的规则打出来的等级得分。\n",
      "步骤二：\n",
      "计算IDCG（IdealDCG），即完美序的DCG；计算方式也同步骤1，只是排序序列不是由算法得出，而\n",
      "是由人工对序列根据一定的评估准则排出来的最佳序列。\n",
      "​步骤三：根据前面2个步骤的出来的结果，计算NDCG，计算公式如下：</paragraph>\n",
      "Prediction: [Relevant](3) 排序质量指标（Quality Indicators for Sorting），这个指标是一个排序质量的概括指标\n",
      "Score: 0.7305473359381379\n",
      "\u001b[0m\u001b[1;3;34m9/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.44 ms /    51 runs   (    0.24 ms per token,  4100.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44850.20 ms /   604 tokens (   74.26 ms per token,    13.47 tokens per second)\n",
      "llama_print_timings:        eval time =    9444.86 ms /    50 runs   (  188.90 ms per token,     5.29 tokens per second)\n",
      "llama_print_timings:       total time =   55795.00 ms /   654 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第41页共46页8、LightGBM对于Xgboost有什么改进\n",
      "模型精度：XGBoost和LightGBM相当。\n",
      "训练速度：LightGBM远快于XGBoost。(快百倍以上，跟数据集有关系)\n",
      "内存消耗：LightGBM远小于XGBoost。(大约是xgb的五分之一)\n",
      "缺失值特征：XGBoost和LightGBM都可以自动处理特征缺失值。\n",
      "分类特征：XGBoost不支持类别特征，需要OneHot编码预处理。LightGBM直接支持类别特征。\n",
      "LightGBM在XGBoost上主要有3方面的优化。\n",
      "1，Histogram算法:直方图算法。\n",
      "2，GOSS算法:基于梯度的单边采样算法。\n",
      "3，EFB算法:互斥特征捆绑算法。\n",
      "9、防止过拟合的方式\n",
      "降低模型复杂度\n",
      "增加更多的训练数据：使用更大的数据集训练模型\n",
      "数据增强\n",
      "正则化：L1、L2、添加BN层\n",
      "添加Dropout策略\n",
      "EarlyStopping\n",
      "10、Adam讲一下\n",
      "Adam算法即自适应时刻估计方法（AdaptiveMomentEstimation），能计算每个参数的自适应学习\n",
      "率。这个方法不仅存储了AdaDelta先前平方梯度的指数衰减平均值，而且保持了先前梯度M(t)的指数衰减\n",
      "平均值，这一点与动量类似。\n",
      "Adam实际上就是将Momentum和RMSprop集合在一起，把一阶动量和二阶动量都使用起来了。</paragraph>\n",
      "Prediction: [Relevant]BN是一种常见的神经网络中的一种特征，用来减少梯度下降的梯度衰减\n",
      "Score: 0.5218602448979809\n",
      "\u001b[0m\u001b[1;3;34m10/10 paragraphs done\n",
      "\n",
      "\u001b[0m\u001b[1;3;34mEnd evaluation\n",
      "\u001b[0m\u001b[1;3;34mSelected the best answer: [Relevant]​[No support / Contradictory][Utility:5]\n",
      "\u001b[0m\u001b[1;3;32mFinal answer: ​\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.43 ms /    50 runs   (    0.25 ms per token,  4023.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1107.46 ms /    15 tokens (   73.83 ms per token,    13.54 tokens per second)\n",
      "llama_print_timings:        eval time =    8560.35 ms /    49 runs   (  174.70 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:       total time =    9850.25 ms /    64 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieval required\n",
      "\u001b[0m\u001b[1;3;34mReceived: 10 documents\n",
      "\u001b[0m\u001b[1;3;34mStart evaluation\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.45 ms /    50 runs   (    0.25 ms per token,  4014.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =   52798.97 ms /   707 tokens (   74.68 ms per token,    13.39 tokens per second)\n",
      "llama_print_timings:        eval time =    9342.08 ms /    49 runs   (  190.65 ms per token,     5.25 tokens per second)\n",
      "llama_print_timings:       total time =   63700.22 ms /   756 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第42页共46页第二十九篇：2022年6月24日字节电商CV实习岗面试题10道\n",
      "1、如何解决类别极度不平衡的问题？\n",
      "2、说下Transformer模型\n",
      "Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个Block，\n",
      "Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder端的\n",
      "Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一个前\n",
      "馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和LayerNormalization\n",
      "层。\n",
      "3、说下FocalLoss\n",
      "Focalloss是目标检测中解决正负样本严重不平衡的方法，在标准交叉熵损失基础上修改得到的。这个函\n",
      "数可以通过减少易分类样本的权重，使得模型在训练时更专注于稀疏的难分类的样本；防止大量易分类负样本\n",
      "在loss中占主导地位。\n",
      "4、介绍下深度可分离卷积和传统卷积的区别\n",
      "传统的卷积是各个通道上采用相不同的卷积核，然后不同的卷积核用于提取不同方面的特征。\n",
      "深度可分离卷积先在各个通道上采用不同的卷积核提取不同的特征，但是这样对于某个通道来说，就只提\n",
      "取了一方面的特征，因此在此基础上加入点卷积，用1*1的卷积对提取特征后的特征图再次提取不同方面的\n",
      "特征，最终产生和普通卷积相同的输出特征图。</paragraph>\n",
      "Prediction: [Relevant]自我注意力（Self Attention）是一种神经语言模型中的一种重要概念，用于模型中的自我参与和内部连接。[Fully supported]\n",
      "Score: 1.491432015330191\n",
      "\u001b[0m\u001b[1;3;34m1/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.88 ms /    50 runs   (    0.26 ms per token,  3881.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   82588.27 ms /  1070 tokens (   77.19 ms per token,    12.96 tokens per second)\n",
      "llama_print_timings:        eval time =   10303.16 ms /    49 runs   (  210.27 ms per token,     4.76 tokens per second)\n",
      "llama_print_timings:       total time =   95250.98 ms /  1119 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第8页共46页第十八篇：2022年4月18日字节跳动机器学习AILab一面面试题6道\n",
      "1、BN和LN区别\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "2、讲讲selfattention\n",
      "SelfAttention与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的\n",
      "隐变量（hiddenstate）计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但\n",
      "SelfAttention不同，它分别在source端和target端进行，仅与sourceinput或者targetinput自身相关\n",
      "的SelfAttention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的\n",
      "selfAttention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系。\n",
      "因此，selfAttentionAttention比传统的Attentionmechanism效果要好，主要原因之一是，传统的\n",
      "Attention机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，selfAttention可以不仅可以得\n",
      "到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系。\n",
      "3、Bert的预训练过程\n",
      "Bert的预训练主要包含两个任务，MLM和NSP，MaskedLanguageModel任务可以理解为完形填\n",
      "空，随机mask每一个句子中15%的词，用其上下文来做预测；NextSentencePrediction任务选择一些句\n",
      "子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中\n",
      "的相关性。BERT预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的Loss相加。\n",
      "4、PreNorm与PostNorm的区别？\n",
      "参考：https://kexue.fm/archives/9009\n",
      "在同一设置下，PreNorm（也就是Normandadd）的效果是要优于PostNorm（AddandNorm）\n",
      "的，但是单独调整的话，PostNorm的效果是更好的，PreNorm结构无形地增加了模型的宽度而降低了模</paragraph>\n",
      "Prediction: [Relevant]以上是对“self attention”的解释，具体如何使用和实现，需要根据自己的需求和使用场景进行选择和\n",
      "Score: 0.5622732080042753\n",
      "\u001b[0m\u001b[1;3;34m2/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.61 ms /    52 runs   (    0.24 ms per token,  4123.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   74091.42 ms /   958 tokens (   77.34 ms per token,    12.93 tokens per second)\n",
      "llama_print_timings:        eval time =   10599.81 ms /    51 runs   (  207.84 ms per token,     4.81 tokens per second)\n",
      "llama_print_timings:       total time =   86846.78 ms /  1009 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第34页共46页第二十六篇：京东健康NLP实习面试题5道\n",
      "1、RNN为什么会出现梯度消失的现象\n",
      "梯度消失的原因：很难捕捉到长期的依赖关系，因为乘法梯度可以随着层的数量呈指数递减/递增。但需\n",
      "要注意的是RNN中总的梯度是不会消失的。即便梯度越传越弱，那也只是远距离的梯度消失，由于近距离的\n",
      "梯度不会消失，所有梯度之和便不会消失。RNN所谓梯度消失的真正含义是，梯度被近距离梯度主导，导致\n",
      "模型难以学到远距离的依赖关系。\n",
      "2、RNN和LSTM的区别\n",
      "RNN没有细胞状态；LSTM通过细胞状态记忆信息。\n",
      "RNN激活函数只有tanh；LSTM通过输入门、遗忘门、输出门引入sigmoid函数并结合tanh函数，添\n",
      "加求和操作，减少梯度消失和梯度爆炸的可能性。\n",
      "RNN只能够处理短期依赖问题；LSTM既能够处理短期依赖问题，又能够处理长期依赖问题。\n",
      "3、word2vec有几种方式\n",
      "word2vec有CBOW模型和Skip-Gram模型。\n",
      "CBOW模型：输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向\n",
      "量。（ContinuousBag-of-Word）\n",
      "Skip-Gram模型：输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。\n",
      "有两种改进方式：\n",
      "一种是基于HierarchicalSoftmax的，另一种是基于NegativeSampling的。\n",
      "4、greedysearch和beamsearch的区别\n",
      "greedysearch比较简单，就是贪婪式的搜索，每一步都选择概率最大的单词输出，最后组成整个句子输\n",
      "出。这种方法给出的结果一般情况结果比较差，因为只考虑了每一步的最优解，往往里全局最优解差距很大。\n",
      "优点：计算速度快，每次取概率最大的词。缺点：局部最优并不等于全局最好的，而且一旦选错了，后\n",
      "续生成的内容很可能也是错误的，具有错误的累加效果。</paragraph>\n",
      "Prediction: [Relevant]beamsearch是一种受限搜索算法，它的思想是每一步都选择概率最大的单词，但\n",
      "Score: 0.7340960660487243\n",
      "\u001b[0m\u001b[1;3;34m3/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.87 ms /    50 runs   (    0.26 ms per token,  3886.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   55755.25 ms /   707 tokens (   78.86 ms per token,    12.68 tokens per second)\n",
      "llama_print_timings:        eval time =    9813.88 ms /    49 runs   (  200.28 ms per token,     4.99 tokens per second)\n",
      "llama_print_timings:       total time =   67309.75 ms /   756 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第28页共46页5、召回主流的做法\n",
      "主流的召回做法包括：规则召回，协同召回，基于内容语义的I2I召回，向量召回（基于\n",
      "embedding），树召回和图召回。\n",
      "6、介绍下embedding召回\n",
      "举一个文本类embedding的例子。\n",
      "文本类的Embedding可以分为两种，一种是比较传统的word2vector、fasttext、glove这些算法的方\n",
      "案，叫做词向量固定表征类算法，这些算法主要是通过分析词的出现频率来进行Embedding生成，不考虑文\n",
      "本上下文。\n",
      "而另一种文本Embedding方法，也是目前最流行的方案是动态词表征算法，比如Bert、ELMo、GPT，\n",
      "这类算法会考虑文本上下文。\n",
      "7、推荐系统冷启动问题，怎么解决\n",
      "1.提供非个性化的推荐\n",
      "最简单的例子就是热门排行榜，我们可以给用户推荐热门排行榜，然后等到用户数据收集到一定的时候，\n",
      "再切换为个性化推荐。\n",
      "2.利用用户注册信息\n",
      "用户注册时提供包括用户的年龄、性别、职业、民族、学历和居住地等数据，做粗粒度的个性化。有一些\n",
      "网站还会让用户用文字描述他们的兴趣。\n",
      "3.利用社交网络信息\n",
      "引导用户通过社交网络账号登录（需要用户授权），导入用户在社交网站上的好友信息，然后给用户推荐\n",
      "其好友喜欢的物品。\n",
      "8、LeetCode101—对称二叉树\n",
      "1.classSolution(object):\n",
      "2. defisSymmetric(self,root):\n",
      "3. \"\"\"\n",
      "4. :typeroot:TreeNode\n",
      "5. :rtype:bool\n",
      "6. \"\"\"\n",
      "7. ifnotroot:</paragraph>\n",
      "Prediction: [Irrelevant]这是一个Python代码，用来实现对称二叉树的Solution。\n",
      "```python\n",
      "class Solution:\n",
      "    def isSymmetric(self, root):\n",
      "        if not root:\n",
      "Score: 0.46780706691910545\n",
      "\u001b[0m\u001b[1;3;34m4/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      13.12 ms /    50 runs   (    0.26 ms per token,  3811.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   80077.67 ms /  1013 tokens (   79.05 ms per token,    12.65 tokens per second)\n",
      "llama_print_timings:        eval time =   10747.48 ms /    49 runs   (  219.34 ms per token,     4.56 tokens per second)\n",
      "llama_print_timings:       total time =   93154.50 ms /  1062 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第15页共46页第二十篇：2022年4月携程暑期实习搜索推荐算法岗面试题8道\n",
      "1、你所理解的推荐系统是什么样的？大致流程？所用模型？\n",
      "推荐系统一方面是为了帮助消费者发现对自己有价值的商品，另一方面是帮助生产者把用户可能感兴趣的\n",
      "商品展现给用户，实现生产者和消费者的双赢。\n",
      "大致流程主要包括：获取用户特征，召回过程，排序过程（粗排、精排）\n",
      "召回模型：\n",
      "·规则召回（兴趣标签top，热门top，新品top等）\n",
      "·协同召回（基于用户的协同过滤，基于商品的协同过滤）\n",
      "·向量召回（FM召回，Item2vec,YoutubeDNN向量召回，GraphEmbedding召回，DSSM双塔召\n",
      "回）\n",
      "排序模型：GBDT+LR、Xgboost、FM/FFM、Wide&Deep、DeepFM、Deep&Cross、DIN、BST\n",
      "等\n",
      "2、双塔模型优势，缺点，如何改进？\n",
      "双塔模型的优势是速度快，但模型精度还有待提升。\n",
      "速度快是因为将所有Item转化成Embedding，并存储进ANN检索系统，比如FAISS，以供查询。类\n",
      "似FAISS这种ANN检索系统对海量数据的查询效率高。\n",
      "而双塔模型为了速度快，必须对用户和Item进行特征分离，而特征分离，又必然导致上述两个原因产生\n",
      "的效果损失。\n",
      "改进：SENet双塔模型，把SENet放在Embedding层之上，目的是通过SENet网络，动态地学习这些\n",
      "特征的重要性：对于每个特征学会一个特征权重，然后再把学习到的权重乘到对应特征的Embedding里，这\n",
      "样就可以动态学习特征权重，通过小权重抑制噪音或者无效低频特征，通过大权重放大重要特征影响的目的。\n",
      "3、粗排的目的是什么？\n",
      "粗排是用来帮精排模型找到那些它本来就要打高分的item，只不过范围更广一些。.按照上面的例子，如\n",
      "果没有粗排，精排模型自己找出来的某top10的item。而粗排的任务就是要找到包含这10个item的一个更\n",
      "小的候选集，既保证了效果，又减少线上预测的负担。</paragraph>\n",
      "Prediction: [Relevant]4、如何理解协同召回？\n",
      "协同召回是基于用户的协同过滤和基于商品的协同过\n",
      "Score: 0.7598202403977503\n",
      "\u001b[0m\u001b[1;3;34m5/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      13.53 ms /    50 runs   (    0.27 ms per token,  3696.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =   81521.54 ms /  1039 tokens (   78.46 ms per token,    12.75 tokens per second)\n",
      "llama_print_timings:        eval time =   11338.90 ms /    49 runs   (  231.41 ms per token,     4.32 tokens per second)\n",
      "llama_print_timings:       total time =   95238.45 ms /  1088 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第27页共46页第二十四篇：2022年5月10日美团搜索推荐算法面试题10道\n",
      "1、介绍下推荐系统的流程\n",
      "推荐系统的流程主要包含一下几个阶段。\n",
      "索引&特征:会根据内容特性提前建立若干种类型的索引。\n",
      "召回阶段:用户请求时会从各种索引种取出千/万条item。\n",
      "粗排阶段:针对这上千/万条item，进行第一遍打分，再筛选出几百条或者千条。这个阶段的排序模型一\n",
      "般都比较简单，能够过滤掉一些与用户兴趣明显不相关的。\n",
      "精排阶段:得到几百条item后，精排阶段会建立相对精细的模型，根据用户的画像，偏好，上下文，结\n",
      "合业务目标进行排序。一般精排后返回50-100条给到engine侧。\n",
      "重排阶段:engine侧拿到精排的50条item。还会做很多的人工干预和产品逻辑，比如item之间的多样\n",
      "性，产品策略逻辑，比如热门，置顶，多种内容之间的位置混合等等。最终会返回5-10条左右的item，曝\n",
      "光给客户端。根据业务特性，在线流程还有许多比较细的模块，比如去重服务，避免给用户推荐重复的内容。\n",
      "特征预处理，特征抽取等模块。\n",
      "2、召回和排序的差异？\n",
      "召回的目的在于减少候选的数量（尽量控制在1000以内），方便后续排序环节使用复杂模型精准排序；\n",
      "因为在短时间内评估海量候选，所以召回的关键点是个快字，受限与此与排序相比，召回的算法模型相对简\n",
      "单，使用的特征比较少。而排序模型相对更加复杂，更追求准确性，使用的特征也会较多。\n",
      "3、结果f1提升的1%怎么保证有效性，如何保证置信呢？\n",
      "实验过程中固定随机种子、多次实验取平均\n",
      "4、固定随机种子后，多次实验结果相同吗？\n",
      "还是会有细微差别，因为在梯度传播过程，梯度（浮点数）精度有差异，随着神经网络层数的增加，梯度\n",
      "差异会从小数后面的位置往前跑。只能设置浮点数精度增加来缓解这个问题。</paragraph>\n",
      "Prediction: [Relevant]这个问题可以通过设置浮点数精度来缓解，但是要注意的是，更高的浮点数精度不一\n",
      "Score: 0.6054243433721611\n",
      "\u001b[0m\u001b[1;3;34m6/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.56 ms /    50 runs   (    0.25 ms per token,  3980.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47534.04 ms /   616 tokens (   77.17 ms per token,    12.96 tokens per second)\n",
      "llama_print_timings:        eval time =    9862.53 ms /    49 runs   (  201.28 ms per token,     4.97 tokens per second)\n",
      "llama_print_timings:       total time =   59017.64 ms /   665 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第35页共46页beamsearch是介于全局搜索和贪婪搜索之间。使用beamsize参数来限制在每一步保留下来的可能性\n",
      "词的数量。beamsearch是在测试阶段为了获得更好准确性而采取的一种策略，在训练阶段无需使用。\n",
      "相比暴力搜索的全局最优解，降低时间复杂度的方法就是寻找次优解，具体就是把搜索空间中的N减下\n",
      "来，每一步计算完只保留K个(beamsize)最大的取值路径，这样时间复杂度降为O(K*N*T)，K取值一般比\n",
      "N小很多。这样得到的虽然不是最优解，但是在seq2seq模型的推理预测中可以兼顾时间和效果。\n",
      "5、你了解的文本表示方法有哪些\n",
      "基于one-hot、tf-idf、textrank等的bag-of-words；\n",
      "主题模型：LSA（SVD）、pLSA、LDA；\n",
      "基于词向量的固定表征：word2vec、fastText、glove\n",
      "基于词向量的动态表征：elmo、GPT、bert\n",
      "对比如下：\n",
      "One-hot表示：维度灾难、语义鸿沟；\n",
      "矩阵分解（LSA）：利用全局语料特征，但SVD求解计算复杂度大；\n",
      "基于NNLM/RNNLM的词向量：词向量为副产物，存在效率不高等问题；\n",
      "word2vec、fastText：优化效率高，但是基于局部语料；\n",
      "glove：基于全局预料，结合了LSA和word2vec的优点；\n",
      "elmo、GPT、bert：动态特征。</paragraph>\n",
      "Prediction: [Irrelevant]你可以在这里获取更多信息：https://www.alibabacloud.com/blog/self-attention-in-transformer-models_594292?spm\n",
      "Score: 0.4853524170170509\n",
      "\u001b[0m\u001b[1;3;34m7/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      13.14 ms /    50 runs   (    0.26 ms per token,  3804.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =   58649.10 ms /   757 tokens (   77.48 ms per token,    12.91 tokens per second)\n",
      "llama_print_timings:        eval time =    9990.36 ms /    49 runs   (  203.88 ms per token,     4.90 tokens per second)\n",
      "llama_print_timings:       total time =   70426.45 ms /   806 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第21页共46页第二十二篇：2022年5月18日2023届广联达提前批面试题5道\n",
      "1、Transformer中encoder和decoder的区别（结构和功能上）\n",
      "结构上：Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个\n",
      "Block，Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder\n",
      "端的Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一\n",
      "个前馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和Layer\n",
      "Normalization层。\n",
      "功能上：Transformer中encoder是双向的，是编码器；decoder是单向的，是解码器，可以用于生成\n",
      "任务。\n",
      "2、有哪些防止过拟合的方法\n",
      "数据的角度：获取和使用更多的数据（数据集增强）；\n",
      "模型角度：降低模型复杂度、L1\\L2\\Dropout正则化、Earlystopping（提前终止）\n",
      "模型融合的角度：使用bagging等模型融合方法。\n",
      "3、L1和L2正则化为什么可以防止过拟合\n",
      "正规化是防止过拟合的一种重要技巧。正则化通过降低模型的复杂性，达到避免过拟合的问题。这里的\n",
      "降低模型的复杂性可以理解为：\n",
      "L1将很多权重变成0，这样起到作用的因素就会减少。\n",
      "L2使权重都趋于0，这样就不会有某个权重占比特别大。\n",
      "4、传统机器学习方法了解多少\n",
      "这个可以说逻辑回归模型，逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度\n",
      "下降法来求解参数，从而达到将数据二分类的目的。\n",
      "逻辑回归的优缺点\n",
      "优点：</paragraph>\n",
      "Prediction: [Relevant]逻辑回归模型是一种基本的机器学习模型，其中，输出为一个分类器，输入为一个特征向\n",
      "Score: 0.6522245571265552\n",
      "\u001b[0m\u001b[1;3;34m8/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      13.59 ms /    50 runs   (    0.27 ms per token,  3679.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =   70375.90 ms /   831 tokens (   84.69 ms per token,    11.81 tokens per second)\n",
      "llama_print_timings:        eval time =   10091.93 ms /    49 runs   (  205.96 ms per token,     4.86 tokens per second)\n",
      "llama_print_timings:       total time =   82794.10 ms /   880 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第16页共46页4、wide&deep模型为什么要有wide层结构，优缺点，如何改进？\n",
      "wide&deep模型中的wide部分可以通过利用交叉特征引入非线性高效的实现记忆能力，但需要依赖人\n",
      "工特征工程。\n",
      "改进：DeepFM在Wide&Deep的基础上进行改进，不需要预训练FM得到隐向量，不需要人工特征工\n",
      "程，能同时学习低阶和高阶的组合特征；FM模块和Deep模块共享FeatureEmbedding部分，可以更快的\n",
      "训练，以及更精确的训练学习。\n",
      "5、推荐领域GBDT+LR的做法了解吗？\n",
      "GBDT+LR由两部分组成，其中GBDT用来对训练集提取特征作为新的训练输入数据，LR作为新训练输\n",
      "入数据的分类器。GBDT+LR的提出意味着特征工程可以完全交由一个独立的模型来完成，模型的输入可以是\n",
      "原始的特征向量，不必在特征工程上投入过多的人工筛选和模型设计的精力，真正实现了端到端的训练。\n",
      "6、粗排有哪些指标？NDCG了解吗？\n",
      "(1)NDCG，排序相似性的指标，看精排的排序结果和粗排有多相似\n",
      "(2)粗排的召回率/重叠率，粗排的topk和精排的topk有多大占比。\n",
      "计算DCG，计算公式如下：\n",
      "其中，rel为这个排序list，结果i的一个等级得分；i是指结果i的当前位置序号；\n",
      "​在搜索引擎中，rel等级得分，是由人工抽样数据，并且根据一定的规则打出来的等级得分。\n",
      "步骤二：\n",
      "计算IDCG（IdealDCG），即完美序的DCG；计算方式也同步骤1，只是排序序列不是由算法得出，而\n",
      "是由人工对序列根据一定的评估准则排出来的最佳序列。\n",
      "​步骤三：根据前面2个步骤的出来的结果，计算NDCG，计算公式如下：</paragraph>\n",
      "Prediction: [Relevant](3) 排序的时间，排序的时间越长越好。\n",
      "(4) 排序的空间，排序的空间越少越好。\n",
      "Score: 0.7340953330576359\n",
      "\u001b[0m\u001b[1;3;34m9/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       8.57 ms /    33 runs   (    0.26 ms per token,  3848.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   79170.12 ms /   978 tokens (   80.95 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:        eval time =    6731.35 ms /    32 runs   (  210.35 ms per token,     4.75 tokens per second)\n",
      "llama_print_timings:       total time =   88402.55 ms /  1010 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第36页共46页第二十七篇：京东广告NLP实习面试题7道\n",
      "1、BeamSearch生成的句子基本都一样，是否有方法扩展生成句子的多样性\n",
      "解决方法：通过分组加入相似性惩罚，具体可以参考论文Diversebeamsearch：\n",
      "https://arxiv.org/pdf/1610.02424.pdf。\n",
      "具体方法：选择Beamsize为B，然后将其分为G组，每一组就有B/G个beam，每个单独的组内跟\n",
      "beamsearch很像，不断延展序列，同时引入一个dissimilarity项来保证组与组之间有差异。\n",
      "组内与beamsearch很像：从t-1到t时刻，不断的减少搜索空间（如同beamsearch一样）。\n",
      "组间差异：对于t=4时刻，我们先对第一组输出y（t=4），然后我们开始对第二组输出y（t=4），但\n",
      "是第二组y（t=4）的score不仅取决于第二组之前的y（t=3），也取决于其与第一组的相似程度。以此类\n",
      "推，在t=4时刻对于第三组的输出，我们从上图可以看到其score的打分标准。这儿对于其dissimilarity项\n",
      "的计算采用的办法是hammingdiversity，这个理解起来很简单，比如这个时刻可能输出的词在上面的组出\n",
      "现过，我们就对这个词的分数-1，如果这个时刻可能输出的词在上面组没有出现过，我们就对这个词的分数不\n",
      "惩罚。\n",
      "2、LayerNormalization和BatchNormalization的区别，padding对这两者有影响吗，\n",
      "对哪一维有影响\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样\n",
      "本的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "padding会对BatchNormalization的seq_len这个维度有影响，计算的时候会把padding也算进去。\n",
      "3、pytorch.Dataloader报错出现数据维度不一样怎么解决。\n",
      "在构建dataset重写的__getitem__方法中要返回相同长度的tensor。\n",
      "可以使用向量补全的方法来解决这个问题，把不同长度的向量补全成等长的。</paragraph>\n",
      "Prediction: [Relevant]I'm sorry, but I'm not sure what you're asking.[No support / Contradictory][No Retrieval]Could you please provide more context or clarify your question?[Utility:4]\n",
      "Score: 1.019263046241107\n",
      "\u001b[0m\u001b[1;3;34m10/10 paragraphs done\n",
      "\n",
      "\u001b[0m\u001b[1;3;34mEnd evaluation\n",
      "\u001b[0m\u001b[1;3;34mSelected the best answer: [Relevant]自我注意力（Self Attention）是一种神经语言模型中的一种重要概念，用于模型中的自我参与和内部连接。[Fully supported]\n",
      "\u001b[0m\u001b[1;3;32mFinal answer: 自我注意力（Self Attention）是一种神经语言模型中的一种重要概念，用于模型中的自我参与和内部连接。\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       8.06 ms /    32 runs   (    0.25 ms per token,  3969.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1814.17 ms /    22 tokens (   82.46 ms per token,    12.13 tokens per second)\n",
      "llama_print_timings:        eval time =    5542.71 ms /    31 runs   (  178.80 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:       total time =    7493.82 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieval required\n",
      "\u001b[0m\u001b[1;3;34mReceived: 10 documents\n",
      "\u001b[0m\u001b[1;3;34mStart evaluation\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.82 ms /    50 runs   (    0.26 ms per token,  3900.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =   51572.05 ms /   666 tokens (   77.44 ms per token,    12.91 tokens per second)\n",
      "llama_print_timings:        eval time =    9812.95 ms /    49 runs   (  200.26 ms per token,     4.99 tokens per second)\n",
      "llama_print_timings:       total time =   62909.45 ms /   715 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第9页共46页型的深度，PostNorm每Norm一次就削弱一次恒等分支的权重，所以PostNorm反而是更突出残差分支\n",
      "的。\n",
      "参考：https://zhuanlan.zhihu.com/p/474988236\n",
      "post-norm在残差之后做归一化，对参数正则化的效果更强，进而模型的鲁棒性也会更好；pre-norm\n",
      "相对于post-norm，因为有一部分参数直接加在了后面，不需要对这部分参数进行正则化，正好可以防止模\n",
      "型的梯度爆炸或者梯度消失，因此，如果层数少post-norm的效果其实要好一些，如果要把层数加大，为了\n",
      "保证模型的训练，pre-norm显然更好一些。\n",
      "5、GPT与Bert的区别\n",
      "1）GPT是单向模型，无法利用上下文信息，只能利用上文；而BERT是双向模型。\n",
      "2）GPT是基于自回归模型，可以应用在NLU和NLG两大任务，而原生的BERT采用的基于自编码模\n",
      "型，只能完成NLU任务，无法直接应用在文本生成上面。\n",
      "6、如何加速Bert模型的训练\n",
      "BERT基线模型的训练使用Adamwithweightdecay（Adam优化器的变体）作为优化器，LAMB是一\n",
      "款通用优化器，它适用于小批量和大批量，且除了学习率以外其他超参数均无需调整。LAMB优化器支持自\n",
      "适应元素级更新（adaptiveelement-wiseupdating）和准确的逐层修正（layer-wisecorrection）。\n",
      "LAMB可将BERT预训练的批量大小扩展到64K，且不会造成准确率损失，76分钟就可以完成BERT的训\n",
      "练。</paragraph>\n",
      "Prediction: [Relevant]参考：https://zhuanlan.zhihu.com/p/474988236\n",
      "LAMB优化器可以将BERT的训练\n",
      "Score: 0.7654733645318321\n",
      "\u001b[0m\u001b[1;3;34m1/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      13.01 ms /    50 runs   (    0.26 ms per token,  3842.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =   83512.51 ms /  1070 tokens (   78.05 ms per token,    12.81 tokens per second)\n",
      "llama_print_timings:        eval time =   10408.16 ms /    49 runs   (  212.41 ms per token,     4.71 tokens per second)\n",
      "llama_print_timings:       total time =   96318.50 ms /  1119 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第8页共46页第十八篇：2022年4月18日字节跳动机器学习AILab一面面试题6道\n",
      "1、BN和LN区别\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "2、讲讲selfattention\n",
      "SelfAttention与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的\n",
      "隐变量（hiddenstate）计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但\n",
      "SelfAttention不同，它分别在source端和target端进行，仅与sourceinput或者targetinput自身相关\n",
      "的SelfAttention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的\n",
      "selfAttention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系。\n",
      "因此，selfAttentionAttention比传统的Attentionmechanism效果要好，主要原因之一是，传统的\n",
      "Attention机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，selfAttention可以不仅可以得\n",
      "到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系。\n",
      "3、Bert的预训练过程\n",
      "Bert的预训练主要包含两个任务，MLM和NSP，MaskedLanguageModel任务可以理解为完形填\n",
      "空，随机mask每一个句子中15%的词，用其上下文来做预测；NextSentencePrediction任务选择一些句\n",
      "子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中\n",
      "的相关性。BERT预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的Loss相加。\n",
      "4、PreNorm与PostNorm的区别？\n",
      "参考：https://kexue.fm/archives/9009\n",
      "在同一设置下，PreNorm（也就是Normandadd）的效果是要优于PostNorm（AddandNorm）\n",
      "的，但是单独调整的话，PostNorm的效果是更好的，PreNorm结构无形地增加了模型的宽度而降低了模</paragraph>\n",
      "Prediction: [Relevant]参考：https://kexue.fm/archives/9009\n",
      "在同一设置下，PreNorm（也就是Normandadd）的效果是要优于\n",
      "Score: 0.5427709817942077\n",
      "\u001b[0m\u001b[1;3;34m2/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.54 ms /    50 runs   (    0.23 ms per token,  4330.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   66041.01 ms /   865 tokens (   76.35 ms per token,    13.10 tokens per second)\n",
      "llama_print_timings:        eval time =    9512.64 ms /    49 runs   (  194.14 ms per token,     5.15 tokens per second)\n",
      "llama_print_timings:       total time =   77551.51 ms /   914 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第39页共46页第二十八篇：京东科技NLP实习面试题10道\n",
      "1、Bert里面为什么用layernormalization，而不用batchnormalization，分别讲一下这\n",
      "两个啥意思。\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "2.、Bert里面为什么Q，K，V要用三个不同的矩阵，用一个不是也行吗\n",
      "如果使用相同的矩阵，相同量级的情况下，q和k进行点积的值会是最大的，进行softmax的加权平均\n",
      "后，该词所占的比重会最大，使得其他词的比重很少，无法有效利用上下文信息来增强当前词的语义表示，而\n",
      "使用不同的QKV后，会很大程度减轻上述的影响。\n",
      "3、Bert和transformer讲一下\n",
      "1bert只有transformer的encode结构，是生成语言模型\n",
      "2bert加入了输入句子的mask机制，在输入的时候会随机mask\n",
      "3模型接收两个句子作为输入，并且预测其中第二个句子是否在原始文档中也是后续句子可以做对话机\n",
      "制的应答。\n",
      "4在训练BERT模型时，MaskedLM和NextSentencePrediction是一起训练的，目标就是要最小化\n",
      "两种策略的组合损失函数。\n",
      "4、AUC指标讲一下\n",
      "AUC：AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。AUC反映的是分\n",
      "类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。</paragraph>\n",
      "Prediction: [Relevant]BERT模型的预训练过程是在句子上进行了卷积和激活函数，以获取句子中的\n",
      "Score: 0.8266907574176545\n",
      "\u001b[0m\u001b[1;3;34m3/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.61 ms /    50 runs   (    0.25 ms per token,  3966.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   76748.86 ms /  1013 tokens (   75.76 ms per token,    13.20 tokens per second)\n",
      "llama_print_timings:        eval time =    9894.06 ms /    49 runs   (  201.92 ms per token,     4.95 tokens per second)\n",
      "llama_print_timings:       total time =   88787.49 ms /  1062 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第15页共46页第二十篇：2022年4月携程暑期实习搜索推荐算法岗面试题8道\n",
      "1、你所理解的推荐系统是什么样的？大致流程？所用模型？\n",
      "推荐系统一方面是为了帮助消费者发现对自己有价值的商品，另一方面是帮助生产者把用户可能感兴趣的\n",
      "商品展现给用户，实现生产者和消费者的双赢。\n",
      "大致流程主要包括：获取用户特征，召回过程，排序过程（粗排、精排）\n",
      "召回模型：\n",
      "·规则召回（兴趣标签top，热门top，新品top等）\n",
      "·协同召回（基于用户的协同过滤，基于商品的协同过滤）\n",
      "·向量召回（FM召回，Item2vec,YoutubeDNN向量召回，GraphEmbedding召回，DSSM双塔召\n",
      "回）\n",
      "排序模型：GBDT+LR、Xgboost、FM/FFM、Wide&Deep、DeepFM、Deep&Cross、DIN、BST\n",
      "等\n",
      "2、双塔模型优势，缺点，如何改进？\n",
      "双塔模型的优势是速度快，但模型精度还有待提升。\n",
      "速度快是因为将所有Item转化成Embedding，并存储进ANN检索系统，比如FAISS，以供查询。类\n",
      "似FAISS这种ANN检索系统对海量数据的查询效率高。\n",
      "而双塔模型为了速度快，必须对用户和Item进行特征分离，而特征分离，又必然导致上述两个原因产生\n",
      "的效果损失。\n",
      "改进：SENet双塔模型，把SENet放在Embedding层之上，目的是通过SENet网络，动态地学习这些\n",
      "特征的重要性：对于每个特征学会一个特征权重，然后再把学习到的权重乘到对应特征的Embedding里，这\n",
      "样就可以动态学习特征权重，通过小权重抑制噪音或者无效低频特征，通过大权重放大重要特征影响的目的。\n",
      "3、粗排的目的是什么？\n",
      "粗排是用来帮精排模型找到那些它本来就要打高分的item，只不过范围更广一些。.按照上面的例子，如\n",
      "果没有粗排，精排模型自己找出来的某top10的item。而粗排的任务就是要找到包含这10个item的一个更\n",
      "小的候选集，既保证了效果，又减少线上预测的负担。</paragraph>\n",
      "Prediction: [Relevant]粗排模型主要是基于协同召回模型，协同召回模型主要是基于协同召回算法\n",
      "Score: 0.7957932645418677\n",
      "\u001b[0m\u001b[1;3;34m4/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.17 ms /    50 runs   (    0.24 ms per token,  4108.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   56925.42 ms /   757 tokens (   75.20 ms per token,    13.30 tokens per second)\n",
      "llama_print_timings:        eval time =    9660.27 ms /    49 runs   (  197.15 ms per token,     5.07 tokens per second)\n",
      "llama_print_timings:       total time =   68296.89 ms /   806 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第21页共46页第二十二篇：2022年5月18日2023届广联达提前批面试题5道\n",
      "1、Transformer中encoder和decoder的区别（结构和功能上）\n",
      "结构上：Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个\n",
      "Block，Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder\n",
      "端的Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一\n",
      "个前馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和Layer\n",
      "Normalization层。\n",
      "功能上：Transformer中encoder是双向的，是编码器；decoder是单向的，是解码器，可以用于生成\n",
      "任务。\n",
      "2、有哪些防止过拟合的方法\n",
      "数据的角度：获取和使用更多的数据（数据集增强）；\n",
      "模型角度：降低模型复杂度、L1\\L2\\Dropout正则化、Earlystopping（提前终止）\n",
      "模型融合的角度：使用bagging等模型融合方法。\n",
      "3、L1和L2正则化为什么可以防止过拟合\n",
      "正规化是防止过拟合的一种重要技巧。正则化通过降低模型的复杂性，达到避免过拟合的问题。这里的\n",
      "降低模型的复杂性可以理解为：\n",
      "L1将很多权重变成0，这样起到作用的因素就会减少。\n",
      "L2使权重都趋于0，这样就不会有某个权重占比特别大。\n",
      "4、传统机器学习方法了解多少\n",
      "这个可以说逻辑回归模型，逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度\n",
      "下降法来求解参数，从而达到将数据二分类的目的。\n",
      "逻辑回归的优缺点\n",
      "优点：</paragraph>\n",
      "Prediction: [Relevant]这里的“逻辑回归”可以理解为一种基本的机器学习模型，其中“极大似然”可以理\n",
      "Score: 0.6839272548977428\n",
      "\u001b[0m\u001b[1;3;34m5/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.89 ms /    50 runs   (    0.26 ms per token,  3877.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47888.26 ms /   616 tokens (   77.74 ms per token,    12.86 tokens per second)\n",
      "llama_print_timings:        eval time =    9587.14 ms /    49 runs   (  195.66 ms per token,     5.11 tokens per second)\n",
      "llama_print_timings:       total time =   58939.28 ms /   665 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第35页共46页beamsearch是介于全局搜索和贪婪搜索之间。使用beamsize参数来限制在每一步保留下来的可能性\n",
      "词的数量。beamsearch是在测试阶段为了获得更好准确性而采取的一种策略，在训练阶段无需使用。\n",
      "相比暴力搜索的全局最优解，降低时间复杂度的方法就是寻找次优解，具体就是把搜索空间中的N减下\n",
      "来，每一步计算完只保留K个(beamsize)最大的取值路径，这样时间复杂度降为O(K*N*T)，K取值一般比\n",
      "N小很多。这样得到的虽然不是最优解，但是在seq2seq模型的推理预测中可以兼顾时间和效果。\n",
      "5、你了解的文本表示方法有哪些\n",
      "基于one-hot、tf-idf、textrank等的bag-of-words；\n",
      "主题模型：LSA（SVD）、pLSA、LDA；\n",
      "基于词向量的固定表征：word2vec、fastText、glove\n",
      "基于词向量的动态表征：elmo、GPT、bert\n",
      "对比如下：\n",
      "One-hot表示：维度灾难、语义鸿沟；\n",
      "矩阵分解（LSA）：利用全局语料特征，但SVD求解计算复杂度大；\n",
      "基于NNLM/RNNLM的词向量：词向量为副产物，存在效率不高等问题；\n",
      "word2vec、fastText：优化效率高，但是基于局部语料；\n",
      "glove：基于全局预料，结合了LSA和word2vec的优点；\n",
      "elmo、GPT、bert：动态特征。</paragraph>\n",
      "Prediction: [Relevant]GPT-2、BERT等模型基于词向量动态表示，其中BERT使用了Transformer模型，可以实现语义相关性，语义相关性\n",
      "Score: 0.703241653366312\n",
      "\u001b[0m\u001b[1;3;34m6/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.34 ms /    49 runs   (    0.25 ms per token,  3972.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   79042.70 ms /  1039 tokens (   76.08 ms per token,    13.14 tokens per second)\n",
      "llama_print_timings:        eval time =    9871.49 ms /    48 runs   (  205.66 ms per token,     4.86 tokens per second)\n",
      "llama_print_timings:       total time =   91288.58 ms /  1087 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第27页共46页第二十四篇：2022年5月10日美团搜索推荐算法面试题10道\n",
      "1、介绍下推荐系统的流程\n",
      "推荐系统的流程主要包含一下几个阶段。\n",
      "索引&特征:会根据内容特性提前建立若干种类型的索引。\n",
      "召回阶段:用户请求时会从各种索引种取出千/万条item。\n",
      "粗排阶段:针对这上千/万条item，进行第一遍打分，再筛选出几百条或者千条。这个阶段的排序模型一\n",
      "般都比较简单，能够过滤掉一些与用户兴趣明显不相关的。\n",
      "精排阶段:得到几百条item后，精排阶段会建立相对精细的模型，根据用户的画像，偏好，上下文，结\n",
      "合业务目标进行排序。一般精排后返回50-100条给到engine侧。\n",
      "重排阶段:engine侧拿到精排的50条item。还会做很多的人工干预和产品逻辑，比如item之间的多样\n",
      "性，产品策略逻辑，比如热门，置顶，多种内容之间的位置混合等等。最终会返回5-10条左右的item，曝\n",
      "光给客户端。根据业务特性，在线流程还有许多比较细的模块，比如去重服务，避免给用户推荐重复的内容。\n",
      "特征预处理，特征抽取等模块。\n",
      "2、召回和排序的差异？\n",
      "召回的目的在于减少候选的数量（尽量控制在1000以内），方便后续排序环节使用复杂模型精准排序；\n",
      "因为在短时间内评估海量候选，所以召回的关键点是个快字，受限与此与排序相比，召回的算法模型相对简\n",
      "单，使用的特征比较少。而排序模型相对更加复杂，更追求准确性，使用的特征也会较多。\n",
      "3、结果f1提升的1%怎么保证有效性，如何保证置信呢？\n",
      "实验过程中固定随机种子、多次实验取平均\n",
      "4、固定随机种子后，多次实验结果相同吗？\n",
      "还是会有细微差别，因为在梯度传播过程，梯度（浮点数）精度有差异，随着神经网络层数的增加，梯度\n",
      "差异会从小数后面的位置往前跑。只能设置浮点数精度增加来缓解这个问题。</paragraph>\n",
      "Prediction: [Relevant]因此，在实验中，我们需要确保我们的实验数据是一致的，以确保我们的结果是可信的。[No support / Contradictory][Utility:5]\n",
      "Score: 1.2328546356547072\n",
      "\u001b[0m\u001b[1;3;34m7/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.63 ms /    50 runs   (    0.25 ms per token,  3958.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   87705.56 ms /  1149 tokens (   76.33 ms per token,    13.10 tokens per second)\n",
      "llama_print_timings:        eval time =   10363.05 ms /    49 runs   (  211.49 ms per token,     4.73 tokens per second)\n",
      "llama_print_timings:       total time =  100599.21 ms /  1198 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第1页共46页目录\n",
      "第十七篇：2022年4月10日百度机器学习方向暑期实习面试题6道.4\n",
      "1、介绍下SVM算法.4\n",
      "2、介绍下逻辑回归算法.4\n",
      "3、介绍下决策树算法.5\n",
      "4、用通俗的语言介绍下强化学习（ReinforcementLearning）.5\n",
      "5、Leetcode34在排序数组中查找元素的第一个和最后一个位置.5\n",
      "6、Letcode102层序遍历.6\n",
      "第十八篇：2022年4月18日字节跳动机器学习AILab一面面试题6道.8\n",
      "1、BN和LN区别.8\n",
      "2、讲讲selfattention.8\n",
      "3、Bert的预训练过程.8\n",
      "4、PreNorm与PostNorm的区别？8\n",
      "5、GPT与Bert的区别.9\n",
      "6、如何加速Bert模型的训练.9\n",
      "第十九篇：2022年4月8日字节跳动机抖音APP推荐实习面试题8道.11\n",
      "1、AUC是什么？如何计算AUC？11\n",
      "2、AUC线上线下不一致怎么办.11\n",
      "3、召回阶段的负采样是怎么做的？12\n",
      "4、FM，DeepFM跟FFM的对比.12\n",
      "5、手撕FM的训练过程.12\n",
      "6、Leetcode—64.最小路径和.13\n",
      "7、剑指Offer10-I.斐波那契数列.14\n",
      "8、Leetcode—215.数组中的第K个最大元素.14\n",
      "第二十篇：2022年4月携程暑期实习搜索推荐算法岗面试题8道.15\n",
      "1、你所理解的推荐系统是什么样的？大致流程？所用模型？15\n",
      "2、双塔模型优势，缺点，如何改进？15\n",
      "3、粗排的目的是什么？15\n",
      "4、wide&deep模型为什么要有wide层结构，优缺点，如何改进？16\n",
      "5、推荐领域GBDT+LR的做法了解吗？16\n",
      "6、粗排有哪些指标？NDCG了解吗？16\n",
      "7、ROC，PR曲线含义，坐标轴代表什么？17\n",
      "8、AUC怎么求，实际意义？17\n",
      "第二十一篇：2022年4月14日美团计算机视觉算法暑期实习面试题6道.18\n",
      "1、简单介绍gbdt算法的原理.18\n",
      "2、pca属于有监督还是无监督.18\n",
      "3、介绍svm算法.18\n",
      "4、介绍transformer算法.19\n",
      "5、layernorm和batchnorm的比较.19\n",
      "6、Leetcode—两数之和.19\n",
      "第二十二篇：2022年5月18日2023届广联达提前批面试题5道.21\n",
      "1、Transformer中encoder和decoder的区别（结构和功能上）.21\n",
      "2、有哪些防止过拟合的方法.21\n",
      "3、L1和L2正则化为什么可以防止过拟合.21\n",
      "4、传统机器学习方法了解多少.21</paragraph>\n",
      "Prediction: [Relevant]这些内容主要包括了BERT模型的基本概念和预训练过程，以及其他相关知识。[No support / Contradictory]\n",
      "Score: 0.8003372310175744\n",
      "\u001b[0m\u001b[1;3;34m8/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.80 ms /    51 runs   (    0.25 ms per token,  3982.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   55412.06 ms /   704 tokens (   78.71 ms per token,    12.70 tokens per second)\n",
      "llama_print_timings:        eval time =    9905.00 ms /    50 runs   (  198.10 ms per token,     5.05 tokens per second)\n",
      "llama_print_timings:       total time =   67093.09 ms /   754 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第42页共46页第二十九篇：2022年6月24日字节电商CV实习岗面试题10道\n",
      "1、如何解决类别极度不平衡的问题？\n",
      "2、说下Transformer模型\n",
      "Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个Block，\n",
      "Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder端的\n",
      "Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一个前\n",
      "馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和LayerNormalization\n",
      "层。\n",
      "3、说下FocalLoss\n",
      "Focalloss是目标检测中解决正负样本严重不平衡的方法，在标准交叉熵损失基础上修改得到的。这个函\n",
      "数可以通过减少易分类样本的权重，使得模型在训练时更专注于稀疏的难分类的样本；防止大量易分类负样本\n",
      "在loss中占主导地位。\n",
      "4、介绍下深度可分离卷积和传统卷积的区别\n",
      "传统的卷积是各个通道上采用相不同的卷积核，然后不同的卷积核用于提取不同方面的特征。\n",
      "深度可分离卷积先在各个通道上采用不同的卷积核提取不同的特征，但是这样对于某个通道来说，就只提\n",
      "取了一方面的特征，因此在此基础上加入点卷积，用1*1的卷积对提取特征后的特征图再次提取不同方面的\n",
      "特征，最终产生和普通卷积相同的输出特征图。</paragraph>\n",
      "Prediction: [Relevant]这里是Bert的预训练过程：\n",
      "\n",
      "1.[No support / Contradictory][No Retrieval]数据集选择和数据准备\n",
      "2.[No Retrieval]模型选择\n",
      "Score: 0.5538953334975258\n",
      "\u001b[0m\u001b[1;3;34m9/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.61 ms /    50 runs   (    0.25 ms per token,  3966.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =   69612.06 ms /   878 tokens (   79.28 ms per token,    12.61 tokens per second)\n",
      "llama_print_timings:        eval time =    9874.28 ms /    49 runs   (  201.52 ms per token,     4.96 tokens per second)\n",
      "llama_print_timings:       total time =   81595.12 ms /   927 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第43页共46页好处是这样做大大节约了参数量。\n",
      "5、如何防止过拟合\n",
      "数据的角度：获取和使用更多的数据（数据集增强）；\n",
      "模型角度：降低模型复杂度、L1\\L2\\Dropout正则化、Earlystopping（提前终止）\n",
      "模型融合的角度：使用bagging等模型融合方法。\n",
      "6、BN在训练和测试的时候的区别？可以防止过拟合吗？\n",
      "对于BN，在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。\n",
      "而在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全\n",
      "量训练数据的均值和方差，这个可以通过移动平均法求得。\n",
      "对于BN，当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，gamma和bata。\n",
      "BN算法防止过拟合：在网络的训练中，BN的使用使得一个minibatch中所有样本都被关联在了一起，\n",
      "因此网络不会从某一个训练样本中生成确定的结果，即同样一个样本的输出不再仅仅取决于样本的本身，也取\n",
      "决于跟这个样本同属一个batch的其他样本，而每次网络都是随机取batch，这样就会使得整个网络不会朝\n",
      "这一个方向使劲学习。一定程度上避免了过拟合。\n",
      "7、什么是AUC？\n",
      "AUC：AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。AUC反映的是分\n",
      "类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。\n",
      "8、卷积核计算公式\n",
      "卷积层计算公式如下：\n",
      "其中，W为输入大小，K为卷积核大小，P为padding大小，S为步幅。\n",
      "如果，想保持卷积前后的特征图大小相同，通常会设定padding为：</paragraph>\n",
      "Prediction: [Relevant]卷积核计算公式如下：\n",
      "其中，W为输入大小，K为卷积核大小，P为padding大小，S为\n",
      "Score: 0.7518601301343627\n",
      "\u001b[0m\u001b[1;3;34m10/10 paragraphs done\n",
      "\n",
      "\u001b[0m\u001b[1;3;34mEnd evaluation\n",
      "\u001b[0m\u001b[1;3;34mSelected the best answer: [Relevant]因此，在实验中，我们需要确保我们的实验数据是一致的，以确保我们的结果是可信的。[No support / Contradictory][Utility:5]\n",
      "\u001b[0m\u001b[1;3;32mFinal answer: 因此，在实验中，我们需要确保我们的实验数据是一致的，以确保我们的结果是可信的。\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       8.42 ms /    33 runs   (    0.26 ms per token,  3921.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1420.97 ms /    18 tokens (   78.94 ms per token,    12.67 tokens per second)\n",
      "llama_print_timings:        eval time =    5801.54 ms /    32 runs   (  181.30 ms per token,     5.52 tokens per second)\n",
      "llama_print_timings:       total time =    7358.00 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieval required\n",
      "\u001b[0m\u001b[1;3;34mReceived: 10 documents\n",
      "\u001b[0m\u001b[1;3;34mStart evaluation\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.29 ms /    50 runs   (    0.25 ms per token,  4070.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   50071.12 ms /   666 tokens (   75.18 ms per token,    13.30 tokens per second)\n",
      "llama_print_timings:        eval time =    9549.31 ms /    49 runs   (  194.88 ms per token,     5.13 tokens per second)\n",
      "llama_print_timings:       total time =   61148.81 ms /   715 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第9页共46页型的深度，PostNorm每Norm一次就削弱一次恒等分支的权重，所以PostNorm反而是更突出残差分支\n",
      "的。\n",
      "参考：https://zhuanlan.zhihu.com/p/474988236\n",
      "post-norm在残差之后做归一化，对参数正则化的效果更强，进而模型的鲁棒性也会更好；pre-norm\n",
      "相对于post-norm，因为有一部分参数直接加在了后面，不需要对这部分参数进行正则化，正好可以防止模\n",
      "型的梯度爆炸或者梯度消失，因此，如果层数少post-norm的效果其实要好一些，如果要把层数加大，为了\n",
      "保证模型的训练，pre-norm显然更好一些。\n",
      "5、GPT与Bert的区别\n",
      "1）GPT是单向模型，无法利用上下文信息，只能利用上文；而BERT是双向模型。\n",
      "2）GPT是基于自回归模型，可以应用在NLU和NLG两大任务，而原生的BERT采用的基于自编码模\n",
      "型，只能完成NLU任务，无法直接应用在文本生成上面。\n",
      "6、如何加速Bert模型的训练\n",
      "BERT基线模型的训练使用Adamwithweightdecay（Adam优化器的变体）作为优化器，LAMB是一\n",
      "款通用优化器，它适用于小批量和大批量，且除了学习率以外其他超参数均无需调整。LAMB优化器支持自\n",
      "适应元素级更新（adaptiveelement-wiseupdating）和准确的逐层修正（layer-wisecorrection）。\n",
      "LAMB可将BERT预训练的批量大小扩展到64K，且不会造成准确率损失，76分钟就可以完成BERT的训\n",
      "练。</paragraph>\n",
      "Prediction: [Relevant]1) GPT is a unidirectional model, unable to utilize context information, only able to utilize previous text.[No support / Contradictory][No Retrieval]2) GPT is based on a recurrent model, able to be used for both NL\n",
      "Score: 0.8096211372466848\n",
      "\u001b[0m\u001b[1;3;34m1/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       6.78 ms /    28 runs   (    0.24 ms per token,  4130.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   54356.72 ms /   723 tokens (   75.18 ms per token,    13.30 tokens per second)\n",
      "llama_print_timings:        eval time =    5290.65 ms /    27 runs   (  195.95 ms per token,     5.10 tokens per second)\n",
      "llama_print_timings:       total time =   61162.30 ms /   750 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第40页共46页5、Precision和Recall讲一下\n",
      "精确度（precision）/查准率：TP/（TP+FP）=TP/P 预测为真中，实际为正样本的概率\n",
      "召回率（recall）/查全率：TP/（TP+FN）正样本中，被识别为真的概率。\n",
      "6、GBDT和Xgboost的区别\n",
      "1）GBDT是机器学习算法，XGBoost是该算法的一种工程实现\n",
      "2）XGBoost在使用CART作为基学习器时，加入了正则项来控制模型的复杂度，有利于防止过拟合，从\n",
      "而提高模型的泛化能力\n",
      "3）GBDT在模型训练时只使用了损失函数的一阶导数信息，XGBoost对损失函数进行二阶泰勒展开，可\n",
      "以同时使用一阶和二阶导数\n",
      "4）XGBoost支持自定义损失函数，增强了模型的扩展性\n",
      "5）传统的GBDT采用CART作为基学习器（也叫基分类器），XGBoost支持多种类型的基学习器，包\n",
      "括树模型（gbtree和dart，dart为一种引入dropout的树模型）和线性模型（gblinear），默认为gbtree\n",
      "6）传统的GBDT在每轮迭代时使用全部的数据，XGBoost支持对数据进列采样，即特征采样，有利于\n",
      "防止过拟合，同时可以减少计算量，提高训练的效率\n",
      "7）传统的GBDT不能支持缺失值的处理（必须填充），XGBoost支持缺失值的处理，能够自动学习出\n",
      "缺失值的分裂方向（无需填充）\n",
      "7、Xgboost叶子结点的值怎么计算的\n",
      "XGBoost目标函数最终推导形式如下：\n",
      "利用一元二次函数求最值的知识，当目标函数达到最小值Obj*时，每个叶子结点的权重为wj*。\n",
      "具体公式如下：</paragraph>\n",
      "Prediction: [Relevant]Bert 是一种语言模型，其中 GPT 是一种模型类型。[No support / Contradictory][Utility:5]\n",
      "Score: 1.3977941486621372\n",
      "\u001b[0m\u001b[1;3;34m2/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.49 ms /    50 runs   (    0.25 ms per token,  4004.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =   65597.55 ms /   861 tokens (   76.19 ms per token,    13.13 tokens per second)\n",
      "llama_print_timings:        eval time =    9776.02 ms /    49 runs   (  199.51 ms per token,     5.01 tokens per second)\n",
      "llama_print_timings:       total time =   77342.80 ms /   910 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第18页共46页第二十一篇：2022年4月14日美团计算机视觉算法暑期实习面试题6道\n",
      "1、简单介绍gbdt算法的原理\n",
      "GBDT是梯度提升决策树，是一种基于Boosting的算法，采用以决策树为基学习器的加法模型，通过不\n",
      "断拟合上一个弱学习器的残差，最终实现分类或回归的模型。关键在于利用损失函数的负梯度在当前模型的值\n",
      "作为残差的近似值，从而拟合一个回归树。对于分类问题：常使用指数损失函数；对于回归问题：常使用平方\n",
      "误差损失函数（此时，其负梯度就是通常意义的残差），对于一般损失函数来说就是残差的近似。\n",
      "无论损失函数是什么形式，每个决策树拟合的都是负梯度。准确的说，不是用负梯度代替残差，而是当损\n",
      "失函数是均方损失时，负梯度刚好是残差，残差只是特例。\n",
      "2、pca属于有监督还是无监督\n",
      "PCA按有监督和无监督划分应该属于无监督学习，所以数据集有无y并不重要，只是改变样本X的属性\n",
      "(特征)维度。\n",
      "3、介绍svm算法\n",
      "是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感\n",
      "知机。\n",
      "SVM可分为三种：\n",
      "线性可分SVM\n",
      "当训练数据线性可分时，通过最大化硬间隔（hardmargin）可以学习得到一个线性分类器，即硬间隔\n",
      "SVM。\n",
      "线性SVM\n",
      "当训练数据不能线性可分但是近似线性可分时，通过最大化软间隔（softmargin）也可以学习到一个线\n",
      "性分类器，即软间隔SVM。\n",
      "非线性SVM\n",
      "当训练数据线性不可分时，通过使用核技巧（kerneltrick）和最大化软间隔，可以学习到一个非线性\n",
      "SVM。</paragraph>\n",
      "Prediction: [Irrelevant]GBDT是梯度提升决策树，是一种基于Boosting的算法，采用以决策树为基\n",
      "Score: 0.48440533202222463\n",
      "\u001b[0m\u001b[1;3;34m3/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.79 ms /    50 runs   (    0.26 ms per token,  3908.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   81088.55 ms /  1070 tokens (   75.78 ms per token,    13.20 tokens per second)\n",
      "llama_print_timings:        eval time =   10249.40 ms /    49 runs   (  209.17 ms per token,     4.78 tokens per second)\n",
      "llama_print_timings:       total time =   93655.99 ms /  1119 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第8页共46页第十八篇：2022年4月18日字节跳动机器学习AILab一面面试题6道\n",
      "1、BN和LN区别\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "2、讲讲selfattention\n",
      "SelfAttention与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的\n",
      "隐变量（hiddenstate）计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但\n",
      "SelfAttention不同，它分别在source端和target端进行，仅与sourceinput或者targetinput自身相关\n",
      "的SelfAttention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的\n",
      "selfAttention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系。\n",
      "因此，selfAttentionAttention比传统的Attentionmechanism效果要好，主要原因之一是，传统的\n",
      "Attention机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，selfAttention可以不仅可以得\n",
      "到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系。\n",
      "3、Bert的预训练过程\n",
      "Bert的预训练主要包含两个任务，MLM和NSP，MaskedLanguageModel任务可以理解为完形填\n",
      "空，随机mask每一个句子中15%的词，用其上下文来做预测；NextSentencePrediction任务选择一些句\n",
      "子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中\n",
      "的相关性。BERT预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的Loss相加。\n",
      "4、PreNorm与PostNorm的区别？\n",
      "参考：https://kexue.fm/archives/9009\n",
      "在同一设置下，PreNorm（也就是Normandadd）的效果是要优于PostNorm（AddandNorm）\n",
      "的，但是单独调整的话，PostNorm的效果是更好的，PreNorm结构无形地增加了模型的宽度而降低了模</paragraph>\n",
      "Prediction: [Relevant]Bert是一种基于Transformer语言模型的模型，其中使用了一种特殊的预处理技术，通过将输入序列中一部\n",
      "Score: 0.5673533465102469\n",
      "\u001b[0m\u001b[1;3;34m4/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.40 ms /    50 runs   (    0.25 ms per token,  4032.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =   68036.69 ms /   865 tokens (   78.66 ms per token,    12.71 tokens per second)\n",
      "llama_print_timings:        eval time =    9523.69 ms /    49 runs   (  194.36 ms per token,     5.15 tokens per second)\n",
      "llama_print_timings:       total time =   79452.98 ms /   914 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第39页共46页第二十八篇：京东科技NLP实习面试题10道\n",
      "1、Bert里面为什么用layernormalization，而不用batchnormalization，分别讲一下这\n",
      "两个啥意思。\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "2.、Bert里面为什么Q，K，V要用三个不同的矩阵，用一个不是也行吗\n",
      "如果使用相同的矩阵，相同量级的情况下，q和k进行点积的值会是最大的，进行softmax的加权平均\n",
      "后，该词所占的比重会最大，使得其他词的比重很少，无法有效利用上下文信息来增强当前词的语义表示，而\n",
      "使用不同的QKV后，会很大程度减轻上述的影响。\n",
      "3、Bert和transformer讲一下\n",
      "1bert只有transformer的encode结构，是生成语言模型\n",
      "2bert加入了输入句子的mask机制，在输入的时候会随机mask\n",
      "3模型接收两个句子作为输入，并且预测其中第二个句子是否在原始文档中也是后续句子可以做对话机\n",
      "制的应答。\n",
      "4在训练BERT模型时，MaskedLM和NextSentencePrediction是一起训练的，目标就是要最小化\n",
      "两种策略的组合损失函数。\n",
      "4、AUC指标讲一下\n",
      "AUC：AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。AUC反映的是分\n",
      "类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。</paragraph>\n",
      "Prediction: [Relevant]AUC指标讲一下\n",
      "AUC：AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取\n",
      "Score: 0.8266546761612655\n",
      "\u001b[0m\u001b[1;3;34m5/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.94 ms /    50 runs   (    0.24 ms per token,  4188.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   74242.79 ms /   957 tokens (   77.58 ms per token,    12.89 tokens per second)\n",
      "llama_print_timings:        eval time =    9810.98 ms /    49 runs   (  200.22 ms per token,     4.99 tokens per second)\n",
      "llama_print_timings:       total time =   86214.38 ms /  1006 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第34页共46页第二十六篇：京东健康NLP实习面试题5道\n",
      "1、RNN为什么会出现梯度消失的现象\n",
      "梯度消失的原因：很难捕捉到长期的依赖关系，因为乘法梯度可以随着层的数量呈指数递减/递增。但需\n",
      "要注意的是RNN中总的梯度是不会消失的。即便梯度越传越弱，那也只是远距离的梯度消失，由于近距离的\n",
      "梯度不会消失，所有梯度之和便不会消失。RNN所谓梯度消失的真正含义是，梯度被近距离梯度主导，导致\n",
      "模型难以学到远距离的依赖关系。\n",
      "2、RNN和LSTM的区别\n",
      "RNN没有细胞状态；LSTM通过细胞状态记忆信息。\n",
      "RNN激活函数只有tanh；LSTM通过输入门、遗忘门、输出门引入sigmoid函数并结合tanh函数，添\n",
      "加求和操作，减少梯度消失和梯度爆炸的可能性。\n",
      "RNN只能够处理短期依赖问题；LSTM既能够处理短期依赖问题，又能够处理长期依赖问题。\n",
      "3、word2vec有几种方式\n",
      "word2vec有CBOW模型和Skip-Gram模型。\n",
      "CBOW模型：输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向\n",
      "量。（ContinuousBag-of-Word）\n",
      "Skip-Gram模型：输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。\n",
      "有两种改进方式：\n",
      "一种是基于HierarchicalSoftmax的，另一种是基于NegativeSampling的。\n",
      "4、greedysearch和beamsearch的区别\n",
      "greedysearch比较简单，就是贪婪式的搜索，每一步都选择概率最大的单词输出，最后组成整个句子输\n",
      "出。这种方法给出的结果一般情况结果比较差，因为只考虑了每一步的最优解，往往里全局最优解差距很大。\n",
      "优点：计算速度快，每次取概率最大的词。缺点：局部最优并不等于全局最好的，而且一旦选错了，后\n",
      "续生成的内容很可能也是错误的，具有错误的累加效果。</paragraph>\n",
      "Prediction: [Relevant]Bert 是一种基于 Transformer 模型的语言模型，其中 GPT 是一种基于 Recurrent Neural Networks (RNN) 的语言模型。[Partially supported][Utility:5]\n",
      "Score: 1.6567812233689607\n",
      "\u001b[0m\u001b[1;3;34m6/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.62 ms /    50 runs   (    0.23 ms per token,  4304.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47042.24 ms /   615 tokens (   76.49 ms per token,    13.07 tokens per second)\n",
      "llama_print_timings:        eval time =    9367.69 ms /    49 runs   (  191.18 ms per token,     5.23 tokens per second)\n",
      "llama_print_timings:       total time =   57898.84 ms /   664 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第35页共46页beamsearch是介于全局搜索和贪婪搜索之间。使用beamsize参数来限制在每一步保留下来的可能性\n",
      "词的数量。beamsearch是在测试阶段为了获得更好准确性而采取的一种策略，在训练阶段无需使用。\n",
      "相比暴力搜索的全局最优解，降低时间复杂度的方法就是寻找次优解，具体就是把搜索空间中的N减下\n",
      "来，每一步计算完只保留K个(beamsize)最大的取值路径，这样时间复杂度降为O(K*N*T)，K取值一般比\n",
      "N小很多。这样得到的虽然不是最优解，但是在seq2seq模型的推理预测中可以兼顾时间和效果。\n",
      "5、你了解的文本表示方法有哪些\n",
      "基于one-hot、tf-idf、textrank等的bag-of-words；\n",
      "主题模型：LSA（SVD）、pLSA、LDA；\n",
      "基于词向量的固定表征：word2vec、fastText、glove\n",
      "基于词向量的动态表征：elmo、GPT、bert\n",
      "对比如下：\n",
      "One-hot表示：维度灾难、语义鸿沟；\n",
      "矩阵分解（LSA）：利用全局语料特征，但SVD求解计算复杂度大；\n",
      "基于NNLM/RNNLM的词向量：词向量为副产物，存在效率不高等问题；\n",
      "word2vec、fastText：优化效率高，但是基于局部语料；\n",
      "glove：基于全局预料，结合了LSA和word2vec的优点；\n",
      "elmo、GPT、bert：动态特征。</paragraph>\n",
      "Prediction: [Relevant]GPT 是一种基于神经网络的模型，其中一个主要特点是它可以在任何时间进行更新，因此可以在任何\n",
      "Score: 0.6970225760683694\n",
      "\u001b[0m\u001b[1;3;34m7/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.68 ms /    50 runs   (    0.23 ms per token,  4281.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   78922.37 ms /  1013 tokens (   77.91 ms per token,    12.84 tokens per second)\n",
      "llama_print_timings:        eval time =   10233.62 ms /    49 runs   (  208.85 ms per token,     4.79 tokens per second)\n",
      "llama_print_timings:       total time =   91537.28 ms /  1062 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第15页共46页第二十篇：2022年4月携程暑期实习搜索推荐算法岗面试题8道\n",
      "1、你所理解的推荐系统是什么样的？大致流程？所用模型？\n",
      "推荐系统一方面是为了帮助消费者发现对自己有价值的商品，另一方面是帮助生产者把用户可能感兴趣的\n",
      "商品展现给用户，实现生产者和消费者的双赢。\n",
      "大致流程主要包括：获取用户特征，召回过程，排序过程（粗排、精排）\n",
      "召回模型：\n",
      "·规则召回（兴趣标签top，热门top，新品top等）\n",
      "·协同召回（基于用户的协同过滤，基于商品的协同过滤）\n",
      "·向量召回（FM召回，Item2vec,YoutubeDNN向量召回，GraphEmbedding召回，DSSM双塔召\n",
      "回）\n",
      "排序模型：GBDT+LR、Xgboost、FM/FFM、Wide&Deep、DeepFM、Deep&Cross、DIN、BST\n",
      "等\n",
      "2、双塔模型优势，缺点，如何改进？\n",
      "双塔模型的优势是速度快，但模型精度还有待提升。\n",
      "速度快是因为将所有Item转化成Embedding，并存储进ANN检索系统，比如FAISS，以供查询。类\n",
      "似FAISS这种ANN检索系统对海量数据的查询效率高。\n",
      "而双塔模型为了速度快，必须对用户和Item进行特征分离，而特征分离，又必然导致上述两个原因产生\n",
      "的效果损失。\n",
      "改进：SENet双塔模型，把SENet放在Embedding层之上，目的是通过SENet网络，动态地学习这些\n",
      "特征的重要性：对于每个特征学会一个特征权重，然后再把学习到的权重乘到对应特征的Embedding里，这\n",
      "样就可以动态学习特征权重，通过小权重抑制噪音或者无效低频特征，通过大权重放大重要特征影响的目的。\n",
      "3、粗排的目的是什么？\n",
      "粗排是用来帮精排模型找到那些它本来就要打高分的item，只不过范围更广一些。.按照上面的例子，如\n",
      "果没有粗排，精排模型自己找出来的某top10的item。而粗排的任务就是要找到包含这10个item的一个更\n",
      "小的候选集，既保证了效果，又减少线上预测的负担。</paragraph>\n",
      "Prediction: [Relevant]粗排模型主要是基于用户的协同过滤，基于商品的协同过滤，以及基于用户和商品的\n",
      "Score: 0.7860490517079531\n",
      "\u001b[0m\u001b[1;3;34m8/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       8.25 ms /    36 runs   (    0.23 ms per token,  4366.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   61339.77 ms /   800 tokens (   76.67 ms per token,    13.04 tokens per second)\n",
      "llama_print_timings:        eval time =    6873.96 ms /    35 runs   (  196.40 ms per token,     5.09 tokens per second)\n",
      "llama_print_timings:       total time =   70119.39 ms /   835 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第22页共46页形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权\n",
      "重值比较高，那么这个特征最后对结果的影响会比较大。\n",
      "模型效果不错。在工程上是可以接受的（作为baseline），如果特征工程做的好，效果不会太差，并且\n",
      "特征工程可以并行开发，大大加快开发的速度。\n",
      "训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。\n",
      "缺点：\n",
      "准确率欠佳。因为形式非常的简单，而现实中的数据非常复杂，因此，很难达到很高的准确性。很难处理\n",
      "数据不平衡的问题。\n",
      "5、生成式模型和判别式模型的区别并举一些例子\n",
      "生成模型：学习得到联合概率分布P(x,y)，即特征x，共同出现的概率。\n",
      "常见的生成模型：朴素贝叶斯模型，混合高斯模型，HMM模型。\n",
      "判别模型：学习得到条件概率分布P(y|x)，即在特征x出现的情况下标记y出现的概率。\n",
      "常见的判别模型：感知机，决策树，逻辑回归，SVM，CRF等。\n",
      "判别式模型：要确定一个羊是山羊还是绵羊，用判别式模型的方法是从历史数据中学习到模型，然后通过\n",
      "提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。\n",
      "生成式模型：是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模\n",
      "型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，再放到绵羊模型中看概率是多少，哪个大就是\n",
      "哪个。</paragraph>\n",
      "Prediction: [Relevant]Bert 是一种语言模型，其中包含了一个语言模型和一个位置模型。[Partially supported][Utility:5]\n",
      "Score: 1.4693892065910208\n",
      "\u001b[0m\u001b[1;3;34m9/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.20 ms /    50 runs   (    0.22 ms per token,  4462.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   74968.58 ms /   978 tokens (   76.65 ms per token,    13.05 tokens per second)\n",
      "llama_print_timings:        eval time =    9890.43 ms /    49 runs   (  201.85 ms per token,     4.95 tokens per second)\n",
      "llama_print_timings:       total time =   87065.56 ms /  1027 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第36页共46页第二十七篇：京东广告NLP实习面试题7道\n",
      "1、BeamSearch生成的句子基本都一样，是否有方法扩展生成句子的多样性\n",
      "解决方法：通过分组加入相似性惩罚，具体可以参考论文Diversebeamsearch：\n",
      "https://arxiv.org/pdf/1610.02424.pdf。\n",
      "具体方法：选择Beamsize为B，然后将其分为G组，每一组就有B/G个beam，每个单独的组内跟\n",
      "beamsearch很像，不断延展序列，同时引入一个dissimilarity项来保证组与组之间有差异。\n",
      "组内与beamsearch很像：从t-1到t时刻，不断的减少搜索空间（如同beamsearch一样）。\n",
      "组间差异：对于t=4时刻，我们先对第一组输出y（t=4），然后我们开始对第二组输出y（t=4），但\n",
      "是第二组y（t=4）的score不仅取决于第二组之前的y（t=3），也取决于其与第一组的相似程度。以此类\n",
      "推，在t=4时刻对于第三组的输出，我们从上图可以看到其score的打分标准。这儿对于其dissimilarity项\n",
      "的计算采用的办法是hammingdiversity，这个理解起来很简单，比如这个时刻可能输出的词在上面的组出\n",
      "现过，我们就对这个词的分数-1，如果这个时刻可能输出的词在上面组没有出现过，我们就对这个词的分数不\n",
      "惩罚。\n",
      "2、LayerNormalization和BatchNormalization的区别，padding对这两者有影响吗，\n",
      "对哪一维有影响\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样\n",
      "本的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "padding会对BatchNormalization的seq_len这个维度有影响，计算的时候会把padding也算进去。\n",
      "3、pytorch.Dataloader报错出现数据维度不一样怎么解决。\n",
      "在构建dataset重写的__getitem__方法中要返回相同长度的tensor。\n",
      "可以使用向量补全的方法来解决这个问题，把不同长度的向量补全成等长的。</paragraph>\n",
      "Prediction: [Relevant]Bert 是一种语言模型，其中包含了一个位于输入序列之后的句子检测器（sentence boundary detector），这个模\n",
      "Score: 0.7345150908417325\n",
      "\u001b[0m\u001b[1;3;34m10/10 paragraphs done\n",
      "\n",
      "\u001b[0m\u001b[1;3;34mEnd evaluation\n",
      "\u001b[0m\u001b[1;3;34mSelected the best answer: [Relevant]Bert 是一种基于 Transformer 模型的语言模型，其中 GPT 是一种基于 Recurrent Neural Networks (RNN) 的语言模型。[Partially supported][Utility:5]\n",
      "\u001b[0m\u001b[1;3;32mFinal answer: Bert 是一种基于 Transformer 模型的语言模型，其中 GPT 是一种基于 Recurrent Neural Networks (RNN) 的语言模型。\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.90 ms /    50 runs   (    0.24 ms per token,  4202.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2222.18 ms /    28 tokens (   79.36 ms per token,    12.60 tokens per second)\n",
      "llama_print_timings:        eval time =    8659.21 ms /    49 runs   (  176.72 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   11073.73 ms /    77 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieval required\n",
      "\u001b[0m\u001b[1;3;34mReceived: 10 documents\n",
      "\u001b[0m\u001b[1;3;34mStart evaluation\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.57 ms /    50 runs   (    0.23 ms per token,  4322.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   64918.38 ms /   864 tokens (   75.14 ms per token,    13.31 tokens per second)\n",
      "llama_print_timings:        eval time =    9557.72 ms /    49 runs   (  195.06 ms per token,     5.13 tokens per second)\n",
      "llama_print_timings:       total time =   76319.60 ms /   913 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第18页共46页第二十一篇：2022年4月14日美团计算机视觉算法暑期实习面试题6道\n",
      "1、简单介绍gbdt算法的原理\n",
      "GBDT是梯度提升决策树，是一种基于Boosting的算法，采用以决策树为基学习器的加法模型，通过不\n",
      "断拟合上一个弱学习器的残差，最终实现分类或回归的模型。关键在于利用损失函数的负梯度在当前模型的值\n",
      "作为残差的近似值，从而拟合一个回归树。对于分类问题：常使用指数损失函数；对于回归问题：常使用平方\n",
      "误差损失函数（此时，其负梯度就是通常意义的残差），对于一般损失函数来说就是残差的近似。\n",
      "无论损失函数是什么形式，每个决策树拟合的都是负梯度。准确的说，不是用负梯度代替残差，而是当损\n",
      "失函数是均方损失时，负梯度刚好是残差，残差只是特例。\n",
      "2、pca属于有监督还是无监督\n",
      "PCA按有监督和无监督划分应该属于无监督学习，所以数据集有无y并不重要，只是改变样本X的属性\n",
      "(特征)维度。\n",
      "3、介绍svm算法\n",
      "是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感\n",
      "知机。\n",
      "SVM可分为三种：\n",
      "线性可分SVM\n",
      "当训练数据线性可分时，通过最大化硬间隔（hardmargin）可以学习得到一个线性分类器，即硬间隔\n",
      "SVM。\n",
      "线性SVM\n",
      "当训练数据不能线性可分但是近似线性可分时，通过最大化软间隔（softmargin）也可以学习到一个线\n",
      "性分类器，即软间隔SVM。\n",
      "非线性SVM\n",
      "当训练数据线性不可分时，通过使用核技巧（kerneltrick）和最大化软间隔，可以学习到一个非线性\n",
      "SVM。</paragraph>\n",
      "Prediction: [Irrelevant]PCA的基本思想是通过对数据集中的特征向量进行变换，使其规范化，从而对数据集中的样本进行\n",
      "Score: 0.45433171584768933\n",
      "\u001b[0m\u001b[1;3;34m1/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.34 ms /    50 runs   (    0.23 ms per token,  4411.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   56254.25 ms /   740 tokens (   76.02 ms per token,    13.15 tokens per second)\n",
      "llama_print_timings:        eval time =    9504.96 ms /    49 runs   (  193.98 ms per token,     5.16 tokens per second)\n",
      "llama_print_timings:       total time =   67349.50 ms /   789 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第4页共46页第十七篇：2022年4月10日百度机器学习方向暑期实习面试题6道\n",
      "1、介绍下SVM算法\n",
      "是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感\n",
      "知机。\n",
      "SVM可分为三种：\n",
      "线性可分SVM\n",
      "当训练数据线性可分时，通过最大化硬间隔（hardmargin）可以学习得到一个线性分类器，即硬间隔\n",
      "SVM。\n",
      "线性SVM\n",
      "当训练数据不能线性可分但是近似线性可分时，通过最大化软间隔（softmargin）也可以学习到一个线\n",
      "性分类器，即软间隔SVM。\n",
      "非线性SVM\n",
      "当训练数据线性不可分时，通过使用核技巧（kerneltrick）和最大化软间隔，可以学习到一个非线性\n",
      "SVM。\n",
      "SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损\n",
      "失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。\n",
      "SVM如何选择核函数\n",
      "Linear核：主要用于线性可分的情形。参数少，速度快，对于一般数据，分类效果已经很理想了。\n",
      "RBF核：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交\n",
      "叉验证来寻找合适的参数，不过这个过程比较耗时。\n",
      "2、介绍下逻辑回归算法\n",
      "逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度下降法来求解参数，从而达\n",
      "到将数据二分类的目的。\n",
      "逻辑回归的优缺点</paragraph>\n",
      "Prediction: [Relevant]PCA (Principal Component Analysis) is a technique for dimensionality reduction and data visualization.[No support / Contradictory][Continue to Use Evidence]It is a supervised learning algorithm that seeks to find a lower-dimensional representation of a given dataset, by identifying the principal\n",
      "Score: 0.9902063263956218\n",
      "\u001b[0m\u001b[1;3;34m2/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.20 ms /    50 runs   (    0.22 ms per token,  4462.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   88706.87 ms /  1149 tokens (   77.20 ms per token,    12.95 tokens per second)\n",
      "llama_print_timings:        eval time =   10114.33 ms /    49 runs   (  206.41 ms per token,     4.84 tokens per second)\n",
      "llama_print_timings:       total time =  101563.15 ms /  1198 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第1页共46页目录\n",
      "第十七篇：2022年4月10日百度机器学习方向暑期实习面试题6道.4\n",
      "1、介绍下SVM算法.4\n",
      "2、介绍下逻辑回归算法.4\n",
      "3、介绍下决策树算法.5\n",
      "4、用通俗的语言介绍下强化学习（ReinforcementLearning）.5\n",
      "5、Leetcode34在排序数组中查找元素的第一个和最后一个位置.5\n",
      "6、Letcode102层序遍历.6\n",
      "第十八篇：2022年4月18日字节跳动机器学习AILab一面面试题6道.8\n",
      "1、BN和LN区别.8\n",
      "2、讲讲selfattention.8\n",
      "3、Bert的预训练过程.8\n",
      "4、PreNorm与PostNorm的区别？8\n",
      "5、GPT与Bert的区别.9\n",
      "6、如何加速Bert模型的训练.9\n",
      "第十九篇：2022年4月8日字节跳动机抖音APP推荐实习面试题8道.11\n",
      "1、AUC是什么？如何计算AUC？11\n",
      "2、AUC线上线下不一致怎么办.11\n",
      "3、召回阶段的负采样是怎么做的？12\n",
      "4、FM，DeepFM跟FFM的对比.12\n",
      "5、手撕FM的训练过程.12\n",
      "6、Leetcode—64.最小路径和.13\n",
      "7、剑指Offer10-I.斐波那契数列.14\n",
      "8、Leetcode—215.数组中的第K个最大元素.14\n",
      "第二十篇：2022年4月携程暑期实习搜索推荐算法岗面试题8道.15\n",
      "1、你所理解的推荐系统是什么样的？大致流程？所用模型？15\n",
      "2、双塔模型优势，缺点，如何改进？15\n",
      "3、粗排的目的是什么？15\n",
      "4、wide&deep模型为什么要有wide层结构，优缺点，如何改进？16\n",
      "5、推荐领域GBDT+LR的做法了解吗？16\n",
      "6、粗排有哪些指标？NDCG了解吗？16\n",
      "7、ROC，PR曲线含义，坐标轴代表什么？17\n",
      "8、AUC怎么求，实际意义？17\n",
      "第二十一篇：2022年4月14日美团计算机视觉算法暑期实习面试题6道.18\n",
      "1、简单介绍gbdt算法的原理.18\n",
      "2、pca属于有监督还是无监督.18\n",
      "3、介绍svm算法.18\n",
      "4、介绍transformer算法.19\n",
      "5、layernorm和batchnorm的比较.19\n",
      "6、Leetcode—两数之和.19\n",
      "第二十二篇：2022年5月18日2023届广联达提前批面试题5道.21\n",
      "1、Transformer中encoder和decoder的区别（结构和功能上）.21\n",
      "2、有哪些防止过拟合的方法.21\n",
      "3、L1和L2正则化为什么可以防止过拟合.21\n",
      "4、传统机器学习方法了解多少.21</paragraph>\n",
      "Prediction: [Relevant]2、哪些卷积神经网络中的卷积层可以防止过拟合.21\n",
      "5、哪\n",
      "Score: 0.5551161221012553\n",
      "\u001b[0m\u001b[1;3;34m3/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       7.27 ms /    31 runs   (    0.23 ms per token,  4266.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =   32805.50 ms /   436 tokens (   75.24 ms per token,    13.29 tokens per second)\n",
      "llama_print_timings:        eval time =    5791.85 ms /    30 runs   (  193.06 ms per token,     5.18 tokens per second)\n",
      "llama_print_timings:       total time =   39625.70 ms /   466 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第17页共46页7、ROC，PR曲线含义，坐标轴代表什么？\n",
      "ROC曲线以真正例率TPR为纵轴，以假正例率FPR为横轴，在不同的阈值下获得坐标点，并连接各个坐\n",
      "标点，得到ROC曲线。\n",
      "PR曲线中的P代表的是Precision（精准率），R代表的是Recall（召回率），其代表的是精准率与召\n",
      "回率的关系，一般情况下，Precision设置为纵坐标，将Recall设置为横坐标。\n",
      "8、AUC怎么求，实际意义？\n",
      "AUC：随机取一个正样本和一个负样本，正样本的预测值大于负样本预测值的概率。\n",
      "AUC计算的关键是找到所有正样本预测值大于负样本预测值的正负样本对。\n",
      "首先，需要将样本按照预测值进行从小到大排序（最小score对应的sample的rank为1，第二小\n",
      "score对应sample的rank为2，以此类推）；\n",
      "其次，把所有的正类样本的rank相加，再减去两个正样本组合的情况。</paragraph>\n",
      "Prediction: [Relevant]PCA 的概念和算法基本上都是无监督的。[No support / Contradictory][Utility:5]\n",
      "Score: 1.3116679501535715\n",
      "\u001b[0m\u001b[1;3;34m4/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.30 ms /    50 runs   (    0.23 ms per token,  4424.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   57690.18 ms /   757 tokens (   76.21 ms per token,    13.12 tokens per second)\n",
      "llama_print_timings:        eval time =    9546.87 ms /    49 runs   (  194.83 ms per token,     5.13 tokens per second)\n",
      "llama_print_timings:       total time =   68877.98 ms /   806 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第21页共46页第二十二篇：2022年5月18日2023届广联达提前批面试题5道\n",
      "1、Transformer中encoder和decoder的区别（结构和功能上）\n",
      "结构上：Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个\n",
      "Block，Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder\n",
      "端的Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一\n",
      "个前馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和Layer\n",
      "Normalization层。\n",
      "功能上：Transformer中encoder是双向的，是编码器；decoder是单向的，是解码器，可以用于生成\n",
      "任务。\n",
      "2、有哪些防止过拟合的方法\n",
      "数据的角度：获取和使用更多的数据（数据集增强）；\n",
      "模型角度：降低模型复杂度、L1\\L2\\Dropout正则化、Earlystopping（提前终止）\n",
      "模型融合的角度：使用bagging等模型融合方法。\n",
      "3、L1和L2正则化为什么可以防止过拟合\n",
      "正规化是防止过拟合的一种重要技巧。正则化通过降低模型的复杂性，达到避免过拟合的问题。这里的\n",
      "降低模型的复杂性可以理解为：\n",
      "L1将很多权重变成0，这样起到作用的因素就会减少。\n",
      "L2使权重都趋于0，这样就不会有某个权重占比特别大。\n",
      "4、传统机器学习方法了解多少\n",
      "这个可以说逻辑回归模型，逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度\n",
      "下降法来求解参数，从而达到将数据二分类的目的。\n",
      "逻辑回归的优缺点\n",
      "优点：</paragraph>\n",
      "Prediction: [Relevant]逻辑回归模型的优点是它很容易实现，很快地可以得出结论，因此\n",
      "Score: 0.5602655761931419\n",
      "\u001b[0m\u001b[1;3;34m5/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      13.76 ms /    40 runs   (    0.34 ms per token,  2907.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   67604.30 ms /   879 tokens (   76.91 ms per token,    13.00 tokens per second)\n",
      "llama_print_timings:        eval time =    7614.41 ms /    39 runs   (  195.24 ms per token,     5.12 tokens per second)\n",
      "llama_print_timings:       total time =   77101.55 ms /   918 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第43页共46页好处是这样做大大节约了参数量。\n",
      "5、如何防止过拟合\n",
      "数据的角度：获取和使用更多的数据（数据集增强）；\n",
      "模型角度：降低模型复杂度、L1\\L2\\Dropout正则化、Earlystopping（提前终止）\n",
      "模型融合的角度：使用bagging等模型融合方法。\n",
      "6、BN在训练和测试的时候的区别？可以防止过拟合吗？\n",
      "对于BN，在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。\n",
      "而在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全\n",
      "量训练数据的均值和方差，这个可以通过移动平均法求得。\n",
      "对于BN，当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，gamma和bata。\n",
      "BN算法防止过拟合：在网络的训练中，BN的使用使得一个minibatch中所有样本都被关联在了一起，\n",
      "因此网络不会从某一个训练样本中生成确定的结果，即同样一个样本的输出不再仅仅取决于样本的本身，也取\n",
      "决于跟这个样本同属一个batch的其他样本，而每次网络都是随机取batch，这样就会使得整个网络不会朝\n",
      "这一个方向使劲学习。一定程度上避免了过拟合。\n",
      "7、什么是AUC？\n",
      "AUC：AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。AUC反映的是分\n",
      "类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。\n",
      "8、卷积核计算公式\n",
      "卷积层计算公式如下：\n",
      "其中，W为输入大小，K为卷积核大小，P为padding大小，S为步幅。\n",
      "如果，想保持卷积前后的特征图大小相同，通常会设定padding为：</paragraph>\n",
      "Prediction: [Relevant]W为输入大小，K为卷积核大小，P为padding大小，S为步幅。[Fully supported][Utility:5]\n",
      "Score: 1.940682781989481\n",
      "\u001b[0m\u001b[1;3;34m6/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       2.34 ms /    10 runs   (    0.23 ms per token,  4273.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =   70947.42 ms /   920 tokens (   77.12 ms per token,    12.97 tokens per second)\n",
      "llama_print_timings:        eval time =    1798.89 ms /     9 runs   (  199.88 ms per token,     5.00 tokens per second)\n",
      "llama_print_timings:       total time =   74658.89 ms /   929 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第11页共46页第十九篇：2022年4月8日字节跳动机抖音APP推荐实习面试题8道\n",
      "1、AUC是什么？如何计算AUC？\n",
      "AUC：随机取一个正样本和一个负样本，正样本的预测值大于负样本预测值的概率。\n",
      "AUC计算的关键是找到所有正样本预测值大于负样本预测值的正负样本对。\n",
      "首先，需要将样本按照预测值进行从小到大排序（最小score对应的sample的rank为1，第二小\n",
      "score对应sample的rank为2，以此类推）；\n",
      "其次，把所有的正类样本的rank相加，再减去两个正样本组合的情况。\n",
      "2、AUC线上线下不一致怎么办\n",
      "线上线下效果不一致，大概率是由线上线下预估环境不一致引起。预估环境，一般涉及2个要素：模型\n",
      "和特征。\n",
      "模型是否一致\n",
      "主要包括校验离线模型格式转换、serving部署，线上模型加载、预估等接口是否有问题\n",
      "·特征是否一致\n",
      "准确是指，线上线下喂给模型的特征是否一致。\n",
      "与模型一致性检验一样，首先需要校验线上线下特征处理逻辑是否一致等；\n",
      "其次，与线上真实预估环境相比，离线环境更容易获取到特征，当离线使用线上获取不到的特征时，就会\n",
      "造成离线效果虚高的假象。\n",
      "严重点的，如特征穿越，即特征中包含标签信息，会造成训练和评估时数据泄露，导致离线评估时AUC\n",
      "虚高；轻一点的，如离线使用的特征比线上实时性高，同样会导致线上效果不符合预期。\n",
      "这就要求离线阶段构造样本时，需要参考线上真实预估环境获取特征时的延迟，通过严格控制离线特征拼\n",
      "接时的咬合时间，保证线下线下喂给模型的特征的一致性。\n",
      "更好地做法是，落地线上特征日志，直接用于离线训练。\n",
      "AUC指标不能有效刻画模型表现。可以尝试GAUC代替。</paragraph>\n",
      "Prediction: [Irrelevant]无监督[Utility:5]\n",
      "Score: 0.7301676245897185\n",
      "\u001b[0m\u001b[1;3;34m7/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       2.74 ms /    12 runs   (    0.23 ms per token,  4381.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   54067.57 ms /   704 tokens (   76.80 ms per token,    13.02 tokens per second)\n",
      "llama_print_timings:        eval time =    2110.82 ms /    11 runs   (  191.89 ms per token,     5.21 tokens per second)\n",
      "llama_print_timings:       total time =   57550.67 ms /   715 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第42页共46页第二十九篇：2022年6月24日字节电商CV实习岗面试题10道\n",
      "1、如何解决类别极度不平衡的问题？\n",
      "2、说下Transformer模型\n",
      "Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个Block，\n",
      "Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder端的\n",
      "Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一个前\n",
      "馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和LayerNormalization\n",
      "层。\n",
      "3、说下FocalLoss\n",
      "Focalloss是目标检测中解决正负样本严重不平衡的方法，在标准交叉熵损失基础上修改得到的。这个函\n",
      "数可以通过减少易分类样本的权重，使得模型在训练时更专注于稀疏的难分类的样本；防止大量易分类负样本\n",
      "在loss中占主导地位。\n",
      "4、介绍下深度可分离卷积和传统卷积的区别\n",
      "传统的卷积是各个通道上采用相不同的卷积核，然后不同的卷积核用于提取不同方面的特征。\n",
      "深度可分离卷积先在各个通道上采用不同的卷积核提取不同的特征，但是这样对于某个通道来说，就只提\n",
      "取了一方面的特征，因此在此基础上加入点卷积，用1*1的卷积对提取特征后的特征图再次提取不同方面的\n",
      "特征，最终产生和普通卷积相同的输出特征图。</paragraph>\n",
      "Prediction: [Irrelevant]无监督模型[Utility:5]\n",
      "Score: 0.7283484868777735\n",
      "\u001b[0m\u001b[1;3;34m8/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       1.07 ms /     5 runs   (    0.21 ms per token,  4651.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   79272.31 ms /  1039 tokens (   76.30 ms per token,    13.11 tokens per second)\n",
      "llama_print_timings:        eval time =     815.67 ms /     4 runs   (  203.92 ms per token,     4.90 tokens per second)\n",
      "llama_print_timings:       total time =   82094.84 ms /  1043 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第27页共46页第二十四篇：2022年5月10日美团搜索推荐算法面试题10道\n",
      "1、介绍下推荐系统的流程\n",
      "推荐系统的流程主要包含一下几个阶段。\n",
      "索引&特征:会根据内容特性提前建立若干种类型的索引。\n",
      "召回阶段:用户请求时会从各种索引种取出千/万条item。\n",
      "粗排阶段:针对这上千/万条item，进行第一遍打分，再筛选出几百条或者千条。这个阶段的排序模型一\n",
      "般都比较简单，能够过滤掉一些与用户兴趣明显不相关的。\n",
      "精排阶段:得到几百条item后，精排阶段会建立相对精细的模型，根据用户的画像，偏好，上下文，结\n",
      "合业务目标进行排序。一般精排后返回50-100条给到engine侧。\n",
      "重排阶段:engine侧拿到精排的50条item。还会做很多的人工干预和产品逻辑，比如item之间的多样\n",
      "性，产品策略逻辑，比如热门，置顶，多种内容之间的位置混合等等。最终会返回5-10条左右的item，曝\n",
      "光给客户端。根据业务特性，在线流程还有许多比较细的模块，比如去重服务，避免给用户推荐重复的内容。\n",
      "特征预处理，特征抽取等模块。\n",
      "2、召回和排序的差异？\n",
      "召回的目的在于减少候选的数量（尽量控制在1000以内），方便后续排序环节使用复杂模型精准排序；\n",
      "因为在短时间内评估海量候选，所以召回的关键点是个快字，受限与此与排序相比，召回的算法模型相对简\n",
      "单，使用的特征比较少。而排序模型相对更加复杂，更追求准确性，使用的特征也会较多。\n",
      "3、结果f1提升的1%怎么保证有效性，如何保证置信呢？\n",
      "实验过程中固定随机种子、多次实验取平均\n",
      "4、固定随机种子后，多次实验结果相同吗？\n",
      "还是会有细微差别，因为在梯度传播过程，梯度（浮点数）精度有差异，随着神经网络层数的增加，梯度\n",
      "差异会从小数后面的位置往前跑。只能设置浮点数精度增加来缓解这个问题。</paragraph>\n",
      "Prediction: [Relevant]​[No support / Contradictory][Utility:5]\n",
      "Score: 0.980478530338437\n",
      "\u001b[0m\u001b[1;3;34m9/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.50 ms /    50 runs   (    0.23 ms per token,  4347.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =   60757.97 ms /   799 tokens (   76.04 ms per token,    13.15 tokens per second)\n",
      "llama_print_timings:        eval time =    9748.64 ms /    49 runs   (  198.95 ms per token,     5.03 tokens per second)\n",
      "llama_print_timings:       total time =   72266.55 ms /   848 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第22页共46页形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权\n",
      "重值比较高，那么这个特征最后对结果的影响会比较大。\n",
      "模型效果不错。在工程上是可以接受的（作为baseline），如果特征工程做的好，效果不会太差，并且\n",
      "特征工程可以并行开发，大大加快开发的速度。\n",
      "训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。\n",
      "缺点：\n",
      "准确率欠佳。因为形式非常的简单，而现实中的数据非常复杂，因此，很难达到很高的准确性。很难处理\n",
      "数据不平衡的问题。\n",
      "5、生成式模型和判别式模型的区别并举一些例子\n",
      "生成模型：学习得到联合概率分布P(x,y)，即特征x，共同出现的概率。\n",
      "常见的生成模型：朴素贝叶斯模型，混合高斯模型，HMM模型。\n",
      "判别模型：学习得到条件概率分布P(y|x)，即在特征x出现的情况下标记y出现的概率。\n",
      "常见的判别模型：感知机，决策树，逻辑回归，SVM，CRF等。\n",
      "判别式模型：要确定一个羊是山羊还是绵羊，用判别式模型的方法是从历史数据中学习到模型，然后通过\n",
      "提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。\n",
      "生成式模型：是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模\n",
      "型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，再放到绵羊模型中看概率是多少，哪个大就是\n",
      "哪个。</paragraph>\n",
      "Prediction: [Relevant]PCA属于生成式模型，即学习得到联合概率分布P(x,y)，即特征x和标记y的\n",
      "Score: 0.5920095905613563\n",
      "\u001b[0m\u001b[1;3;34m10/10 paragraphs done\n",
      "\n",
      "\u001b[0m\u001b[1;3;34mEnd evaluation\n",
      "\u001b[0m\u001b[1;3;34mSelected the best answer: [Relevant]W为输入大小，K为卷积核大小，P为padding大小，S为步幅。[Fully supported][Utility:5]\n",
      "\u001b[0m\u001b[1;3;32mFinal answer: W为输入大小，K为卷积核大小，P为padding大小，S为步幅。\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       1.32 ms /     6 runs   (    0.22 ms per token,  4559.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1290.52 ms /    16 tokens (   80.66 ms per token,    12.40 tokens per second)\n",
      "llama_print_timings:        eval time =     848.36 ms /     5 runs   (  169.67 ms per token,     5.89 tokens per second)\n",
      "llama_print_timings:       total time =    2185.72 ms /    21 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieval required\n",
      "\u001b[0m\u001b[1;3;34mReceived: 10 documents\n",
      "\u001b[0m\u001b[1;3;34mStart evaluation\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.86 ms /    50 runs   (    0.24 ms per token,  4216.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   57871.21 ms /   760 tokens (   76.15 ms per token,    13.13 tokens per second)\n",
      "llama_print_timings:        eval time =    9681.51 ms /    49 runs   (  197.58 ms per token,     5.06 tokens per second)\n",
      "llama_print_timings:       total time =   69169.96 ms /   809 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第21页共46页第二十二篇：2022年5月18日2023届广联达提前批面试题5道\n",
      "1、Transformer中encoder和decoder的区别（结构和功能上）\n",
      "结构上：Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个\n",
      "Block，Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder\n",
      "端的Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一\n",
      "个前馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和Layer\n",
      "Normalization层。\n",
      "功能上：Transformer中encoder是双向的，是编码器；decoder是单向的，是解码器，可以用于生成\n",
      "任务。\n",
      "2、有哪些防止过拟合的方法\n",
      "数据的角度：获取和使用更多的数据（数据集增强）；\n",
      "模型角度：降低模型复杂度、L1\\L2\\Dropout正则化、Earlystopping（提前终止）\n",
      "模型融合的角度：使用bagging等模型融合方法。\n",
      "3、L1和L2正则化为什么可以防止过拟合\n",
      "正规化是防止过拟合的一种重要技巧。正则化通过降低模型的复杂性，达到避免过拟合的问题。这里的\n",
      "降低模型的复杂性可以理解为：\n",
      "L1将很多权重变成0，这样起到作用的因素就会减少。\n",
      "L2使权重都趋于0，这样就不会有某个权重占比特别大。\n",
      "4、传统机器学习方法了解多少\n",
      "这个可以说逻辑回归模型，逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度\n",
      "下降法来求解参数，从而达到将数据二分类的目的。\n",
      "逻辑回归的优缺点\n",
      "优点：</paragraph>\n",
      "Prediction: [Relevant]逻辑回归模型的优点是它可以很好地处理二分类问题，因为它的参数只有两个，\n",
      "Score: 0.7015084317750376\n",
      "\u001b[0m\u001b[1;3;34m1/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       9.77 ms /    41 runs   (    0.24 ms per token,  4198.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   54112.46 ms /   704 tokens (   76.86 ms per token,    13.01 tokens per second)\n",
      "llama_print_timings:        eval time =    7797.66 ms /    40 runs   (  194.94 ms per token,     5.13 tokens per second)\n",
      "llama_print_timings:       total time =   63426.76 ms /   744 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第42页共46页第二十九篇：2022年6月24日字节电商CV实习岗面试题10道\n",
      "1、如何解决类别极度不平衡的问题？\n",
      "2、说下Transformer模型\n",
      "Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个Block，\n",
      "Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder端的\n",
      "Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一个前\n",
      "馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和LayerNormalization\n",
      "层。\n",
      "3、说下FocalLoss\n",
      "Focalloss是目标检测中解决正负样本严重不平衡的方法，在标准交叉熵损失基础上修改得到的。这个函\n",
      "数可以通过减少易分类样本的权重，使得模型在训练时更专注于稀疏的难分类的样本；防止大量易分类负样本\n",
      "在loss中占主导地位。\n",
      "4、介绍下深度可分离卷积和传统卷积的区别\n",
      "传统的卷积是各个通道上采用相不同的卷积核，然后不同的卷积核用于提取不同方面的特征。\n",
      "深度可分离卷积先在各个通道上采用不同的卷积核提取不同的特征，但是这样对于某个通道来说，就只提\n",
      "取了一方面的特征，因此在此基础上加入点卷积，用1*1的卷积对提取特征后的特征图再次提取不同方面的\n",
      "特征，最终产生和普通卷积相同的输出特征图。</paragraph>\n",
      "Prediction: [Relevant]Transformer 是一种深度学习模型，主要用于机器翻译和自然语言处理任务。[Fully supported][Utility:5]\n",
      "Score: 2.0576465028671422\n",
      "\u001b[0m\u001b[1;3;34m2/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.06 ms /    50 runs   (    0.24 ms per token,  4146.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =   79518.85 ms /  1039 tokens (   76.53 ms per token,    13.07 tokens per second)\n",
      "llama_print_timings:        eval time =   10065.27 ms /    49 runs   (  205.41 ms per token,     4.87 tokens per second)\n",
      "llama_print_timings:       total time =   91772.68 ms /  1088 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第27页共46页第二十四篇：2022年5月10日美团搜索推荐算法面试题10道\n",
      "1、介绍下推荐系统的流程\n",
      "推荐系统的流程主要包含一下几个阶段。\n",
      "索引&特征:会根据内容特性提前建立若干种类型的索引。\n",
      "召回阶段:用户请求时会从各种索引种取出千/万条item。\n",
      "粗排阶段:针对这上千/万条item，进行第一遍打分，再筛选出几百条或者千条。这个阶段的排序模型一\n",
      "般都比较简单，能够过滤掉一些与用户兴趣明显不相关的。\n",
      "精排阶段:得到几百条item后，精排阶段会建立相对精细的模型，根据用户的画像，偏好，上下文，结\n",
      "合业务目标进行排序。一般精排后返回50-100条给到engine侧。\n",
      "重排阶段:engine侧拿到精排的50条item。还会做很多的人工干预和产品逻辑，比如item之间的多样\n",
      "性，产品策略逻辑，比如热门，置顶，多种内容之间的位置混合等等。最终会返回5-10条左右的item，曝\n",
      "光给客户端。根据业务特性，在线流程还有许多比较细的模块，比如去重服务，避免给用户推荐重复的内容。\n",
      "特征预处理，特征抽取等模块。\n",
      "2、召回和排序的差异？\n",
      "召回的目的在于减少候选的数量（尽量控制在1000以内），方便后续排序环节使用复杂模型精准排序；\n",
      "因为在短时间内评估海量候选，所以召回的关键点是个快字，受限与此与排序相比，召回的算法模型相对简\n",
      "单，使用的特征比较少。而排序模型相对更加复杂，更追求准确性，使用的特征也会较多。\n",
      "3、结果f1提升的1%怎么保证有效性，如何保证置信呢？\n",
      "实验过程中固定随机种子、多次实验取平均\n",
      "4、固定随机种子后，多次实验结果相同吗？\n",
      "还是会有细微差别，因为在梯度传播过程，梯度（浮点数）精度有差异，随着神经网络层数的增加，梯度\n",
      "差异会从小数后面的位置往前跑。只能设置浮点数精度增加来缓解这个问题。</paragraph>\n",
      "Prediction: [Relevant]因此，使用固定随机种子后，多次实验结果可能会相同，但是因为浮点数的不确定性，实\n",
      "Score: 0.6843746630167201\n",
      "\u001b[0m\u001b[1;3;34m3/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.67 ms /    50 runs   (    0.23 ms per token,  4286.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   65426.78 ms /   861 tokens (   75.99 ms per token,    13.16 tokens per second)\n",
      "llama_print_timings:        eval time =    9691.30 ms /    49 runs   (  197.78 ms per token,     5.06 tokens per second)\n",
      "llama_print_timings:       total time =   77189.42 ms /   910 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第18页共46页第二十一篇：2022年4月14日美团计算机视觉算法暑期实习面试题6道\n",
      "1、简单介绍gbdt算法的原理\n",
      "GBDT是梯度提升决策树，是一种基于Boosting的算法，采用以决策树为基学习器的加法模型，通过不\n",
      "断拟合上一个弱学习器的残差，最终实现分类或回归的模型。关键在于利用损失函数的负梯度在当前模型的值\n",
      "作为残差的近似值，从而拟合一个回归树。对于分类问题：常使用指数损失函数；对于回归问题：常使用平方\n",
      "误差损失函数（此时，其负梯度就是通常意义的残差），对于一般损失函数来说就是残差的近似。\n",
      "无论损失函数是什么形式，每个决策树拟合的都是负梯度。准确的说，不是用负梯度代替残差，而是当损\n",
      "失函数是均方损失时，负梯度刚好是残差，残差只是特例。\n",
      "2、pca属于有监督还是无监督\n",
      "PCA按有监督和无监督划分应该属于无监督学习，所以数据集有无y并不重要，只是改变样本X的属性\n",
      "(特征)维度。\n",
      "3、介绍svm算法\n",
      "是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感\n",
      "知机。\n",
      "SVM可分为三种：\n",
      "线性可分SVM\n",
      "当训练数据线性可分时，通过最大化硬间隔（hardmargin）可以学习得到一个线性分类器，即硬间隔\n",
      "SVM。\n",
      "线性SVM\n",
      "当训练数据不能线性可分但是近似线性可分时，通过最大化软间隔（softmargin）也可以学习到一个线\n",
      "性分类器，即软间隔SVM。\n",
      "非线性SVM\n",
      "当训练数据线性不可分时，通过使用核技巧（kerneltrick）和最大化软间隔，可以学习到一个非线性\n",
      "SVM。</paragraph>\n",
      "Prediction: [Relevant]这里介绍了Transformer算法的基本概念和原理，以及如何使用Transformer算法进行语言模型的建模和预测。\n",
      "Score: 0.8059199448300844\n",
      "\u001b[0m\u001b[1;3;34m4/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.31 ms /    52 runs   (    0.24 ms per token,  4225.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   78044.06 ms /  1012 tokens (   77.12 ms per token,    12.97 tokens per second)\n",
      "llama_print_timings:        eval time =   10261.64 ms /    51 runs   (  201.21 ms per token,     4.97 tokens per second)\n",
      "llama_print_timings:       total time =   90539.52 ms /  1063 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第15页共46页第二十篇：2022年4月携程暑期实习搜索推荐算法岗面试题8道\n",
      "1、你所理解的推荐系统是什么样的？大致流程？所用模型？\n",
      "推荐系统一方面是为了帮助消费者发现对自己有价值的商品，另一方面是帮助生产者把用户可能感兴趣的\n",
      "商品展现给用户，实现生产者和消费者的双赢。\n",
      "大致流程主要包括：获取用户特征，召回过程，排序过程（粗排、精排）\n",
      "召回模型：\n",
      "·规则召回（兴趣标签top，热门top，新品top等）\n",
      "·协同召回（基于用户的协同过滤，基于商品的协同过滤）\n",
      "·向量召回（FM召回，Item2vec,YoutubeDNN向量召回，GraphEmbedding召回，DSSM双塔召\n",
      "回）\n",
      "排序模型：GBDT+LR、Xgboost、FM/FFM、Wide&Deep、DeepFM、Deep&Cross、DIN、BST\n",
      "等\n",
      "2、双塔模型优势，缺点，如何改进？\n",
      "双塔模型的优势是速度快，但模型精度还有待提升。\n",
      "速度快是因为将所有Item转化成Embedding，并存储进ANN检索系统，比如FAISS，以供查询。类\n",
      "似FAISS这种ANN检索系统对海量数据的查询效率高。\n",
      "而双塔模型为了速度快，必须对用户和Item进行特征分离，而特征分离，又必然导致上述两个原因产生\n",
      "的效果损失。\n",
      "改进：SENet双塔模型，把SENet放在Embedding层之上，目的是通过SENet网络，动态地学习这些\n",
      "特征的重要性：对于每个特征学会一个特征权重，然后再把学习到的权重乘到对应特征的Embedding里，这\n",
      "样就可以动态学习特征权重，通过小权重抑制噪音或者无效低频特征，通过大权重放大重要特征影响的目的。\n",
      "3、粗排的目的是什么？\n",
      "粗排是用来帮精排模型找到那些它本来就要打高分的item，只不过范围更广一些。.按照上面的例子，如\n",
      "果没有粗排，精排模型自己找出来的某top10的item。而粗排的任务就是要找到包含这10个item的一个更\n",
      "小的候选集，既保证了效果，又减少线上预测的负担。</paragraph>\n",
      "Prediction: [Relevant]粗排模型的目的是帮助精排模型尽量减少负载，减少线上预测的负\n",
      "Score: 0.8329282868390154\n",
      "\u001b[0m\u001b[1;3;34m5/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       1.06 ms /     4 runs   (    0.26 ms per token,  3784.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =   65398.68 ms /   837 tokens (   78.13 ms per token,    12.80 tokens per second)\n",
      "llama_print_timings:        eval time =     598.27 ms /     3 runs   (  199.42 ms per token,     5.01 tokens per second)\n",
      "llama_print_timings:       total time =   67718.74 ms /   840 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第19页共46页SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损\n",
      "失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。\n",
      "SVM如何选择核函数\n",
      "Linear核：主要用于线性可分的情形。参数少，速度快，对于一般数据，分类效果已经很理想了。\n",
      "RBF核：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交\n",
      "叉验证来寻找合适的参数，不过这个过程比较耗时。\n",
      "4、介绍transformer算法\n",
      "Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个Block，\n",
      "Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder端的\n",
      "Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一个前\n",
      "馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和LayerNormalization\n",
      "层。\n",
      "5、layernorm和batchnorm的比较\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "6、Leetcode—两数之和\n",
      "该题较为简单。\n",
      "1.classSolution(object):\n",
      "2. deftwoSum(self,nums,target):\n",
      "3. \"\"\"\n",
      "4. :typenums:List[int]\n",
      "5. :typetarget:int\n",
      "6. :rtype:List[int]\n",
      "7. \"\"\"\n",
      "8. dic={}\n",
      "9. fori,numinenumerate(nums):\n",
      "10. iftarget-numindic:</paragraph>\n",
      "Prediction: [Irrelevant]​[Utility:5]\n",
      "Score: 0.7745346473996275\n",
      "\u001b[0m\u001b[1;3;34m6/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.08 ms /    50 runs   (    0.24 ms per token,  4137.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   57953.99 ms /   740 tokens (   78.32 ms per token,    12.77 tokens per second)\n",
      "llama_print_timings:        eval time =    9731.99 ms /    49 runs   (  198.61 ms per token,     5.03 tokens per second)\n",
      "llama_print_timings:       total time =   69378.99 ms /   789 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第4页共46页第十七篇：2022年4月10日百度机器学习方向暑期实习面试题6道\n",
      "1、介绍下SVM算法\n",
      "是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感\n",
      "知机。\n",
      "SVM可分为三种：\n",
      "线性可分SVM\n",
      "当训练数据线性可分时，通过最大化硬间隔（hardmargin）可以学习得到一个线性分类器，即硬间隔\n",
      "SVM。\n",
      "线性SVM\n",
      "当训练数据不能线性可分但是近似线性可分时，通过最大化软间隔（softmargin）也可以学习到一个线\n",
      "性分类器，即软间隔SVM。\n",
      "非线性SVM\n",
      "当训练数据线性不可分时，通过使用核技巧（kerneltrick）和最大化软间隔，可以学习到一个非线性\n",
      "SVM。\n",
      "SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损\n",
      "失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。\n",
      "SVM如何选择核函数\n",
      "Linear核：主要用于线性可分的情形。参数少，速度快，对于一般数据，分类效果已经很理想了。\n",
      "RBF核：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交\n",
      "叉验证来寻找合适的参数，不过这个过程比较耗时。\n",
      "2、介绍下逻辑回归算法\n",
      "逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度下降法来求解参数，从而达\n",
      "到将数据二分类的目的。\n",
      "逻辑回归的优缺点</paragraph>\n",
      "Prediction: [Relevant]逻辑回归是一种常见的分类模型，它假设数据服从伯努利分布，通过极\n",
      "Score: 0.9323531103241683\n",
      "\u001b[0m\u001b[1;3;34m7/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.67 ms /    50 runs   (    0.23 ms per token,  4284.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   66934.58 ms /   865 tokens (   77.38 ms per token,    12.92 tokens per second)\n",
      "llama_print_timings:        eval time =    9799.45 ms /    49 runs   (  199.99 ms per token,     5.00 tokens per second)\n",
      "llama_print_timings:       total time =   78569.83 ms /   914 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第39页共46页第二十八篇：京东科技NLP实习面试题10道\n",
      "1、Bert里面为什么用layernormalization，而不用batchnormalization，分别讲一下这\n",
      "两个啥意思。\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "2.、Bert里面为什么Q，K，V要用三个不同的矩阵，用一个不是也行吗\n",
      "如果使用相同的矩阵，相同量级的情况下，q和k进行点积的值会是最大的，进行softmax的加权平均\n",
      "后，该词所占的比重会最大，使得其他词的比重很少，无法有效利用上下文信息来增强当前词的语义表示，而\n",
      "使用不同的QKV后，会很大程度减轻上述的影响。\n",
      "3、Bert和transformer讲一下\n",
      "1bert只有transformer的encode结构，是生成语言模型\n",
      "2bert加入了输入句子的mask机制，在输入的时候会随机mask\n",
      "3模型接收两个句子作为输入，并且预测其中第二个句子是否在原始文档中也是后续句子可以做对话机\n",
      "制的应答。\n",
      "4在训练BERT模型时，MaskedLM和NextSentencePrediction是一起训练的，目标就是要最小化\n",
      "两种策略的组合损失函数。\n",
      "4、AUC指标讲一下\n",
      "AUC：AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。AUC反映的是分\n",
      "类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。</paragraph>\n",
      "Prediction: [Relevant]AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "Score: 0.8405405122724328\n",
      "\u001b[0m\u001b[1;3;34m8/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      10.06 ms /    23 runs   (    0.44 ms per token,  2286.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =  465203.92 ms /   831 tokens (  559.81 ms per token,     1.79 tokens per second)\n",
      "llama_print_timings:        eval time =    7131.03 ms /    22 runs   (  324.14 ms per token,     3.09 tokens per second)\n",
      "llama_print_timings:       total time =  476835.26 ms /   853 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第16页共46页4、wide&deep模型为什么要有wide层结构，优缺点，如何改进？\n",
      "wide&deep模型中的wide部分可以通过利用交叉特征引入非线性高效的实现记忆能力，但需要依赖人\n",
      "工特征工程。\n",
      "改进：DeepFM在Wide&Deep的基础上进行改进，不需要预训练FM得到隐向量，不需要人工特征工\n",
      "程，能同时学习低阶和高阶的组合特征；FM模块和Deep模块共享FeatureEmbedding部分，可以更快的\n",
      "训练，以及更精确的训练学习。\n",
      "5、推荐领域GBDT+LR的做法了解吗？\n",
      "GBDT+LR由两部分组成，其中GBDT用来对训练集提取特征作为新的训练输入数据，LR作为新训练输\n",
      "入数据的分类器。GBDT+LR的提出意味着特征工程可以完全交由一个独立的模型来完成，模型的输入可以是\n",
      "原始的特征向量，不必在特征工程上投入过多的人工筛选和模型设计的精力，真正实现了端到端的训练。\n",
      "6、粗排有哪些指标？NDCG了解吗？\n",
      "(1)NDCG，排序相似性的指标，看精排的排序结果和粗排有多相似\n",
      "(2)粗排的召回率/重叠率，粗排的topk和精排的topk有多大占比。\n",
      "计算DCG，计算公式如下：\n",
      "其中，rel为这个排序list，结果i的一个等级得分；i是指结果i的当前位置序号；\n",
      "​在搜索引擎中，rel等级得分，是由人工抽样数据，并且根据一定的规则打出来的等级得分。\n",
      "步骤二：\n",
      "计算IDCG（IdealDCG），即完美序的DCG；计算方式也同步骤1，只是排序序列不是由算法得出，而\n",
      "是由人工对序列根据一定的评估准则排出来的最佳序列。\n",
      "​步骤三：根据前面2个步骤的出来的结果，计算NDCG，计算公式如下：</paragraph>\n",
      "Prediction: [Relevant]NDCG = DCG / (DCG + 0.5 * IIDCG)[No support / Contradictory][Utility:5]\n",
      "Score: 1.4900611161061235\n",
      "\u001b[0m\u001b[1;3;34m9/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      23.32 ms /    50 runs   (    0.47 ms per token,  2144.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 1916958.72 ms /   616 tokens ( 3111.95 ms per token,     0.32 tokens per second)\n",
      "llama_print_timings:        eval time =   12368.01 ms /    49 runs   (  252.41 ms per token,     3.96 tokens per second)\n",
      "llama_print_timings:       total time = 1933642.38 ms /   665 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第35页共46页beamsearch是介于全局搜索和贪婪搜索之间。使用beamsize参数来限制在每一步保留下来的可能性\n",
      "词的数量。beamsearch是在测试阶段为了获得更好准确性而采取的一种策略，在训练阶段无需使用。\n",
      "相比暴力搜索的全局最优解，降低时间复杂度的方法就是寻找次优解，具体就是把搜索空间中的N减下\n",
      "来，每一步计算完只保留K个(beamsize)最大的取值路径，这样时间复杂度降为O(K*N*T)，K取值一般比\n",
      "N小很多。这样得到的虽然不是最优解，但是在seq2seq模型的推理预测中可以兼顾时间和效果。\n",
      "5、你了解的文本表示方法有哪些\n",
      "基于one-hot、tf-idf、textrank等的bag-of-words；\n",
      "主题模型：LSA（SVD）、pLSA、LDA；\n",
      "基于词向量的固定表征：word2vec、fastText、glove\n",
      "基于词向量的动态表征：elmo、GPT、bert\n",
      "对比如下：\n",
      "One-hot表示：维度灾难、语义鸿沟；\n",
      "矩阵分解（LSA）：利用全局语料特征，但SVD求解计算复杂度大；\n",
      "基于NNLM/RNNLM的词向量：词向量为副产物，存在效率不高等问题；\n",
      "word2vec、fastText：优化效率高，但是基于局部语料；\n",
      "glove：基于全局预料，结合了LSA和word2vec的优点；\n",
      "elmo、GPT、bert：动态特征。</paragraph>\n",
      "Prediction: [Relevant]以上文本表示方法中，基于一-hot表示的方式，每个单词都被看作一个独立的特征，因此对应的特\n",
      "Score: 0.5503470700438311\n",
      "\u001b[0m\u001b[1;3;34m10/10 paragraphs done\n",
      "\n",
      "\u001b[0m\u001b[1;3;34mEnd evaluation\n",
      "\u001b[0m\u001b[1;3;34mSelected the best answer: [Relevant]Transformer 是一种深度学习模型，主要用于机器翻译和自然语言处理任务。[Fully supported][Utility:5]\n",
      "\u001b[0m\u001b[1;3;32mFinal answer: Transformer 是一种深度学习模型，主要用于机器翻译和自然语言处理任务。\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "answers = {'question':[], 'llm_answer': [], 'expected_answer': []}\n",
    "\n",
    "for question in qna_dict.keys():\n",
    "    expected_answer = qna_dict[question]\n",
    "    # result\n",
    "    response = agent_pack.run(question)\n",
    "    answers['question'].append(question)\n",
    "    answers['llm_answer'].append(response.response)\n",
    "    answers['expected_answer'].append(expected_answer)\n",
    "\n",
    "answers_df = pd.DataFrame(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e73e77",
   "metadata": {},
   "source": [
    "查看 self-Rag 检索器的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6af5b26c-71c1-490c-b792-1ea3e5a36e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>llm_answer</th>\n",
       "      <th>expected_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>介绍下SVM算法</td>\n",
       "      <td>这里介绍的是线性可分SVM。</td>\n",
       "      <td>是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>用通俗的语言介绍下强化学习</td>\n",
       "      <td>好的</td>\n",
       "      <td>监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BN 和 LN 区别</td>\n",
       "      <td>​</td>\n",
       "      <td>Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>讲讲self attention</td>\n",
       "      <td>自我注意力（Self Attention）是一种神经语言模型中的一种重要概念，用于模型中的自我参与和内部连接。</td>\n",
       "      <td>Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bert 的预训练过程</td>\n",
       "      <td>因此，在实验中，我们需要确保我们的实验数据是一致的，以确保我们的结果是可信的。</td>\n",
       "      <td>Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GPT 与 Bert 的区别</td>\n",
       "      <td>Bert 是一种基于 Transformer 模型的语言模型，其中 GPT 是一种基于 Recurrent Neural Networks (RNN) 的语言模型。</td>\n",
       "      <td>GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pca 属于有监督还是无监督</td>\n",
       "      <td>W为输入大小，K为卷积核大小，P为padding大小，S为步幅。</td>\n",
       "      <td>PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>介绍 transformer 算法</td>\n",
       "      <td>Transformer 是一种深度学习模型，主要用于机器翻译和自然语言处理任务。</td>\n",
       "      <td>Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            question  \\\n",
       "0           介绍下SVM算法   \n",
       "1      用通俗的语言介绍下强化学习   \n",
       "2         BN 和 LN 区别   \n",
       "3   讲讲self attention   \n",
       "4        Bert 的预训练过程   \n",
       "5     GPT 与 Bert 的区别   \n",
       "6     pca 属于有监督还是无监督   \n",
       "7  介绍 transformer 算法   \n",
       "\n",
       "                                                                           llm_answer  \\\n",
       "0                                                                      这里介绍的是线性可分SVM。   \n",
       "1                                                                                  好的   \n",
       "2                                                                                   ​   \n",
       "3                             自我注意力（Self Attention）是一种神经语言模型中的一种重要概念，用于模型中的自我参与和内部连接。   \n",
       "4                                             因此，在实验中，我们需要确保我们的实验数据是一致的，以确保我们的结果是可信的。   \n",
       "5  Bert 是一种基于 Transformer 模型的语言模型，其中 GPT 是一种基于 Recurrent Neural Networks (RNN) 的语言模型。   \n",
       "6                                                    W为输入大小，K为卷积核大小，P为padding大小，S为步幅。   \n",
       "7                                            Transformer 是一种深度学习模型，主要用于机器翻译和自然语言处理任务。   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          expected_answer  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机  \n",
       "1                                                                                                                                                               监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略  \n",
       "2                                                                                                                                                                                                                                                                                        Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。  \n",
       "3  Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系  \n",
       "4                                                                                                                                                                                                                                                                                                             Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                               GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。  \n",
       "7                                                                                                                                                                                                                                                                Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(answers_df)\n",
    "answers_df.to_excel('self_rag_pack_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e95784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b789a806",
   "metadata": {},
   "source": [
    "## 二、查询路由和Multi-Document Agent相关概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06116af8",
   "metadata": {},
   "source": [
    "### 查询路由\n",
    "查询路由是LLM驱动的关键决策环节，它决定了在接收到用户查询后应采取的最佳行动——可能的行动包括生成摘要、对特定数据索引执行搜索或尝试多种不同的路由策略，并将它们的输出汇总成一个统一的答案。\n",
    "\n",
    "查询路由还负责选择最合适的数据存储位置来响应用户查询。这些数据存储位置可以是多种形式，如传统的向量存储、图形数据库、关系型数据库，或是分层次的索引系统。在处理包含多个文档的存储时，通常会利用两种不同的索引方式：摘要索引和文档块向量索引。\n",
    "\n",
    "定义查询路由器涉及设定它可以采取的行动选项。\n",
    "\n",
    "选择特定路由的过程是通过调用大型语言模型来实现的，其结果会按照预定义的格式返回，以便将查询定向到指定的索引。如果涉及到复杂的关联操作，这些查询还可能被发送到子链或其他智能体，如下文所述的多文档智能体方案所示。\n",
    "\n",
    "LlamaIndex和LangChain都提供了对查询路由的支持。\n",
    "\n",
    "### 智能体（Agent）\n",
    "在智能体技术的实现上，尤其是在基于大型语言模型（LLM）的智能体构建中，LLM在智能体的智能化中扮演着至关重要的角色。这些智能体能够通过整合LLM与规划、记忆以及其他关键技术模块，执行复杂的任务。在此框架中，LLM充当核心处理单元或“大脑”，负责管理和执行为特定任务或响应用户查询所需的一系列操作。\n",
    "智能体也是Langchain和LlamaIndex支持的一个概念，它从第一个LLM API发布之初就已存在。这个概念旨在为具备推理能力的LLM提供一套工具和任务。这些工具可能包括一些确定性的功能，如代码函数或外部API，甚至包括其他智能体——这种LLM之间的链接是LangChain名称的灵感来源。\n",
    "智能体本身是一个复杂的技术，下面将继续基于智能体的多文档检索案例，并在下文将要介绍的RAG系统中发挥作用。\n",
    "在LlamaIndex中，有一个OpenAIAgent类，它将这种高级逻辑与ChatEngine和QueryEngine类结合在一起，提供基于知识和上下文感知的聊天，以及在一个对话轮次中调用多个OpenAI函数的能力，这真正实现了智能代理行为。\n",
    "\n",
    "### 多文档智能体方案\n",
    "![mda](./figures/mda.png)\n",
    "\n",
    "多文档智能体的方案是一个高度复杂的配置，它在每个文档上初始化一个Agent（OpenAIAgent），该智能体能进行文档摘要制作和传统问答机制的操作，还有一个顶层智能体，负责将查询分配到各个文档智能体，并综合形成最终的答案。\n",
    "\n",
    "每个文档智能体都配备了两个工具：向量存储索引和摘要索引，它根据路由查询来决定使用哪一个。对于顶层智能体来说，所有文档智能体都是其工具。\n",
    "\n",
    "该方案展示了一种高级RAG架构，其中每个智能体都进行多个决策的导航。这种方法的优势在于能够比较不同的解决方案或实体在不同文档及其摘要中的描述，以及传统的单个文档摘要和QA机制——这基本上涵盖了与文档集合聊天的最常见用例。\n",
    "\n",
    "这种复杂配置的缺点在于，由于需要在智能体内部的大语言模型之间进行多次往返迭代，其运行速度较慢。值得一提的是，LLM调用通常是RAG管道中最耗时的操作，而搜索则是为了速度而优化的设计。因此，对于大型的多文档存储，建议考虑对这种方案进行简化，以便实现扩展。\n",
    "\n",
    "下面将根据llamaindex官方示例进行讲解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d223f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-你的sk\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleKeywordTableIndex,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core.callbacks import CallbackManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac4252",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_titles = [\n",
    "    \"Toronto\",\n",
    "    \"Seattle\",\n",
    "    \"Chicago\",\n",
    "    \"Boston\",\n",
    "    \"Houston\",\n",
    "    \"Tokyo\",\n",
    "    \"Berlin\",\n",
    "    \"Lisbon\",\n",
    "    \"Paris\",\n",
    "    \"London\",\n",
    "    \"Atlanta\",\n",
    "    \"Munich\",\n",
    "    \"Shanghai\",\n",
    "    \"Beijing\",\n",
    "    \"Copenhagen\",\n",
    "    \"Moscow\",\n",
    "    \"Cairo\",\n",
    "    \"Karachi\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3934e5fd",
   "metadata": {},
   "source": [
    "将爬取到的文档存在本地 将在下面构建多文档智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808d6dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "for title in wiki_titles:\n",
    "    response = requests.get(\n",
    "        \"https://en.wikipedia.org/w/api.php\",\n",
    "        params={\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            # 'exintro': True,\n",
    "            \"explaintext\": True,\n",
    "        },\n",
    "    ).json()\n",
    "    page = next(iter(response[\"query\"][\"pages\"].values()))\n",
    "    wiki_text = page[\"extract\"]\n",
    "\n",
    "    data_path = Path(\"./data/mutli_documents_data\")\n",
    "    if not data_path.exists():\n",
    "        Path.mkdir(data_path)\n",
    "\n",
    "    with open(data_path / f\"{title}.txt\", \"w\",encoding=\"utf-8\") as fp:\n",
    "        fp.write(wiki_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddc9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all wiki documents\n",
    "city_docs = {}\n",
    "for wiki_title in wiki_titles:\n",
    "    city_docs[wiki_title] = SimpleDirectoryReader(\n",
    "        input_files=[f\"./data/mutli_documents_data/{wiki_title}.txt\"]\n",
    "    ).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a3af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c00211",
   "metadata": {},
   "source": [
    "\n",
    "在下面小节中，将展示如何构建多文档智能体。首先为每个文档构建一个文档智能体，然后定义一个带有对象索引的顶层父级智能体。\n",
    "\n",
    "#### 为每个文档构建文档智能体\n",
    "\n",
    "\n",
    "1. 在本节中，为每个文档定义了“文档智能体”。\n",
    "\n",
    "2. 为每个文档定义了一个向量索引（用于语义搜索）和摘要索引（用于生成摘要）。然后将这两个查询引擎转换为工具，并传递给调用OpenAI函数的智能体。\n",
    "\n",
    "3. 该文档智能体可以动态选择在给定文档内执行语义搜索或生成摘要。\n",
    "\n",
    "4. 我们为每个城市文档创建了一个单独的文档智能体。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c1f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core import load_index_from_storage, StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import os\n",
    "\n",
    "node_parser = SentenceSplitter()\n",
    "\n",
    "# Build agents dictionary\n",
    "agents = {}\n",
    "query_engines = {}\n",
    "\n",
    "# this is for the baseline\n",
    "all_nodes = []\n",
    "\n",
    "for idx, wiki_title in enumerate(wiki_titles):\n",
    "    nodes = node_parser.get_nodes_from_documents(city_docs[wiki_title])\n",
    "    all_nodes.extend(nodes)\n",
    "\n",
    "    if not os.path.exists(f\"./data/mutli_documents_data/{wiki_title}\"):\n",
    "        # build vector index\n",
    "        vector_index = VectorStoreIndex(nodes)\n",
    "        vector_index.storage_context.persist(\n",
    "            persist_dir=f\"./data/mutli_documents_data/{wiki_title}\"\n",
    "        )\n",
    "    else:\n",
    "        vector_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=f\"./data/mutli_documents_data/{wiki_title}\"),\n",
    "        )\n",
    "\n",
    "    # build summary index\n",
    "    summary_index = SummaryIndex(nodes)\n",
    "    # define query engines\n",
    "    vector_query_engine = vector_index.as_query_engine(llm=llm)\n",
    "    summary_query_engine = summary_index.as_query_engine(llm=llm)\n",
    "\n",
    "    # define tools\n",
    "    query_engine_tools = [\n",
    "        QueryEngineTool(\n",
    "            query_engine=vector_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"vector_tool\",\n",
    "                description=(\n",
    "                    \"Useful for questions related to specific aspects of\"\n",
    "                    f\" {wiki_title} (e.g. the history, arts and culture,\"\n",
    "                    \" sports, demographics, or more).\"\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "        QueryEngineTool(\n",
    "            query_engine=summary_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"summary_tool\",\n",
    "                description=(\n",
    "                    \"Useful for any requests that require a holistic summary\"\n",
    "                    f\" of EVERYTHING about {wiki_title}. For questions about\"\n",
    "                    \" more specific sections, please use the vector_tool.\"\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # build agent\n",
    "    function_llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "    agent = OpenAIAgent.from_tools(\n",
    "        query_engine_tools,\n",
    "        llm=function_llm,\n",
    "        verbose=True,\n",
    "        system_prompt=f\"\"\"\\\n",
    "You are a specialized agent designed to answer queries about {wiki_title}.\n",
    "You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\\\n",
    "\"\"\",\n",
    "    )\n",
    "\n",
    "    agents[wiki_title] = agent\n",
    "    query_engines[wiki_title] = vector_index.as_query_engine(\n",
    "        similarity_top_k=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff71678",
   "metadata": {},
   "source": [
    "#### 构建检索器支持的OpenAI智能体\n",
    "\n",
    "下面构建了一个顶层智能体，它可以协调不同的文档智能体来回答用户的任何查询。\n",
    "\n",
    "这个智能体将所有文档智能体作为工具。这个特定的智能体`RetrieverOpenAIAgent`在工具使用前执行工具检索（不同于默认智能体，后者尝试将所有工具放入提示中）。\n",
    "\n",
    "在这里，使用了一个top-k检索器，也可以自定义其他工具检索方法！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5b9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tool for each document agent\n",
    "all_tools = []\n",
    "for wiki_title in wiki_titles:\n",
    "    wiki_summary = (\n",
    "        f\"This content contains Wikipedia articles about {wiki_title}. Use\"\n",
    "        f\" this tool if you want to answer any questions about {wiki_title}.\\n\"\n",
    "    )\n",
    "    doc_tool = QueryEngineTool(\n",
    "        query_engine=agents[wiki_title],\n",
    "        metadata=ToolMetadata(\n",
    "            name=f\"tool_{wiki_title}\",\n",
    "            description=wiki_summary,\n",
    "        ),\n",
    "    )\n",
    "    all_tools.append(doc_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e67709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dd03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "\n",
    "top_agent = OpenAIAgent.from_tools(\n",
    "    tool_retriever=obj_index.as_retriever(similarity_top_k=3),\n",
    "    system_prompt=\"\"\" \\\n",
    "You are an agent designed to answer queries about a set of given cities.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866a770",
   "metadata": {},
   "source": [
    "#### 定义基线向量存储索引\n",
    "\n",
    "作为比较的基准，我们定义了一个“朴素”的RAG流程，它将所有文档倾泻到一个单一的向量索引集合中。\n",
    "\n",
    "我们设置 top_k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18493a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_index = VectorStoreIndex(all_nodes)\n",
    "base_query_engine = base_index.as_query_engine(similarity_top_k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a5c59",
   "metadata": {},
   "source": [
    "#### 执行示例查询\n",
    "\n",
    "执行一些示例查询，范围从单个文档的问答/摘要到多个文档的问答/摘要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9e3e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the arts and culture in Boston\n",
      "=== Calling Function ===\n",
      "Calling function: tool_Boston with args: {\"input\":\"arts and culture\"}\n",
      "Added user message to memory: arts and culture\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"input\":\"arts and culture in Boston\"}\n",
      "Got output: Boston has a rich cultural scene with a strong emphasis on arts and music. The city is home to several art museums and galleries, including the Museum of Fine Arts and the Isabella Stewart Gardner Museum. The Boston Symphony Orchestra, one of the \"Big Five\" American orchestras, is based in Symphony Hall. Boston also hosts various performing arts organizations, such as the Boston Ballet, Boston Lyric Opera Company, and the Handel and Haydn Society. Additionally, the city is known for its literary culture, with historical figures like Ralph Waldo Emerson and Nathaniel Hawthorne contributing to its reputation as \"the intellectual capital of the United States.\"\n",
      "========================\n",
      "\n",
      "Got output: Boston has a rich cultural scene with a strong emphasis on arts and music. The city is home to several art museums and galleries, including the Museum of Fine Arts and the Isabella Stewart Gardner Museum. The Boston Symphony Orchestra, one of the \"Big Five\" American orchestras, is based in Symphony Hall. Boston also hosts various performing arts organizations, such as the Boston Ballet, Boston Lyric Opera Company, and the Handel and Haydn Society. Additionally, the city is known for its literary culture, with historical figures like Ralph Waldo Emerson and Nathaniel Hawthorne contributing to its reputation as \"the intellectual capital of the United States.\"\n",
      "========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# should use Boston agent -> vector tool\n",
    "response = top_agent.query(\"Tell me about the arts and culture in Boston\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6421adea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston has a rich cultural scene with a strong emphasis on arts and music. The city is home to several art museums and galleries, including the Museum of Fine Arts and the Isabella Stewart Gardner Museum. The Boston Symphony Orchestra, one of the \"Big Five\" American orchestras, is based in Symphony Hall. Boston also hosts various performing arts organizations, such as the Boston Ballet, Boston Lyric Opera Company, and the Handel and Haydn Society. Additionally, the city is known for its literary culture, with historical figures like Ralph Waldo Emerson and Nathaniel Hawthorne contributing to its reputation as \"the intellectual capital of the United States.\"\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62253494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston has a rich arts and culture scene, with numerous art museums and galleries like the Museum of Fine Arts, Isabella Stewart Gardner Museum, and Institute of Contemporary Art. The city also boasts a vibrant music scene, highlighted by the Boston Symphony Orchestra, one of the \"Big Five\" American orchestras. Annual events such as the Boston Early Music Festival, Boston Arts Festival, and the Boston gay pride parade contribute to the city's cultural vibrancy. Boston also has a strong literary history, with renowned writers like Ralph Waldo Emerson, Nathaniel Hawthorne, and Henry Wadsworth Longfellow shaping the city's intellectual landscape.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# baseline\n",
    "response = base_query_engine.query(\n",
    "    \"Tell me about the arts and culture in Boston\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d57c153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In terms of history, the Premier League was established in 1992 and has since become one of the most popular and financially lucrative football leagues in the world. It is known for its competitive nature and has been dominated by clubs like Manchester United, Arsenal, Chelsea, Liverpool, and Manchester City. The league has also seen remarkable moments, such as Leicester City's title victory in the 2015-2016 season.\n",
      "\n",
      "On the other hand, La Liga has a longer history, dating back to 1929. It has been home to some of the most successful and iconic clubs in football, including Real Madrid and Barcelona. The league has witnessed periods of dominance from various clubs and has seen intense rivalries between teams. Real Madrid has been particularly successful in the Champions League, winning numerous titles.\n",
      "\n",
      "In terms of UEFA Champions League (UCL) performance, both leagues have had strong showings. English clubs have won a total of 15 UCL titles, making England the second-most successful country in the competition's history. Spanish clubs, especially Real Madrid and Barcelona, have also been highly successful in the UCL. Real Madrid holds the record for the most UCL titles won, while Barcelona has also had multiple triumphs in the competition. Other Spanish clubs like Atlético Madrid, Sevilla, and Valencia have also made their mark in European competitions.\n",
      "\n",
      "Overall, both the Premier League and La Liga have rich histories and have produced successful teams in the UEFA Champions League. The Premier League is known for its competitiveness and financial strength, while La Liga has a longer history and has been home to some of the most iconic clubs in football.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# should use Houston agent -> vector tool\n",
    "response = top_agent.query(\n",
    "    \"Give me a summary of all the positive aspects of Houston\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d4f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Houston is a vibrant city with a robust economy fueled by exports, especially in petroleum products and oil-related industries. It has a diverse and expanding international community, showcasing various cultural events that highlight its multicultural essence. The city is famous for its culinary scene and restaurant culture, along with a lively arts and theater scene featuring numerous performing arts organizations and museums. Houston is a sports hub, hosting teams in major professional leagues, and offers a plethora of tourist attractions, parks, and recreational areas, making it an attractive destination for residents and visitors alike.\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "response = base_query_engine.query(\n",
    "    \"Give me a summary of all the positive aspects of Houston\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a42ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The city is home to the nation\\'s third-largest concentration of consular offices, representing 92 countries.Many annual events celebrate the diverse cultures of Houston. The largest and longest-running is the annual Houston Livestock Show and Rodeo, held over 20 days from early to late March, and is the largest annual livestock show and rodeo in the world. Another large celebration is the annual night-time Houston Gay Pride Parade, held at the end of June. Other notable annual events include the Houston Greek Festival, Art Car Parade, the Houston Auto Show, the Houston International Festival, and the Bayou City Art Festival, which is considered to be one of the top five art festivals in the United States.Houston is highly regarded for its diverse food and restaurant culture. Houston received the official nickname of \"Space City\" in 1967 because it is the location of NASA\\'s Lyndon B. Johnson Space Center. Other nicknames often used by locals include \"Bayou City\", \"Clutch City\", \"Crush City\", \"Magnolia City\", \"H-Town\", and \"Culinary Capital of the South\".\\r\\n\\r\\n\\r\\n=== Arts and theater ===\\r\\nThe Houston Theater District, in Downtown, is home to nine major performing arts organizations and six performance halls. It is the second-largest concentration of theater seats in a Downtown area in the United States.Houston is one of the few United States cities with permanent, professional, resident companies in all major performing arts disciplines: opera (Houston Grand Opera), ballet (Houston Ballet), music (Houston Symphony Orchestra), and theater (The Alley Theatre, Theatre Under the Stars). Houston is also home to folk artists, art groups and various small progressive arts organizations.Houston attracts many touring Broadway acts, concerts, shows, and exhibitions for a variety of interests. Facilities in the Theater District include the Jones Hall—home of the Houston Symphony Orchestra and Society for the Performing Arts—and the Hobby Center for the Performing Arts.\\r\\nThe Museum District\\'s cultural institutions and exhibits attract more than 7 million visitors a year. Notable facilities include The Museum of Fine Arts, the Houston Museum of Natural Science, the Contemporary Arts Museum Houston, the Station Museum of Contemporary Art, the Holocaust Museum Houston, the Children\\'s Museum of Houston, and the Houston Zoo.Located near the Museum District are The Menil Collection, Rothko Chapel, the Moody Center for the Arts and the Byzantine Fresco Chapel Museum.\\r\\nBayou Bend is a 14-acre (5.7 ha) facility of the Museum of Fine Arts that houses one of America\\'s most prominent collections of decorative art, paintings, and furniture. Bayou Bend is the former home of Houston philanthropist Ima Hogg.The National Museum of Funeral History is in Houston near the George Bush Intercontinental Airport. The museum houses the original Popemobile used by Pope John Paul II in the 1980s along with numerous hearses, embalming displays, and information on famous funerals.\\r\\nVenues across Houston regularly host local and touring rock, blues, country, dubstep, and Tejano musical acts. While Houston has never been widely known for its music scene, Houston hip hop has become a significant, independent music scene that is influential nationwide. Houston is the birthplace of the chopped and screwed remixing-technique in hip-hop which was pioneered by DJ Screw from the city. Some other notable hip-hop artists from the area include Destiny\\'s Child, Don Toliver, Slim Thug, Paul Wall, Mike Jones, Bun B, Geto Boys, Trae tha Truth, Kirko Bangz, Z-Ro, South Park Mexican, Travis Scott and Megan Thee Stallion.\\r\\n\\r\\n\\r\\n=== Tourism and recreation ===\\r\\nThe Theater District is a 17-block area in the center of Downtown Houston that is home to the Bayou Place entertainment complex, restaurants, movies, plazas, and parks. Bayou Place is a large multilevel building containing full-service restaurants, bars, live music, billiards, and Sundance Cinema. The Bayou Music Center stages live concerts, stage plays, and stand-up comedy.\\r\\nSpace Center Houston is the official visitors\\' center of NASA\\'s Lyndon B. Johnson Space Center. The Space Center has many interactive exhibits including Moon rocks, a Space Shuttle simulator, and presentations about the history of NASA\\'s manned space flight program. Other tourist attractions include the Galleria (Texas\\'s largest shopping mall, in the Uptown District), Old Market Square, the Downtown Aquarium, and Sam Houston Race Park.\\r\\nHouston\\'s current Chinatown and the Mahatma Gandhi District are two major ethnic enclaves, reflecting Houston\\'s multicultural makeup. Restaurants, bakeries, traditional-clothing boutiques, and specialty shops can be found in both areas.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# baseline: the response doesn't quite match the sources...\n",
    "response.source_nodes[1].get_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf50647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell the demographics of Houston, and then compare that with the demographics of Chicago\n",
      "=== Calling Function ===\n",
      "Calling function: tool_Houston with args: {\"input\": \"demographics\"}\n",
      "Added user message to memory: demographics\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\":\"demographics\"}\n",
      "Got output: Houston is known for its diverse population, with various ethnic and religious backgrounds represented. The city has been described as the most racially and ethnically diverse major city in the U.S. Houston's population includes a large and growing international community, contributing to its status as a global city. Additionally, the city has a significant African American population, with historical ties to the post-Civil War era and the Great Migration.\n",
      "========================\n",
      "\n",
      "Got output: Houston is known for its diverse population, with various ethnic and religious backgrounds represented. The city has been described as the most racially and ethnically diverse major city in the U.S. Houston's population includes a large and growing international community, contributing to its status as a global city. Additionally, the city has a significant African American population, with historical ties to the post-Civil War era and the Great Migration.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: tool_Chicago with args: {\"input\": \"demographics\"}\n",
      "Added user message to memory: demographics\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\":\"Chicago demographics\"}\n",
      "Got output: Chicago's population in 2020 was 2,746,388 according to the census. In 1900, more than 77% of the population were either foreign-born or born in the United States of foreign parentage, with Germans, Irish, Poles, Swedes, and Czechs making up nearly two-thirds of the foreign-born population. Between 1910 and 1930, the African American population of Chicago increased dramatically, from 44,103 to 233,903, leading to the Chicago Black Renaissance.\n",
      "========================\n",
      "\n",
      "Got output: The population of Chicago in 2020 was 2,746,388 according to the census. In 1900, a significant percentage of the population were either foreign-born or born in the United States of foreign parentage, with Germans, Irish, Poles, Swedes, and Czechs being prominent. The African American population in Chicago saw a significant increase between 1910 and 1930, leading to the Chicago Black Renaissance.\n",
      "========================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = top_agent.query(\n",
    "    \"Tell the demographics of Houston, and then compare that with the\"\n",
    "    \" demographics of Chicago\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10491d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shanghai's history is marked by international trade and foreign influence, especially during the 19th and early 20th centuries, shaping its identity as a major commercial and financial center. In contrast, Beijing has a more traditional background as the political and cultural heart of China, emphasizing governance and imperial legacy.\n",
      "\n",
      "Currently, Shanghai is renowned for its emphasis on finance, innovation, and global trade, making a significant contribution to China's GDP. On the other hand, Beijing is known for its role as the political capital, housing government institutions and headquarters of state-owned enterprises, focusing on administrative functions and policy-making.\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "response = base_query_engine.query(\n",
    "    \"Tell me the differences between Shanghai and Beijing in terms of history\"\n",
    "    \" and current economy\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9805f752",
   "metadata": {},
   "source": [
    "参考链接：https://docs.llamaindex.ai/en/v0.10.33/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
