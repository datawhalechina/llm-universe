{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata和权限体系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、理论介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在RAG（Retrieval-Augmented Generation）中，元数据（metadata）和权限体系是确保信息安全和有效管理的重要组成部分。元数据提供了关于数据的上下文信息，例如数据的来源、创建时间、修改历史等，这些信息有助于用户理解和使用数据。\n",
    "\n",
    "权限体系则是对数据访问和操作的控制机制，确保只有授权用户才能访问特定的数据。通过定义不同用户角色和权限，系统可以有效地管理数据的安全性，防止未授权访问和数据泄露。\n",
    "\n",
    "在RAG的应用中，合理的元数据管理和权限控制能提高数据的可用性，还能增强系统的安全性，确保用户在获取信息时能够遵循相关的法律法规和公司政策。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def clean_text(text: str):\n",
    "    \"\"\"\n",
    "    实现文本清理函数\n",
    "\n",
    "    参数:\n",
    "        text: 需要清理的字段\n",
    "\n",
    "    返回:\n",
    "        清理完成后返回的字段\n",
    "    \n",
    "    \"\"\"\n",
    "    # 删除每页开头与结尾标语及链接\n",
    "    text = re.sub(r'→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←', '', text)\n",
    "    text = re.sub(r'→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', '', text)\n",
    "    # 删除字符串开头的空格\n",
    "    text = re.sub(r'\\s+', '', text)\n",
    "    # 删除回车\n",
    "    text = re.sub(r'\\n+', '', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"data/pumpkin_book.pdf\"\n",
    "embedding = HuggingFaceEmbeddings(model_name='BAAI/bge-small-zh-v1.5')\n",
    "\n",
    "chunk_size=5000\n",
    "chunk_overlap=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个 PyMuPDFLoader Class 实例，输入为待加载的 pdf 文档路径，加载PDF\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "\n",
    "# 调用 PyMuPDFLoader Class 的函数 load 对 pdf 文件进行加载\n",
    "pdf_pages = loader.load()\n",
    "\n",
    "# 对内容进行一下清洗\n",
    "data_pages = pdf_pages[13:-13]\n",
    "for page in data_pages:\n",
    "    page.page_content = clean_text(page.page_content)\n",
    "\n",
    "# 文档分块\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap, separator='')\n",
    "\n",
    "split_docs = text_splitter.split_documents(data_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 查看/修改metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/pumpkin_book.pdf',\n",
       " 'file_path': 'data/pumpkin_book.pdf',\n",
       " 'page': 17,\n",
       " 'total_pages': 196,\n",
       " 'format': 'PDF 1.5',\n",
       " 'title': '',\n",
       " 'author': '',\n",
       " 'subject': '',\n",
       " 'keywords': '',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'producer': 'xdvipdfmx (20200315)',\n",
       " 'creationDate': \"D:20230303170709-00'00'\",\n",
       " 'modDate': '',\n",
       " 'trapped': ''}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选择其中一个分片查看metadata，现在的metadata为分片过程中默认内容\n",
    "split_docs[4].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改分块文档的metadata，加入chunk_size 和 chunk_overlap\n",
    "for doc in split_docs:\n",
    "    doc.metadata['chunk_size'] = chunk_size\n",
    "    doc.metadata['chunk_overlap'] = chunk_overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 权限体系"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际使用中，我们经常使用metadata来设置权限体系，从而实现不同权限用户的数据隔离。下面我们将用上述数据集来模拟权限体系的操作。\n",
    "\n",
    "假定：本书的50-60页为敏感页面，需要访问权限为2才能进行访问。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    使用 OpenAI 嵌入将 PDF 书籍编码为向量存储。\n",
    "\n",
    "    参数:\n",
    "        path: PDF 文件的路径。\n",
    "        chunk_size: 每个文本块的期望大小。\n",
    "        chunk_overlap: 连续块之间的重叠量。\n",
    "\n",
    "    返回:\n",
    "        包含内容的向量存储。\n",
    "    \"\"\"\n",
    "\n",
    "    # 创建一个 PyMuPDFLoader Class 实例，输入为待加载的 pdf 文档路径，加载PDF\n",
    "    loader = PyMuPDFLoader(path)\n",
    "    \n",
    "    # 调用 PyMuPDFLoader Class 的函数 load 对 pdf 文件进行加载\n",
    "    pdf_pages = loader.load()\n",
    "    \n",
    "    # 第13页为南瓜书第一页正文，因此从13页开始,从倒数13页涉及敏感用语，因此从-13页结束\n",
    "    data_pages = pdf_pages[13:-13]\n",
    "\n",
    "    for page in data_pages:\n",
    "        page.page_content = clean_text(page.page_content)\n",
    "\n",
    "    # 文档分块\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, separator='')\n",
    "\n",
    "    split_docs = text_splitter.split_documents(data_pages)\n",
    "\n",
    "    # 修改metadat\n",
    "    for doc in split_docs:\n",
    "        # 加入chunk_size和chunk_overlap\n",
    "        doc.metadata['chunk_size'] = chunk_size\n",
    "        doc.metadata['chunk_overlap'] = chunk_overlap\n",
    "        # 加入数据访问权限\n",
    "        doc.metadata['data_level'] = 2 if doc.metadata['page'] >= 50 and doc.metadata['page'] <= 60 else 1\n",
    "\n",
    "    # 构建向量库\n",
    "    vectordb = Chroma.from_documents(documents=split_docs, embedding=embedding)\n",
    "\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将pdf放入到向量数据库中\n",
    "vectordb = encode_pdf(pdf_path, chunk_size=chunk_size, chunk_overlap=chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'author': '', 'chunk_overlap': 1000, 'chunk_size': 5000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'data_level': 2, 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 53, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='第5章神经网络神经网络类算法可以堪称当今最主流的一类机器学习算法，其本质上和前几章讲到的线性回归、对数几率回归、决策树等算法一样均属于机器学习算法，也是被发明用来完成分类和回归等任务。不过由于神经网络类算法在如今超强算力的加持下效果表现极其出色，且从理论角度来说神经网络层堆叠得越深其效果越好，因此也单独称用深层神经网络类算法所做的机器学习为深度学习，属于机器学习的子集。5.1神经元模型本节对神经元模型的介绍通俗易懂，在此不再赘述。本节第2段提到“阈值”(threshold)的概念时，“西瓜书”左侧边注特意强调是“阈(yù)”而不是“阀(fá)”，这是因为该字确实很容易认错，读者注意一下即可。图5.1所示的M-P神经元模型，其中的“M-P”便是两位作者McCulloch和Pitts的首字母简写。5.2感知机与多层网络5.2.1式(5.1)和式(5.2)的推导此式是感知机学习算法中的参数更新公式，下面依次给出感知机模型、学习策略和学习算法的具体介绍[1]：感知机模型：已知感知机由两层神经元组成，故感知机模型的公式可表示为y=fnXi=1wixi−θ!=f(wTx−θ)其中，x∈Rn，为样本的特征向量，是感知机模型的输入；w,θ是感知机模型的参数，w∈Rn，为权重，θ为阈值。假定f为阶跃函数，那么感知机模型的公式可进一步表示为（用ε(·)代表阶跃函数）y=ε(wTx−θ)=(1,wTx−θ⩾0;0,wTx−θ<0.由于n维空间中的超平面方程为w1x1+w2x2+···+wnxn+b=wTx+b=0所以此时感知机模型公式中的wTx−θ可以看作是n维空间中的一个超平面，将n维空间划分为wTx−θ⩾0和wTx−θ<0两个子空间，落在前一个子空间的样本对应的模型输出值为1，落在后一个子空间的样本对应的模型输出值为0，如此便实现了分类功能。感知机学习策略：给定一个数据集T={(x1,y1),(x2,y2),···,(xN,yN)}其中xi∈Rn,yi∈{0,1},i=1,2,···,N。如果存在某个超平面wTx+b=0能将数据集T中的正样本和负样本完全正确地划分到超平面两侧，即对所有yi=1的样本xi有wTxi+b⩾0，对所有yi=0的样本xi有wTxi+b<0，则称数据集T线性可分，否则称数据集T线性不可分。现给定一个线性可分的数据集T，感知机的学习目标是求得能对数据集T中的正负样本完全正确划分的分离超平面wTx−θ=0'),\n",
       " Document(metadata={'author': '', 'chunk_overlap': 1000, 'chunk_size': 5000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'data_level': 2, 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 57, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='5.3.5式(5.15)的推导参见式(5.13)的推导5.4全局最小与局部极小由图5.10可以直观理解局部极小和全局最小的概念，其余概念如模拟退火、遗传算法、启发式等，则需要查阅专业资料系统化学习。5.5其他常见神经网络本节所提到的神经网络其实如今已不太常见，更为常见的神经网络是下一节深度学习里提到的卷积神经网络、循环神经网络等。5.5.1式(5.18)的解释从式(5.18)可以看出，对于样本x来说，RBF网络的输出为q个ρ(x,ci)的线性组合。若换个角度来看这个问题，将q个ρ(x,ci)当作是将d维向量x基于式(5.19)进行特征转换后所得的q维特征，即˜x=(ρ(x,c1);ρ(x,c2);...;ρ(x,cq))，则式(5.18)求线性加权系数wi相当于求解第3.2节的线性回归f(˜x)=wT˜x+b，对于仅有的差别b来说，当然可以在式(5.18)中补加一个b。因此，RBF网络在确定q个神经元中心ci之后，接下来要做的就是线性回归。5.5.2式(5.20)的解释Boltzmann机（RestrictedBoltzmannMachine，简称RBM）本质上是一个引入了隐变量的无向图模型，其能量可理解为Egraph=Eedges+Enodes其中，Egraph表示图的能量，Eedges表示图中边的能量，Enodes表示图中结点的能量。边能量由两连接结点的值及其权重的乘积确定，即Eedgeij=−wijsisj；结点能量由结点的值及其阈值的乘积确定，即Enodei=−θisi。图中边的能量为所有边能量之和为Eedges=n−1Xi=1nXj=i+1Eedgeij=−n−1Xi=1nXj=i+1wijsisj图中结点的能量为所有结点能量之和Enodes=nXi=1Enodei=−nXi=1θisi故状态向量s所对应的Boltzmann机能量Egraph=Eedges+Enodes=−n−1Xi=1nXj=i+1wijsisj−nXi=1θisi5.5.3式(5.22)的解释受限Boltzmann机仅保留显层与隐层之间的连接。显层状态向量v=(v1;v2;...;vd)，隐层状态向量h=(h1;h2;...;hq)。显层状态向量v中的变量vi仅与隐层状态向量h有关，所以给定隐层状态向量h，有v1,v2,...,vd相互独立。5.5.4式(5.23)的解释由式(5.22)的解释同理可得，给定显层状态向量v，有h1,h2,...,hq相互独立。'),\n",
       " Document(metadata={'author': '', 'chunk_overlap': 1000, 'chunk_size': 5000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'data_level': 1, 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 30, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='第3章线性模型作为“西瓜书”介绍机器学习模型的开篇，线性模型也是机器学习中最为基础的模型，很多复杂模型均可认为由线性模型衍生而得，无论是曾经红极一时的支持向量机还是如今万众瞩目的神经网络，其中都有线性模型的影子。本章的线性回归和对数几率回归分别是回归和分类任务上常用的算法，因此属于重点内容，线性判别分析不常用，但是其核心思路和后续第10章将会讲到的经典降维算法主成分分析相同，因此也属于重点内容，且两者结合在一起看理解会更深刻。3.1基本形式第1章的1.2基本术语中讲述样本的定义时，我们说明了“西瓜书”和本书中向量的写法，当向量中的元素用分号“;”分隔时表示此向量为列向量，用逗号“,”分隔时表示为行向量。因此，式(3.2)中w=(w1;w2;...;wd)和x=(x1;x2;...;xd)均为d行1列的列向量。3.2线性回归3.2.1属性数值化为了能进行数学运算，样本中的非数值类属性都需要进行数值化。对于存在“序”关系的属性，可通过连续化将其转化为带有相对大小关系的连续值；对于不存在“序”关系的属性，可根据属性取值将其拆解为多个属性，例如“西瓜书”中所说的“瓜类”属性，可将其拆解为“是否是西瓜”、“是否是南瓜”、“是否是黄瓜”3个属性，其中每个属性的取值为1或0，1表示“是”，0表示“否”。具体地，假如现有3个瓜类样本：x1=(甜度=高;瓜类=西瓜),x2=(甜度=中;瓜类=南瓜),x3=(甜度=低;瓜类=黄瓜)，其中“甜度”属性存在序关系，因此可将“高”、“中”、“低”转化为{1.0,0.5,0.0}，“瓜类”属性不存在序关系，则按照上述方法进行拆解，3个瓜类样本数值化后的结果为：x1=(1.0;1;0;0),x1=(0.5;0;1;0),x1=(0.0;0;0;1)。以上针对样本属性所进行的处理工作便是第1章1.2基本术语中提到的“特征工程”范畴，完成属性数值化以后通常还会进行缺失值处理、规范化、降维等一系列处理工作。由于特征工程属于算法实践过程中需要掌握的内容，待学完机器学习算法以后，再进一步学习特征工程相关知识即可，在此先不展开。3.2.2式(3.4)的解释下面仅针对式(3.4)中的数学符号进行解释。首先解释一下符号“argmin”，其中“arg”是“argument”（参数）的前三个字母，“min”是“minimum”（最小值）的前三个字母，该符号表示求使目标函数达到最小值的参数取值。例如式(3.4)表示求出使目标函数Pmi=1(yi−wxi−b)2达到最小值的参数取值(w∗,b∗)，注意目标函数是以(w,b)为自变量的函数，(xi,yi)均是已知常量，即训练集中的样本数据。类似的符号还有“min”，例如将式(3.4)改为min(w,b)mXi=1(yi−wxi−b)2则表示求目标函数的最小值。对比知道，“min”和“argmin”的区别在于，前者输出目标函数的最小值，而后者输出使得目标函数达到最小值时的参数取值。若进一步修改式(3.4)为min(w,b)mXi=1(yi−wxi−b)2s.t.w>0,b<0.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一个人是权限较高，拥有数据权限为2，可以查看全部数据库\n",
    "hypothetical_doc = \"神经元模型\"\n",
    "similar_docs = vectordb.similarity_search(hypothetical_doc, k=3)\n",
    "similar_docs\n",
    "\n",
    "# 召回top3分别是 53、57、30页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'author': '', 'chunk_overlap': 1000, 'chunk_size': 5000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'data_level': 1, 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 30, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='第3章线性模型作为“西瓜书”介绍机器学习模型的开篇，线性模型也是机器学习中最为基础的模型，很多复杂模型均可认为由线性模型衍生而得，无论是曾经红极一时的支持向量机还是如今万众瞩目的神经网络，其中都有线性模型的影子。本章的线性回归和对数几率回归分别是回归和分类任务上常用的算法，因此属于重点内容，线性判别分析不常用，但是其核心思路和后续第10章将会讲到的经典降维算法主成分分析相同，因此也属于重点内容，且两者结合在一起看理解会更深刻。3.1基本形式第1章的1.2基本术语中讲述样本的定义时，我们说明了“西瓜书”和本书中向量的写法，当向量中的元素用分号“;”分隔时表示此向量为列向量，用逗号“,”分隔时表示为行向量。因此，式(3.2)中w=(w1;w2;...;wd)和x=(x1;x2;...;xd)均为d行1列的列向量。3.2线性回归3.2.1属性数值化为了能进行数学运算，样本中的非数值类属性都需要进行数值化。对于存在“序”关系的属性，可通过连续化将其转化为带有相对大小关系的连续值；对于不存在“序”关系的属性，可根据属性取值将其拆解为多个属性，例如“西瓜书”中所说的“瓜类”属性，可将其拆解为“是否是西瓜”、“是否是南瓜”、“是否是黄瓜”3个属性，其中每个属性的取值为1或0，1表示“是”，0表示“否”。具体地，假如现有3个瓜类样本：x1=(甜度=高;瓜类=西瓜),x2=(甜度=中;瓜类=南瓜),x3=(甜度=低;瓜类=黄瓜)，其中“甜度”属性存在序关系，因此可将“高”、“中”、“低”转化为{1.0,0.5,0.0}，“瓜类”属性不存在序关系，则按照上述方法进行拆解，3个瓜类样本数值化后的结果为：x1=(1.0;1;0;0),x1=(0.5;0;1;0),x1=(0.0;0;0;1)。以上针对样本属性所进行的处理工作便是第1章1.2基本术语中提到的“特征工程”范畴，完成属性数值化以后通常还会进行缺失值处理、规范化、降维等一系列处理工作。由于特征工程属于算法实践过程中需要掌握的内容，待学完机器学习算法以后，再进一步学习特征工程相关知识即可，在此先不展开。3.2.2式(3.4)的解释下面仅针对式(3.4)中的数学符号进行解释。首先解释一下符号“argmin”，其中“arg”是“argument”（参数）的前三个字母，“min”是“minimum”（最小值）的前三个字母，该符号表示求使目标函数达到最小值的参数取值。例如式(3.4)表示求出使目标函数Pmi=1(yi−wxi−b)2达到最小值的参数取值(w∗,b∗)，注意目标函数是以(w,b)为自变量的函数，(xi,yi)均是已知常量，即训练集中的样本数据。类似的符号还有“min”，例如将式(3.4)改为min(w,b)mXi=1(yi−wxi−b)2则表示求目标函数的最小值。对比知道，“min”和“argmin”的区别在于，前者输出目标函数的最小值，而后者输出使得目标函数达到最小值时的参数取值。若进一步修改式(3.4)为min(w,b)mXi=1(yi−wxi−b)2s.t.w>0,b<0.'),\n",
       " Document(metadata={'author': '', 'chunk_overlap': 1000, 'chunk_size': 5000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'data_level': 1, 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 177, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='第14章概率图模型本章介绍概率图模型，前三节分别介绍了有向图模型之隐马尔可夫模型以及无向图模型之马尔可夫随机场和条件随机场；接下来两节分别介绍精确推断和近似推断；最后一节简单介绍了话题模型的典型代表隐狄利克雷分配模型(LDA)。14.1隐马尔可夫模型本节前三段内容实际上是本章的概述，从第四段才开始介绍“隐马尔可夫模型”。马尔可夫的大名相信很多人听说过，比如马尔可夫链；虽然隐马尔可夫模型与马尔可夫链并非同一人提出，但其中关键字“马尔可夫”蕴含的概念是相同的，即系统下一时刻的状态仅由当前状态决定。14.1.1生成式模型和判别式模型一般来说,机器学习的任务是根据输入特征x预测输出变量y;生成式模型最终求得联合概率P(x,y),而判别式模型最终求得条件概率P(y|x)。统计机器学习算法都是基于样本独立同分布(independentandidenticallydistributed,简称i.i.d..)的假设,也就是说,假设样本空间中全体样本服从一个末知的“分布”D,我们获得的每个样本都是独立地从这个分布上采样获得的。对于一个样本(x,y),联合概率P(x,y)表示从样本空间中采样得到该样本的概率;因为P(x,y)表示“生成”样本本身的概率,故称之为“生成式模型”。而条件概率P(y|x)则表示已知x的条件下输出为y的概率,即根据x“判别”y,因此称为“判别式模型”。常见的对率回归、支持向量机等都属于判别式模型,而朴素贝叶斯则属于生成式模型。14.1.2式(14.1)的推导由概率公式P(AB)=P(A|B)·P(B)可得:P(x1,y1,...,xn,yn)=P(x1,...,xn|y1,...,yn)·P(y1,...,yn)其中,进一步可将P(y1,...,yn)做如下变换:P(y1,...,yn)=P(yn|y1,...,yn−1)·P(y1,...,yn−1)=P(yn|y1,...,yn−1)·P(yn−1|y1,...,yn−2)·P(y1,...,yn−2)=......=P(yn|y1,...,yn−1)·P(yn−1|y1,...,yn−2)·...·P(y2|y1)·P(y1)由于状态y1,...,yn构成马尔可夫链,即yt仅由yt−1决定;基于这种依赖关系,有P(yn|y1,...,yn−1)=P(yn|yn−1)P(yn−1|y1,...,yn−2)=P(yn−1|yn−2)P(yn−2|y1,...,yn−3)=P(yn−2|yn−3)因此P(y1,...,yn)可化简为P(y1,...,yn)=P(yn|yn−1)·P(yn−1|yn−2)·...·P(y2|y1)·P(y1)=P(y1)nYi=2P(yi|yi−1)'),\n",
       " Document(metadata={'author': '', 'chunk_overlap': 1000, 'chunk_size': 5000, 'creationDate': \"D:20230303170709-00'00'\", 'creator': 'LaTeX with hyperref', 'data_level': 1, 'file_path': 'data/pumpkin_book.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': '', 'page': 74, 'producer': 'xdvipdfmx (20200315)', 'source': 'data/pumpkin_book.pdf', 'subject': '', 'title': '', 'total_pages': 196, 'trapped': ''}, page_content='第7章贝叶斯分类器本章是从概率框架下的贝叶斯视角给出机器学习问题的建模方法，不同于前几章着重于算法具体实现，本章的理论性会更强。朴素贝叶斯算法常用于文本分类，例如用于广告邮件检测，贝叶斯网和EM算法均属于概率图模型的范畴，因此可合并至第14章一起学习。7.1贝叶斯决策论7.1.1式(7.5)的推导由式(7.1)和式(7.4)可得R(ci|x)=1∗P(c1|x)+...+1∗P(ci−1|x)+0∗P(ci|x)+1∗P(ci+1|x)+...+1∗P(cN|x)又PNj=1P(cj|x)=1，则R(ci|x)=1−P(ci|x)此即式(7.5）。7.1.2式(7.6)的推导将式(7.5)代入式(7.3)即可推得此式7.1.3判别式模型与生成式模型对于判别式模型来说，就是在已知x的条件下判别其类别标记c，即求后验概率P(c|x)，前几章介绍的模型都属于判别式模型的范畴，尤其是对数几率回归最为直接明了，式(3.23)和式(3.24)直接就是后验概率的形式。对于生成式模型来说，理解起来比较抽象，但是可通过思考以下两个问题来理解。(1)对于数据集来说，其中的样本是如何生成的？通常假设数据集中的样本服从独立同分布，即每个样本都是按照联合概率分布P(x,c)采样而得，也可以描述为根据P(x,c)生成的。(2)若已知样本x和联合概率分布P(x,c)，如何预测类别呢？若样本x和联合概率分布P(x,c)已知，则可以分别求出x属于各个类别的概率，即P(x,c1),P(x,c2),...,P(x,cN)，然后选择概率最大的类别作为样本x的预测结果。因此，之所以称为“生成式”模型，是因为所求的概率P(x,c)是生成样本x的概率。7.2极大似然估计7.2.1式(7.12)和(7.13)的推导根据式(7.11)和式(7.10)可知参数求解式为ˆθc=argmaxθcLL(θc)=argminθc−LL(θc)=argminθc−Xx∈DclogP(x|θc)由“西瓜书”上下文可知，此时假设概率密度函数p(x|c)∼N(µc,σ2c)，其等价于假设P(x|θc)=P\\x00x|µc,σ2c\\x01=1p(2π)d|Σc|exp\\x12−12(x−µc)TΣ−1c(x−µc)\\x13')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第一个人是权限较低，拥有数据权限为1，只能查看对应部分数据\n",
    "hypothetical_doc = \"神经元模型\"\n",
    "similar_docs = vectordb.similarity_search(hypothetical_doc, k=3, filter={\"data_level\": 1})\n",
    "similar_docs\n",
    "\n",
    "# 召回top3分别是 30、177、74页"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
