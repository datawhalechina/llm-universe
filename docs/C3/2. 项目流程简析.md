# 二、项目流程简析

以下我们将结合本实践项目与上文的整体流程介绍，简要分析本项目开发流程如下：

### 步骤一：项目规划与需求分析

#### 1.**项目目标**：基于个人知识库的问答助手

#### 2.**核心功能**：

1. 上传文档、创建知识库；
2. 选择知识库，检索用户提问的知识片段；
3. 提供知识片段与提问，获取大模型回答；
4. 流式回复；
5. 历史对话记录

#### 3.**确定技术架构和工具**

1. LangChain框架
2. Chroma知识库
3. 大模型使用 GPT、Claude、科大讯飞的星火大模型、文心一言、Chat-GLM2等
4. 前后端使用 Gradio 和 Streamlit。

### 步骤二：数据准备与向量知识库构建

本项目实现原理如下图所示（图片来源<https://github.com/chatchat-space/Langchain-Chatchat/blob/master/img/langchain+chatglm.png>），过程包括加载本地文档 -> 读取文本 -> 文本分割 -> 文本向量化 -> question向量化 -> 在文本向量中匹配出与问句向量最相似的 top k个 -> 匹配出的文本作为上下文和问题一起添加到 prompt中 -> 提交给 LLM生成回答。

![](../figures/flow_chart.png)

#### 1. 收集和整理用户提供的文档。

用户常用文档格式有 pdf、txt、doc 等，首先使用工具读取文本，通常使用 langchain 的文档加载器模块可以方便的将用户提供的文档加载进来，也可以使用一些 python 比较成熟的包进行读取。

由于目前大模型使用 token 的限制，我们需要对读取的文本进行切分，将较长的文本切分为较小的文本，这时一段文本就是一个单位的知识。

#### 2. 将文档词向量化

使用文本嵌入(Embeddings)对分割后的文档进行向量化，使语义相似的文本片段具有接近的向量表示。然后，存入向量数据库，这个流程正是创建索引(index)的过程。

向量数据库对各文档片段进行索引，支持快速检索。这样，当用户提出问题时，可以先将问题转换为向量，在数据库中快速找到语义最相关的文档片段。然后将这些文档片段与问题一起传递给语言模型，生成回答

#### 3. 将向量化后的文档导入Chroma知识库，建立知识库索引。

Langchain集成了超过30个不同的向量存储库。我们选择 Chroma 向量库是因为它轻量级且数据存储在内存中，这使得它非常容易启动和开始使用。

将用户知识库内容经过 embedding 存入向量知识库，然后用户每一次提问也会经过 embedding，利用向量相关性算法（例如余弦算法）找到最匹配的几个知识库片段，将这些知识库片段作为上下文，与用户问题一起作为 promt 提交给 LLM 回答。

### 步骤三：大模型集成与API连接

1. 集成GPT、Claude、星火、文心、GLM等大模型，配置API连接。
2. 编写代码，实现与大模型API的交互，以便获取问题答案。

### 步骤四：核心功能实现

1. 构建 Prompt Engineering，实现大模型回答功能，根据用户提问和知识库内容生成回答。
3. 实现流式回复，允许用户进行多轮对话。
4. 添加历史对话记录功能，保存用户与助手的交互历史。

### 步骤五：核心功能迭代优化

1. 进行验证评估，收集 Bad Case。
2. 根据 Bad Case 迭代优化核心功能实现。

### 步骤六：前端与用户交互界面开发

1. 使用Gradio和Streamlit搭建前端界面。
2. 实现用户上传文档、创建知识库的功能。
3. 设计用户界面，包括问题输入、知识库选择、历史记录展示等。

### 步骤七：部署测试与上线

1. 部署问答助手到服务器或云平台，确保可在互联网上访问。
2. 进行生产环境测试，确保系统稳定。
3. 上线并向用户发布。

### 步骤八：维护与持续改进

1. 监测系统性能和用户反馈，及时处理问题。
2. 定期更新知识库，添加新的文档和信息。
3. 收集用户需求，进行系统改进和功能扩展。

整个流程将确保项目从规划、开发、测试到上线和维护都能够顺利进行，为用户提供高质量的基于个人知识库的问答助手。