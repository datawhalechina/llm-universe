{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505a940f-80d8-49e2-8be2-2da687b9fc88",
   "metadata": {},
   "source": [
    "# 增强阶段"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1164d27",
   "metadata": {},
   "source": [
    "在 RAG 流程中，检索到的文档片段往往是孤立的，缺乏足够的上下文信息。这就像盲人摸象，LLM 只能根据有限的信息进行推断，导致答案的准确性和完整性受到影响。为了解决这个问题，我们需要对检索到的上下文进行增强，为 LLM 提供更全面的信息，帮助它更好地理解文档的含义和用户的意图。上下文增强的目标是打破信息孤岛，建立信息之间的联系，从而提高 RAG 系统的整体性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f02a46c",
   "metadata": {},
   "source": [
    "   先说一下文档层次结构概念：\n",
    "   \n",
    "   这是一种在切割文档时按照特定层次进行的方法。\n",
    "   层次结构包括广度和深度两个维度，例如，一个长文档可以看作是由多个章节组成，每个章节由多个段落组成，每个段落进一步分割成多个句子。在这里，章节、段落和句子之间的相互关系分别代表了不同的层次关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1122fcb-3981-4039-bedc-b1641786f457",
   "metadata": {},
   "source": [
    "## 一、AutoMergingRetriever自动合并检索\n",
    "\n",
    "### 自动合并检索\n",
    "   自动合并检索技术将文档按照层次结构切割成不同的块，然后在检索时，将最小的结构单元（叶子节点）与查询问题进行相似度匹配。如果一个父节点上的多数叶子节点与问题匹配，则父节点文档作为上下文（context）返回给大型语言模型（LLM）。\n",
    "   自动合并检索技术通过文档层次结构和自动合并的方法提高了检索的准确性和效率。\n",
    "1. 加载文档建立分层节点图（父节点+子节点),会把大块文档放在父节点，并且将大块文档分割之后的小文档放在子节点\n",
    "2. 查询的时候检索子节点，当查询的子节点数量足够多的时候即同一个父节点下检索子节点达到给定阈值则返回此父文档，否则不返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53689205",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"sk-你的sk\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17627155",
   "metadata": {},
   "source": [
    "首先构建一个字典，包含文件的八个标准问答对，key是问题，value是期望rag的答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701bc0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_dict = {\n",
    "        \"介绍下SVM算法\": \"是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机\",\n",
    "        \"用通俗的语言介绍下强化学习\": \"监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略\",\n",
    "        \"BN 和 LN 区别\": \"Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。\",\n",
    "        \"讲讲self attention\": \"Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系\",\n",
    "        \"Bert 的预训练过程\": \"Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。\",\n",
    "        \"GPT 与 Bert 的区别\": \"GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。\",\n",
    "        \"pca 属于有监督还是无监督\": \"PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。\",\n",
    "        \"介绍 transformer 算法\": \"Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5d7996",
   "metadata": {},
   "source": [
    "构建评估prompt和评估函数：\n",
    "\n",
    "\n",
    "意在让大模型判断生成的结果和期望的结果是否含义一致，在下面使用各种advanecd-rag方法时会经常调用， 主要使用Llama Packs，Llama Packs是对llamaindex的更加高级的封装，几行代码实现各种高级RAG，如果想看具体的流程也可以直接进入github查看源码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c22b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "import re\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "eval_template = PromptTemplate(\"\"\"\n",
    "            输入:\n",
    "            Question: {question}\n",
    "            LLM-Answer: {llm_answer}\n",
    "            Expected-Answer: {expected_answer}\n",
    "\n",
    "            任务:\n",
    "            比较LLM-Answer和Expected-Answer，确定它们是否传达类似的含义。\n",
    "\n",
    "            输出:\n",
    "            意思一致：如果LLM-Answer和Expected-Answer传达类似的含义。 \n",
    "            意思不同：如果LLM-Answer和Expected-Answer传达完全不同的含义。\n",
    "\n",
    "            示例： \n",
    "            Question：法国的首都是什么？ \n",
    "            LLM-Answer: 巴黎 \n",
    "            Expected-Answer: 光之城\n",
    "\n",
    "            输出:\n",
    "            意思一致\"\"\")\n",
    "    \n",
    "def eval_llm_output(row, llm):\n",
    "    ''' Eval llm_answer and expected_answer for the question with llm '''\n",
    "    question = row['question']\n",
    "    llm_answer = row['llm_answer']\n",
    "    expected_answer = row['expected_answer']\n",
    "    prompt_message = eval_template.format(question=question, llm_answer=llm_answer, expected_answer=expected_answer)\n",
    "    answer = llm.complete(prompt_message).text\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ff0325",
   "metadata": {},
   "source": [
    "初始化 llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0223a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a966a7",
   "metadata": {},
   "source": [
    "载入需要查询的数据文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea2ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PDFReader\n",
    "from pathlib import Path\n",
    "\n",
    "loader = PDFReader()\n",
    "documents = loader.load_data(\n",
    "        file=Path(f\"./data/face.pdf\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1f7adb",
   "metadata": {},
   "source": [
    "下载自动合并检索 pack 并构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2962bddd-d644-40d6-9d20-b8ae376b12a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "\n",
    "AutoMergingRetrieverPack = download_llama_pack(\n",
    "    \"AutoMergingRetrieverPack\",\n",
    "    \"./auto_merging_retriever_pack\",\n",
    "    # leave the below commented out (was for testing purposes)\n",
    "    # llama_hub_url=\"https://raw.githubusercontent.com/run-llama/llama-hub/jerry/add_llama_packs/llama_hub\"\n",
    ")\n",
    "auto_merging_pack = AutoMergingRetrieverPack(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c066a43",
   "metadata": {},
   "source": [
    "用自动合并检索 pack 进行检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ef1fdce1-a638-4452-9c7b-16078c094add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Merging 3 nodes into parent node.\n",
      "> Parent node id: 0349a78c-e46c-4eb0-a93f-980ca83956cd.\n",
      "> Parent node text: 第4页共46页第十七篇：2022年4月10日百度机器学习方向暑期实习面试题6道\n",
      "1、介绍下SVM算法\n",
      "是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感...\n",
      "\n",
      "> Merging 2 nodes into parent node.\n",
      "> Parent node id: 5d6a3c28-7aeb-4ee0-8d51-fd30107080e0.\n",
      "> Parent node text: 3、介绍svm算法\n",
      "是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感\n",
      "知机。\n",
      "SVM可分为三种：\n",
      "线性可分SVM\n",
      "当训练数据线性可分时，通过最大化硬间...\n",
      "\n",
      "> Merging 1 nodes into parent node.\n",
      "> Parent node id: 9c3f958c-7739-4b5f-98df-217ed9a30f0c.\n",
      "> Parent node text: 4、传统机器学习方法了解多少\n",
      "这个可以说逻辑回归模型，逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度\n",
      "下降法来求解参数，从而达到将数据二分类的目的。\n",
      "逻辑回归的优缺点\n",
      "优点：\n",
      "\n",
      "> Merging 1 nodes into parent node.\n",
      "> Parent node id: 5b4a0423-6125-4399-954a-33b99380e478.\n",
      "> Parent node text: LAMB可将BERT预训练的批量大小扩展到64K，且不会造成准确率损失，76分钟就可以完成BERT的训\n",
      "练。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "answers = {'question':[], 'llm_answer': [], 'expected_answer': []}\n",
    "\n",
    "for question in qna_dict.keys():\n",
    "    expected_answer = qna_dict[question]\n",
    "    # result\n",
    "    response = auto_merging_pack.run(question)\n",
    "    answers['question'].append(question)\n",
    "    answers['llm_answer'].append(response.response)\n",
    "    answers['expected_answer'].append(expected_answer)\n",
    "\n",
    "answers_df = pd.DataFrame(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a983198f",
   "metadata": {},
   "source": [
    "用 `eval_llm_output` 来评估 llm-answer 和 expected-answer 是否一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe2a8bd-4e41-401b-acc7-10e8cdfff345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>llm_answer</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>rag_eval_results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>介绍下SVM算法</td>\n",
       "      <td>SVM算法是一种二分类模型，其基本模型是定义在特征空间上的间隔最大的线性分类器，通过最大化硬间隔可以学习得到一个线性分类器，即硬间隔SVM。当训练数据不能线性可分但是近似线性可分时，可以通过最大化软间隔学习到一个线性分类器，即软间隔SVM。此外，当训练数据线性不可分时，可以通过使用核技巧和最大化软间隔学习到一个非线性SVM。</td>\n",
       "      <td>是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都在介绍SVM算法是一种二分类模型，其基本模型是定义在特征空间上的间隔最大的线性分类器。虽然LLM-Answer中提到了硬间隔SVM、软间隔SVM和非线性SVM的概念，但整体来看，它们传达的含义是相似的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>用通俗的语言介绍下强化学习</td>\n",
       "      <td>Reinforcement Learning can be explained in simple terms as a learning process where an agent learns to make decisions by receiving feedback in the form of rewards or penalties based on its actions. It is like a trial-and-error method where the agent learns through experience by interacting with its environment.</td>\n",
       "      <td>监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BN 和 LN 区别</td>\n",
       "      <td>LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。</td>\n",
       "      <td>Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都在解释BN和LN的区别，提到了LN是对单个样本的所有维度特征做归一化，而BN是对不同神经元输入计算均值和方差。两者都强调了LN中同层神经元输入拥有相同的均值和方差，而BN中同一个batch中的输入拥有相同的均值和方差。因此，虽然表达方式有些许不同，但传达的含义是一致的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>讲讲self attention</td>\n",
       "      <td>Self attention is a mechanism that differs from traditional attention in how it operates. While traditional attention calculates dependencies between hidden states of the source and target sides, self attention focuses on capturing dependencies within the source or target input itself. By separately considering the source and target sides, self attention captures relationships between words within each side before incorporating this information into the overall attention mechanism. This approach allows self attention to effectively model dependencies within each side and between the source and target sides of the input.</td>\n",
       "      <td>Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bert 的预训练过程</td>\n",
       "      <td>Bert的预训练过程主要包含两个任务，即MLM和NSP。MLM任务涉及对句子中15%的词进行随机mask，并利用上下文来预测这些被mask的词；而NSP任务则是预测两个句子是否在原始文档中是相邻的。BERT在预训练阶段同时进行这两个任务，并将它们的Loss相加。</td>\n",
       "      <td>Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都描述了Bert的预训练过程包含MLM和NSP两个任务，以及任务的具体操作方式。两者传达的含义是相似的，只是表达方式略有不同。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GPT 与 Bert 的区别</td>\n",
       "      <td>GPT是单向模型，只能利用上文信息，而BERT是双向模型，可以同时利用上下文信息。</td>\n",
       "      <td>GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pca 属于有监督还是无监督</td>\n",
       "      <td>PCA belongs to unsupervised learning.</td>\n",
       "      <td>PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer传达了相同的含义，都表明PCA属于无监督学习。虽然表达方式略有不同，但传达的核心信息是一致的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>介绍 transformer 算法</td>\n",
       "      <td>Transformer算法是一个典型的encoder-decoder模型，其中包括Encoder端和Decoder端，每个端都由6个Block组成。Encoder端的Block包括两个模块，多头self-attention模块和前馈神经网络模块，同时每个模块都包含残差层和Layer Normalization层。在功能上，Transformer中的encoder是双向的，用于编码；而decoder是单向的，用于解码，可用于生成任务。</td>\n",
       "      <td>Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            question  \\\n",
       "0           介绍下SVM算法   \n",
       "1      用通俗的语言介绍下强化学习   \n",
       "2         BN 和 LN 区别   \n",
       "3   讲讲self attention   \n",
       "4        Bert 的预训练过程   \n",
       "5     GPT 与 Bert 的区别   \n",
       "6     pca 属于有监督还是无监督   \n",
       "7  介绍 transformer 算法   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            llm_answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  SVM算法是一种二分类模型，其基本模型是定义在特征空间上的间隔最大的线性分类器，通过最大化硬间隔可以学习得到一个线性分类器，即硬间隔SVM。当训练数据不能线性可分但是近似线性可分时，可以通过最大化软间隔学习到一个线性分类器，即软间隔SVM。此外，当训练数据线性不可分时，可以通过使用核技巧和最大化软间隔学习到一个非线性SVM。   \n",
       "1                                                                                                                                                                                                                                                                                                                             Reinforcement Learning can be explained in simple terms as a learning process where an agent learns to make decisions by receiving feedback in the form of rewards or penalties based on its actions. It is like a trial-and-error method where the agent learns through experience by interacting with its environment.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。   \n",
       "3  Self attention is a mechanism that differs from traditional attention in how it operates. While traditional attention calculates dependencies between hidden states of the source and target sides, self attention focuses on capturing dependencies within the source or target input itself. By separately considering the source and target sides, self attention captures relationships between words within each side before incorporating this information into the overall attention mechanism. This approach allows self attention to effectively model dependencies within each side and between the source and target sides of the input.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Bert的预训练过程主要包含两个任务，即MLM和NSP。MLM任务涉及对句子中15%的词进行随机mask，并利用上下文来预测这些被mask的词；而NSP任务则是预测两个句子是否在原始文档中是相邻的。BERT在预训练阶段同时进行这两个任务，并将它们的Loss相加。   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            GPT是单向模型，只能利用上文信息，而BERT是双向模型，可以同时利用上下文信息。   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                PCA belongs to unsupervised learning.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                           Transformer算法是一个典型的encoder-decoder模型，其中包括Encoder端和Decoder端，每个端都由6个Block组成。Encoder端的Block包括两个模块，多头self-attention模块和前馈神经网络模块，同时每个模块都包含残差层和Layer Normalization层。在功能上，Transformer中的encoder是双向的，用于编码；而decoder是单向的，用于解码，可用于生成任务。   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          expected_answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机   \n",
       "1                                                                                                                                                               监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略   \n",
       "2                                                                                                                                                                                                                                                                                        Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。   \n",
       "3  Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系   \n",
       "4                                                                                                                                                                                                                                                                                                             Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                               GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。   \n",
       "7                                                                                                                                                                                                                                                                Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。   \n",
       "\n",
       "                                                                                                                                                          rag_eval_results  \n",
       "0                                    意思一致。LLM-Answer和Expected-Answer都在介绍SVM算法是一种二分类模型，其基本模型是定义在特征空间上的间隔最大的线性分类器。虽然LLM-Answer中提到了硬间隔SVM、软间隔SVM和非线性SVM的概念，但整体来看，它们传达的含义是相似的。  \n",
       "1                                                                                                                                                                     意思不同  \n",
       "2  意思一致。LLM-Answer和Expected-Answer都在解释BN和LN的区别，提到了LN是对单个样本的所有维度特征做归一化，而BN是对不同神经元输入计算均值和方差。两者都强调了LN中同层神经元输入拥有相同的均值和方差，而BN中同一个batch中的输入拥有相同的均值和方差。因此，虽然表达方式有些许不同，但传达的含义是一致的。  \n",
       "3                                                                                                                                                                     意思不同  \n",
       "4                                                                           意思一致。LLM-Answer和Expected-Answer都描述了Bert的预训练过程包含MLM和NSP两个任务，以及任务的具体操作方式。两者传达的含义是相似的，只是表达方式略有不同。  \n",
       "5                                                                                                                                                                     意思不同  \n",
       "6                                                                                           意思一致。LLM-Answer和Expected-Answer传达了相同的含义，都表明PCA属于无监督学习。虽然表达方式略有不同，但传达的核心信息是一致的。  \n",
       "7                                                                                                                                                                     意思不同  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "answers_df['rag_eval_results'] = answers_df.apply(lambda r: eval_llm_output(r, llm), axis=1)\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(answers_df)\n",
    "answers_df.to_excel('auto_merging_pack_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b83f743-c9a4-4319-b3ef-4cdd9705f18c",
   "metadata": {},
   "source": [
    "## 二、Small-to-big Retrieval小到大块检索:\n",
    "1. 输入文档分成初始父分块,将父块细分成子块,\n",
    "2. 子文档和父文档建链接,并为子块建立索引。\n",
    "3. 检索时中用子块,但提供对应父块给llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420fc983",
   "metadata": {},
   "source": [
    "下载小到大块检索 Pack 并构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "476756bd-1ac5-4b68-abe7-c5570353038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install llama-index-embeddings-huggingface==0.2.0\n",
    "\n",
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "\n",
    "RecursiveRetrieverSmallToBigPack = download_llama_pack(\n",
    "    \"RecursiveRetrieverSmallToBigPack\",\n",
    "    \"./recursive_retriever_stb_pack\",\n",
    "    # leave the below commented out (was for testing purposes)\n",
    "    # llama_hub_url=\"https://raw.githubusercontent.com/run-llama/llama-hub/jerry/add_llama_packs/llama_hub\",\n",
    ")\n",
    "\n",
    "recursive_retriever_stb_pack = RecursiveRetrieverSmallToBigPack(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e62518-fd2f-478b-bb61-109d6b67398d",
   "metadata": {},
   "source": [
    "用小到大块检索 Pack 进行检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad1d1c95-9401-4032-9136-e7979529cfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieving with query id None: 介绍下SVM算法\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-18\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-18: 介绍下SVM算法\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id None: 用通俗的语言介绍下强化学习\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-35\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-35: 用通俗的语言介绍下强化学习\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-0\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-0: 用通俗的语言介绍下强化学习\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id None: BN 和 LN 区别\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-28\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-28: BN 和 LN 区别\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id None: 讲讲self attention\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-7\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-7: 讲讲self attention\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id None: Bert 的预训练过程\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-37\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-37: Bert 的预训练过程\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-1\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-1: Bert 的预训练过程\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id None: GPT 与 Bert 的区别\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-8\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-8: GPT 与 Bert 的区别\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id None: pca 属于有监督还是无监督\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-17\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-17: pca 属于有监督还是无监督\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id None: 介绍 transformer 算法\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-18\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-18: 介绍 transformer 算法\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-0\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-0: 介绍 transformer 算法\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "answers = {'question':[], 'llm_answer': [], 'expected_answer': []}\n",
    "\n",
    "for question in qna_dict.keys():\n",
    "    expected_answer = qna_dict[question]\n",
    "    # result\n",
    "    response = recursive_retriever_stb_pack.run(question)\n",
    "    answers['question'].append(question)\n",
    "    answers['llm_answer'].append(response.response)\n",
    "    answers['expected_answer'].append(expected_answer)\n",
    "\n",
    "answers_df = pd.DataFrame(answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df14f88",
   "metadata": {},
   "source": [
    "用 `eval_llm_output` 来评估 llm-answer 和 expected-answer 是否一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "750866bd-dfc2-4e33-90e0-0ffd1209a451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>llm_answer</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>rag_eval_results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>介绍下SVM算法</td>\n",
       "      <td>The SVM algorithm's learning strategy focuses on maximizing the margin, which can be formalized as solving a convex quadratic programming problem or equivalently minimizing the regularized hinge loss function. The learning algorithm for SVM involves solving the convex quadratic programming optimization problem. SVM selects kernel functions based on the data separability - Linear kernel is primarily used for linearly separable cases with fewer parameters and faster speed, while RBF kernel is suitable for linearly inseparable cases but relies heavily on parameters, often requiring parameter tuning through techniques like cross-validation.</td>\n",
       "      <td>是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>用通俗的语言介绍下强化学习</td>\n",
       "      <td>In simple terms, reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with its environment. It works on a reward-based system, where the agent receives feedback in the form of rewards or penalties for its actions. The goal is for the agent to learn the best sequence of actions that will lead to the highest cumulative reward over time.</td>\n",
       "      <td>监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略</td>\n",
       "      <td>意思不同。LLM-Answer介绍了强化学习的基本概念和原理，而Expected-Answer则是在比喻的方式下解释了强化学习与监督学习的区别，强调了强化学习通过实践来获取知识的特点。两者虽然都在介绍强化学习，但从不同角度和方式进行了阐述。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BN 和 LN 区别</td>\n",
       "      <td>BN 和 LN 区别在于，BN 是指二叉树的节点，通常表示二叉树的左节点，而 LN 则是指二维列表中的元素，通常表示二维列表中的行或列。</td>\n",
       "      <td>Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>讲讲self attention</td>\n",
       "      <td>SelfAttention is a mechanism that differs significantly from traditional Attention. While traditional Attention calculates dependencies between hidden states of the source and target ends, SelfAttention focuses on the source or target input itself. It captures dependencies between words within the source or target input. The self-attention results from the source end are then incorporated into the attention obtained from the target end to capture dependencies between words in both the source and target ends. This approach allows SelfAttention to effectively capture dependencies not only between words in the source and target ends but also within the source or target input itself.</td>\n",
       "      <td>Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系</td>\n",
       "      <td>意思不同。LLM-Answer和Expected-Answer虽然都在讲Self Attention，但表达方式和详细内容有所不同。LLM-Answer更加技术化和简洁，而Expected-Answer更加详细和易懂。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bert 的预训练过程</td>\n",
       "      <td>During the pre-training process of Bert, a portion of words in the sentence encoding are masked. This allows the model to predict the masked words based on the surrounding words. By artificially masking these words, the computer knows the correct values of the masked words and can assess the accuracy of the model's predictions. This process helps enhance Bert's bidirectional encoding capabilities.</td>\n",
       "      <td>Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GPT 与 Bert 的区别</td>\n",
       "      <td>GPT是单向模型，只能利用上文信息，无法利用上下文信息；而BERT是双向模型，可以同时利用上文和下文信息。</td>\n",
       "      <td>GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。</td>\n",
       "      <td>意思不同。LLM-Answer提到了GPT是单向模型，无法利用上下文信息，只能利用上文；而BERT是双向模型。而Expected-Answer则提到了GPT是基于自回归模型，可以应用在NLU和NLG两大任务，而BERT是基于自编码模型，只能完成NLU任务，无法直接应用在文本生成上面。两者虽然都在讨论GPT和BERT的区别，但具体提到的内容有所不同。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pca 属于有监督还是无监督</td>\n",
       "      <td>PCA belongs to unsupervised learning.</td>\n",
       "      <td>PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都表达了PCA属于无监督学习这一事实，只是表达方式略有不同。LLM-Answer直接指出了PCA属于无监督学习，而Expected-Answer则解释了为什么PCA属于无监督学习。因此，它们传达的含义是相似的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>介绍 transformer 算法</td>\n",
       "      <td>Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个Block，Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder端的Block包括三个模块，多头self-attention模块，多头Encoder-Decoder attention交互模块，以及一个前馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和LayerNormalization层。</td>\n",
       "      <td>Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。</td>\n",
       "      <td>：是的，意思一致。LLM-Answer和Expected-Answer传达的含义非常相似，描述了Transformer算法的结构和组成部分。虽然表达方式有些许差异，但整体内容是一致的。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            question  \\\n",
       "0           介绍下SVM算法   \n",
       "1      用通俗的语言介绍下强化学习   \n",
       "2         BN 和 LN 区别   \n",
       "3   讲讲self attention   \n",
       "4        Bert 的预训练过程   \n",
       "5     GPT 与 Bert 的区别   \n",
       "6     pca 属于有监督还是无监督   \n",
       "7  介绍 transformer 算法   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         llm_answer  \\\n",
       "0                                              The SVM algorithm's learning strategy focuses on maximizing the margin, which can be formalized as solving a convex quadratic programming problem or equivalently minimizing the regularized hinge loss function. The learning algorithm for SVM involves solving the convex quadratic programming optimization problem. SVM selects kernel functions based on the data separability - Linear kernel is primarily used for linearly separable cases with fewer parameters and faster speed, while RBF kernel is suitable for linearly inseparable cases but relies heavily on parameters, often requiring parameter tuning through techniques like cross-validation.   \n",
       "1                                                                                                                                                                                                                                                                                                           In simple terms, reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with its environment. It works on a reward-based system, where the agent receives feedback in the form of rewards or penalties for its actions. The goal is for the agent to learn the best sequence of actions that will lead to the highest cumulative reward over time.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              BN 和 LN 区别在于，BN 是指二叉树的节点，通常表示二叉树的左节点，而 LN 则是指二维列表中的元素，通常表示二维列表中的行或列。   \n",
       "3  SelfAttention is a mechanism that differs significantly from traditional Attention. While traditional Attention calculates dependencies between hidden states of the source and target ends, SelfAttention focuses on the source or target input itself. It captures dependencies between words within the source or target input. The self-attention results from the source end are then incorporated into the attention obtained from the target end to capture dependencies between words in both the source and target ends. This approach allows SelfAttention to effectively capture dependencies not only between words in the source and target ends but also within the source or target input itself.   \n",
       "4                                                                                                                                                                                                                                                                                                  During the pre-training process of Bert, a portion of words in the sentence encoding are masked. This allows the model to predict the masked words based on the surrounding words. By artificially masking these words, the computer knows the correct values of the masked words and can assess the accuracy of the model's predictions. This process helps enhance Bert's bidirectional encoding capabilities.   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             GPT是单向模型，只能利用上文信息，无法利用上下文信息；而BERT是双向模型，可以同时利用上文和下文信息。   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             PCA belongs to unsupervised learning.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                   Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个Block，Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder端的Block包括三个模块，多头self-attention模块，多头Encoder-Decoder attention交互模块，以及一个前馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和LayerNormalization层。   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          expected_answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机   \n",
       "1                                                                                                                                                               监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略   \n",
       "2                                                                                                                                                                                                                                                                                        Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。   \n",
       "3  Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系   \n",
       "4                                                                                                                                                                                                                                                                                                             Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                               GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。   \n",
       "7                                                                                                                                                                                                                                                                Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。   \n",
       "\n",
       "                                                                                                                                                                  rag_eval_results  \n",
       "0                                                                                                                                                                             意思不同  \n",
       "1                                                         意思不同。LLM-Answer介绍了强化学习的基本概念和原理，而Expected-Answer则是在比喻的方式下解释了强化学习与监督学习的区别，强调了强化学习通过实践来获取知识的特点。两者虽然都在介绍强化学习，但从不同角度和方式进行了阐述。  \n",
       "2                                                                                                                                                                             意思不同  \n",
       "3                                                                    意思不同。LLM-Answer和Expected-Answer虽然都在讲Self Attention，但表达方式和详细内容有所不同。LLM-Answer更加技术化和简洁，而Expected-Answer更加详细和易懂。  \n",
       "4                                                                                                                                                                             意思不同  \n",
       "5  意思不同。LLM-Answer提到了GPT是单向模型，无法利用上下文信息，只能利用上文；而BERT是双向模型。而Expected-Answer则提到了GPT是基于自回归模型，可以应用在NLU和NLG两大任务，而BERT是基于自编码模型，只能完成NLU任务，无法直接应用在文本生成上面。两者虽然都在讨论GPT和BERT的区别，但具体提到的内容有所不同。  \n",
       "6                                         意思一致。LLM-Answer和Expected-Answer都表达了PCA属于无监督学习这一事实，只是表达方式略有不同。LLM-Answer直接指出了PCA属于无监督学习，而Expected-Answer则解释了为什么PCA属于无监督学习。因此，它们传达的含义是相似的。  \n",
       "7                                                                                     ：是的，意思一致。LLM-Answer和Expected-Answer传达的含义非常相似，描述了Transformer算法的结构和组成部分。虽然表达方式有些许差异，但整体内容是一致的。  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eval llm-answer with expected-answer\n",
    "answers_df['rag_eval_results'] = answers_df.apply(lambda r: eval_llm_output(r, llm), axis=1)\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(answers_df)\n",
    "answers_df.to_excel('recursive_retriever_stb_pack_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc43c51-18a5-42c9-86f2-7eb8e5f9ff33",
   "metadata": {},
   "source": [
    "## 三、Sentence Window Retrieval句子窗口检索\n",
    "1. 类似小到大块检索，先将文档解析为每个子块中的句子，\n",
    "2. 对单句进行检索。并将检索到的句子连同句子窗口(当前句子的左边的句子和右边的句子)一起传递llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967bf56a",
   "metadata": {},
   "source": [
    "下载句子窗口检索 Pack 并构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a05032a3-59c6-4ddb-b6e6-fabf8ee93569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d041a666dc41089b74ebebdebb6709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AAAAAAAAAAAIGC\\miniconda\\envs\\llamaindex\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Alienware\\AppData\\Local\\llama_index\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e0079896aa4b06ab6141052f3da74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a685e24a9d475fb46441c1ee23a8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d778ecafddc4ec7b92f8589d84c62eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c431a71ad134651a756f4409fca80ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4483c9a578342b2a53dfa70200b3540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e7a89a360343578cfd29b1db7c3e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c037a0ff302485da496eaf95293b3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3032b7f843cb4510875151138afb8769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "\n",
    "SentenceWindowRetrieverPack = download_llama_pack(\n",
    "    \"SentenceWindowRetrieverPack\",\n",
    "    \"./sentence_window_retriever_pack\",\n",
    "    # leave the below commented out (was for testing purposes)\n",
    "    # llama_hub_url=\"https://raw.githubusercontent.com/run-llama/llama-hub/jerry/add_llama_packs/llama_hub\",\n",
    ")\n",
    "sentence_window_retriever_pack = SentenceWindowRetrieverPack(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b52b2b",
   "metadata": {},
   "source": [
    "用句子窗口检索 Pack 进行检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ea219d6-a94e-4ca3-9f07-d5f7188e3af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "answers = {'question':[], 'llm_answer': [], 'expected_answer': []}\n",
    "\n",
    "for question in qna_dict.keys():\n",
    "    expected_answer = qna_dict[question]\n",
    "    # result\n",
    "    response = sentence_window_retriever_pack.run(question)\n",
    "    answers['question'].append(question)\n",
    "    answers['llm_answer'].append(response.response)\n",
    "    answers['expected_answer'].append(expected_answer)\n",
    "\n",
    "answers_df = pd.DataFrame(answers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112d2a9b",
   "metadata": {},
   "source": [
    "用 `eval_llm_output` 来评估 llm-answer 和 expected-answer 是否一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5b54025-15e7-4fd2-b947-ae3c68ca557b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>llm_answer</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>rag_eval_results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>介绍下SVM算法</td>\n",
       "      <td>SVM is a binary classification model that defines the maximum margin linear classifier in the feature space, distinguishing it from the perceptron. SVM can be categorized into three types: linear separable SVM for linearly separable training data, linear SVM for data that is not linearly separable but approximately so, and nonlinear SVM for data that is not linearly separable, utilizing kernel tricks and maximizing soft margins. SVM's learning strategy focuses on maximizing the margin, which can be formalized as solving a convex quadratic programming problem or minimizing the regularized hinge loss function. The selection of the kernel function in SVM depends on the data characteristics, with Linear kernel suitable for linearly separable cases and RBF kernel for linearly inseparable cases, although parameter tuning for RBF kernel often involves cross-validation due to its dependency on parameters.</td>\n",
       "      <td>是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都在介绍SVM算法的基本原理和特点，虽然表达方式略有不同，但传达的含义是一致的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>用通俗的语言介绍下强化学习</td>\n",
       "      <td>Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with its environment. It works on a reward system, where the agent receives positive or negative feedback based on its actions. The goal is for the agent to learn the best way to achieve a specific objective by maximizing rewards over time through trial and error.</td>\n",
       "      <td>监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BN 和 LN 区别</td>\n",
       "      <td>Batch Normalization (BN) is applied to each batch of training data by normalizing based on the batch's mean and variance, while Layer Normalization (LN) normalizes across the features for each sample. LN does not depend on batch size and is used in scenarios like recurrent neural networks.</td>\n",
       "      <td>Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。</td>\n",
       "      <td>意思一致</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>讲讲self attention</td>\n",
       "      <td>SelfAttention is a mechanism that differs significantly from traditional Attention. While traditional Attention calculates dependencies between hidden states of the source and target ends, SelfAttention operates independently on the source and target ends. It focuses solely on the self-related dependencies within the source or target input, capturing the relationships between words within the source or target end. The self-attention results from the source end are then incorporated into the attention obtained from the target end, capturing dependencies between words in both the source and target ends. This approach is more effective than traditional Attention mechanisms because it considers not only the dependencies between words in the source and target ends but also effectively captures dependencies between words within the source or target end themselves.</td>\n",
       "      <td>Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都在讲述Self Attention与传统Attention机制的区别，以及Self Attention的工作原理和优势。虽然表达方式有所不同，但传达的含义是一致的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bert 的预训练过程</td>\n",
       "      <td>During the pre-training process of Bert, a portion of words in the sentence encoding is masked. This allows the model to predict the masked words based on the surrounding words. Since the computer knows the correct values of the masked words, it can assess the accuracy of the model's predictions. This process helps enhance Bert's bidirectional encoding capabilities.</td>\n",
       "      <td>Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GPT 与 Bert 的区别</td>\n",
       "      <td>GPT是单向模型，只能利用上文信息，而BERT是双向模型，可以同时利用上下文信息。 GPT基于自回归模型，适用于NLU和NLG任务，而BERT基于自编码模型，主要用于NLU任务，无法直接应用在文本生成任务上。</td>\n",
       "      <td>GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。</td>\n",
       "      <td>意思不同。LLM-Answer提到了GPT是单向模型，无法利用上下文信息，只能利用上文；而BERT是双向模型。而Expected-Answer则提到了GPT是基于自回归模型，可以应用在NLU和NLG两大任务，而原生的BERT采用的基于自编码模型，只能完成NLU任务，无法直接应用在文本生成上面。两者虽然都提到了GPT和BERT的区别，但具体描述的重点和细节有所不同。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pca 属于有监督还是无监督</td>\n",
       "      <td>PCA belongs to unsupervised learning.</td>\n",
       "      <td>PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都表明PCA属于无监督学习。虽然表达方式略有不同，但传达的含义是一致的。LLM-Answer指出PCA属于无监督学习，而Expected-Answer则进一步解释了PCA的特点。因此，它们传达的含义是相似的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>介绍 transformer 算法</td>\n",
       "      <td>The algorithm described in the context is related to printing elements of a matrix in a spiral order. It involves moving in a clockwise direction along the matrix borders and adding the elements to a result list. The algorithm tracks the boundaries of the matrix and the current position to determine the movement direction.</td>\n",
       "      <td>Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            question  \\\n",
       "0           介绍下SVM算法   \n",
       "1      用通俗的语言介绍下强化学习   \n",
       "2         BN 和 LN 区别   \n",
       "3   讲讲self attention   \n",
       "4        Bert 的预训练过程   \n",
       "5     GPT 与 Bert 的区别   \n",
       "6     pca 属于有监督还是无监督   \n",
       "7  介绍 transformer 算法   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       llm_answer  \\\n",
       "0  SVM is a binary classification model that defines the maximum margin linear classifier in the feature space, distinguishing it from the perceptron. SVM can be categorized into three types: linear separable SVM for linearly separable training data, linear SVM for data that is not linearly separable but approximately so, and nonlinear SVM for data that is not linearly separable, utilizing kernel tricks and maximizing soft margins. SVM's learning strategy focuses on maximizing the margin, which can be formalized as solving a convex quadratic programming problem or minimizing the regularized hinge loss function. The selection of the kernel function in SVM depends on the data characteristics, with Linear kernel suitable for linearly separable cases and RBF kernel for linearly inseparable cases, although parameter tuning for RBF kernel often involves cross-validation due to its dependency on parameters.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with its environment. It works on a reward system, where the agent receives positive or negative feedback based on its actions. The goal is for the agent to learn the best way to achieve a specific objective by maximizing rewards over time through trial and error.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Batch Normalization (BN) is applied to each batch of training data by normalizing based on the batch's mean and variance, while Layer Normalization (LN) normalizes across the features for each sample. LN does not depend on batch size and is used in scenarios like recurrent neural networks.   \n",
       "3                                          SelfAttention is a mechanism that differs significantly from traditional Attention. While traditional Attention calculates dependencies between hidden states of the source and target ends, SelfAttention operates independently on the source and target ends. It focuses solely on the self-related dependencies within the source or target input, capturing the relationships between words within the source or target end. The self-attention results from the source end are then incorporated into the attention obtained from the target end, capturing dependencies between words in both the source and target ends. This approach is more effective than traditional Attention mechanisms because it considers not only the dependencies between words in the source and target ends but also effectively captures dependencies between words within the source or target end themselves.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                During the pre-training process of Bert, a portion of words in the sentence encoding is masked. This allows the model to predict the masked words based on the surrounding words. Since the computer knows the correct values of the masked words, it can assess the accuracy of the model's predictions. This process helps enhance Bert's bidirectional encoding capabilities.   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        GPT是单向模型，只能利用上文信息，而BERT是双向模型，可以同时利用上下文信息。 GPT基于自回归模型，适用于NLU和NLG任务，而BERT基于自编码模型，主要用于NLU任务，无法直接应用在文本生成任务上。   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           PCA belongs to unsupervised learning.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The algorithm described in the context is related to printing elements of a matrix in a spiral order. It involves moving in a clockwise direction along the matrix borders and adding the elements to a result list. The algorithm tracks the boundaries of the matrix and the current position to determine the movement direction.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          expected_answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机   \n",
       "1                                                                                                                                                               监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略   \n",
       "2                                                                                                                                                                                                                                                                                        Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。   \n",
       "3  Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系   \n",
       "4                                                                                                                                                                                                                                                                                                             Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                               GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。   \n",
       "7                                                                                                                                                                                                                                                                Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。   \n",
       "\n",
       "                                                                                                                                                                          rag_eval_results  \n",
       "0                                                                                                                  意思一致。LLM-Answer和Expected-Answer都在介绍SVM算法的基本原理和特点，虽然表达方式略有不同，但传达的含义是一致的。  \n",
       "1                                                                                                                                                                                     意思不同  \n",
       "2                                                                                                                                                                                     意思一致  \n",
       "3                                                                       意思一致。LLM-Answer和Expected-Answer都在讲述Self Attention与传统Attention机制的区别，以及Self Attention的工作原理和优势。虽然表达方式有所不同，但传达的含义是一致的。  \n",
       "4                                                                                                                                                                                     意思不同  \n",
       "5  意思不同。LLM-Answer提到了GPT是单向模型，无法利用上下文信息，只能利用上文；而BERT是双向模型。而Expected-Answer则提到了GPT是基于自回归模型，可以应用在NLU和NLG两大任务，而原生的BERT采用的基于自编码模型，只能完成NLU任务，无法直接应用在文本生成上面。两者虽然都提到了GPT和BERT的区别，但具体描述的重点和细节有所不同。  \n",
       "6                                                  意思一致。LLM-Answer和Expected-Answer都表明PCA属于无监督学习。虽然表达方式略有不同，但传达的含义是一致的。LLM-Answer指出PCA属于无监督学习，而Expected-Answer则进一步解释了PCA的特点。因此，它们传达的含义是相似的。  \n",
       "7                                                                                                                                                                                     意思不同  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eval llm-answer with expected-answer\n",
    "answers_df['rag_eval_results'] = answers_df.apply(lambda r: eval_llm_output(r, llm), axis=1)\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(answers_df)\n",
    "answers_df.to_excel('sentence_window_retriever_pack_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6671e0",
   "metadata": {},
   "source": [
    "参考链接：https://docs.llamaindex.ai/en/v0.10.33/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
