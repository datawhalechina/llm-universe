{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c6746ac0",
      "metadata": {},
      "source": [
        "# 五、生成\n",
        "\n",
        "本章介绍 RAG 系统中**生成阶段**的关键优化技术，包括后处理（信息压缩、重排序）和参考引用。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55602023",
      "metadata": {},
      "source": [
        "## 5.1 后处理\n",
        "\n",
        "后处理是 RAG 系统中的一个关键环节，通常发生在检索阶段之后，旨在优化和提升检索和生成结果的质量和相关性。\n",
        "\n",
        "<img src=\"./postprocess.png\" width=\"680px\">\n",
        "\n",
        "- Embedding Model : 将文档段落编码为向量的嵌入模型。\n",
        "- Retriever：通过嵌入模型编码用户问题 query，并返回嵌入问题附近的任意编码文档 documents。\n",
        "- 后处理（可选）: Compressor 提取关键信息；ReRanker 根据某规则重新计算 query 与 documents 间的得分。\n",
        "- Language Model : 语言模型用于接收来自检索器或重排器的记录以及问题，并返回答案。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f36ffdc",
      "metadata": {},
      "source": [
        "**为什么进行后处理？**\n",
        "\n",
        "1. **优化信息密度**：由于 LLM 对输入的字数有限制，后处理可以通过筛选和压缩信息从大量检索结果中提炼出最核心的信息。这样可以确保即使在字数限制内，也能向模型提供最相关和最有价值的数据，从而生成高质量和高密度的信息内容。\n",
        "\n",
        "1. **提高生成效率**：在有限的字数下，模型需要处理的信息越精炼，其生成答案的效率就越高。后处理通过去除冗余和不相关信息，确保模型专注于最关键的内容，这样可以在有限的交互中快速生成准确和有用的回答。\n",
        "\n",
        "1. **确保内容的连贯性**：在字数受限的情况下，模型可能无法一次性接收并处理所有相关信息。后处理可以帮助组织和结构化信息，使其在生成阶段能够以连贯的方式呈现，避免因字数限制而导致的信息断层或不完整。\n",
        "\n",
        "1. **提升生成文本的质量**：后处理不仅可以优化输入数据，还可以通过调整生成策略来提升输出文本的质量。例如，通过设置优先级，确保最重要的信息首先被包含在生成的文本中，即使在字数限制的情况下也能保证回答的核心价值。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ab49b8c",
      "metadata": {},
      "source": [
        "后处理通常可包括以下方面：\n",
        "\n",
        "1. **信息压缩**：由于检索阶段可能会返回大量相关信息，为了提高生成阶段的效率和性能，可对大量信息进行压缩，提取最关键的内容。这可能涉及到提取关键句子、短语或概念，以便在生成答案时能够集中于最相关的信息。\n",
        "\n",
        "1. **重新排序**：根据与用户查询的相关性对检索结果进行重新排序。这通常基于文档与查询的匹配程度、文档的权威性、用户的历史偏好等因素。通过这种方式，最相关和最有用的信息会被放在最前面，以供生成模型优先考虑。\n",
        "\n",
        "通过这些后检索处理步骤，RAG 系统能够确保生成阶段的输入是精炼、相关且高质量的，从而提高最终生成文本的准确性和用户满意度。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d0789cc",
      "metadata": {},
      "source": [
        "### 5.1.1 信息压缩\n",
        "\n",
        "**为什么进行信息压缩？**\n",
        "\n",
        "信息压缩通常指的是减少数据量而不丢失关键信息的过程。以往的方法在整合 LLMs 到检索式问答框架中存在一定的局限性，如计算成本高、对长文本的处理不足等。为了提高生成阶段的效率和性能，可对检索阶段返回的大量信息进行压缩，提取最关键的内容，以便在生成答案时能够集中于最相关的信息。\n",
        "\n",
        "**信息压缩**\n",
        "\n",
        "信息压缩可以对于 retriever 返回的内容进行压缩即上下文压缩（contextual compression）；也可以对于已组成的结构化 prompt 信息进行压缩即提示压缩（prompt compression）。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "731119e0",
      "metadata": {},
      "source": [
        "1）**上下文压缩（Contextual Compression）案例**\n",
        "- 上下文压缩：使用给定查询的上下文对文档进行压缩，从而只返回相关信息，而不是立即按原样返回检索到的文档。这里的 \"压缩 \"既指压缩单个文档的内容，也指整体过滤掉文档。可以借助 langchain 包中的 [ContextualCompressionRetriever](https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.contextual_compression.ContextualCompressionRetriever.html) 实现此功能。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76cff42e",
      "metadata": {},
      "source": [
        "**1. 原始检索效果展示**\n",
        "\n",
        "为了演示 RAG 系统中上下文压缩的必要性，我们使用南瓜书《机器学习公式详解》作为示例。这本书是《西瓜书》（周志华教授的《机器学习》）的配套讲解，涵盖了机器学习中各类算法的数学公式推导和详解。\n",
        "\n",
        "我们将文档分割成多个文档块，然后进行检索。通过对比压缩前后的检索结果，展示上下文压缩如何过滤掉不相关的文档块，提高检索质量。\n",
        "\n",
        "> **数据说明**：本教程使用与「4. 检索阶段」相同的数据集，保持教程连贯性。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bd57ec8",
      "metadata": {},
      "source": [
        "#### 环境准备\n",
        "\n",
        "在开始之前，请确保安装以下依赖：\n",
        "\n",
        "```bash\n",
        "pip install langchain langchain-community langchain-core\n",
        "pip install chromadb pymupdf\n",
        "pip install sentence-transformers  # 用于重排序模型\n",
        "pip install llmlingua   # 用于提示压缩\n",
        "pip install modelscope  # 用于国内下载模型\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad9a51b6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 环境准备完成\n"
          ]
        }
      ],
      "source": [
        "# 导入必要的库\n",
        "import os\n",
        "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
        "import re\n",
        "import json\n",
        "from langchain_community.embeddings import ZhipuAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.document_loaders.pdf import PyMuPDFLoader\n",
        "from langchain_community.chat_models import ChatZhipuAI\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from dotenv import load_dotenv\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# 加载环境变量\n",
        "load_dotenv()\n",
        "\n",
        "# 检查 API Key 是否已设置\n",
        "api_key = os.environ.get(\"ZHIPUAI_API_KEY\")\n",
        "llm = ChatZhipuAI(\n",
        "    model=\"glm-4-flash\", \n",
        "    temperature=0.0, \n",
        "    timeout=1200, \n",
        "    max_retries=3, \n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "print(\"✅ 环境准备完成\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0b38a5d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Embedding模型初始化成功\n",
            "✅ 加载了 119 个问答对\n",
            "正在初始化向量数据库...\n",
            "发现已存在的向量数据库: ./chroma_db，正在加载...\n",
            "✅ 加载成功！库中包含 170 个文档块。\n",
            "==================== 原始检索结果 (Top 5) ====================\n",
            "Document 1 [页码:51]:\n",
            "式(4.7)可知，此时i依次取1到16，那么“密度”这个属性的候选划分点集合为Ta={0.243+0.2452,0.245+0.3432,0.343+0.3602,0.360+0.4032,0.403+0.4372,0.437+0.4812,0.481+0.5562,0.556+0.5932,0.5...\n",
            "\n",
            "Document 2 [页码:49]:\n",
            "Gini_index(D,敲声=浊响)=0.450Gini_index(D,敲声=沉闷)=0.494Gini_index(D,敲声=清脆)=0.439Gini_index(D,纹理=清晰)=0.286Gini_index(D,纹理=稍稀)=0.437Gini_index(D,纹理=模糊)=0.403...\n",
            "\n",
            "Document 3 [页码:50]:\n",
            "(2)遍历所有属性，找到最优划分属性a∗，然后根据a∗的最优划分点v∗将特征空间划分为两个子空间，接着对每个子空间重复上述步骤，直至满足停止条件.这样就生成了一棵CART回归树，假设最终将特征空间划分为M个子空间R1,R2,···,RM，那么CART回归树的模型式可以表示为f(x)=MXm=1cmI...\n",
            "\n",
            "Document 4 [页码:52]:\n",
            "经过四次划分已无空白部分，表示决策树生成完毕，从图4-2(d)中可以清晰地看出好瓜与坏瓜的分类边界。含糖率密度0.60.40.20.20.40.60.80(a)第一次划分含糖率密度0.60.40.20.20.40.60.80(b)第二次划分含糖率密度0.60.40.20.20.40.60.80(c)...\n",
            "\n",
            "Document 5 [页码:48]:\n",
            "易证以上两式之和等于1，证明过程如下|Y|=3Xk=1p2k+|Y|=3Xk=1Xk′̸=kpkpk′=(p1p1+p2p2+p3p3)+(p1p2+p1p3+p2p1+p2p3+p3p1+p3p2)=(p1p1+p1p2+p1p3)+(p2p1+p2p2+p2p3)+(p3p1+p3p2+p3p3...\n",
            "\n",
            ">>> 原始上下文总长度: 5461 字符\n"
          ]
        }
      ],
      "source": [
        "# ==================== 数据准备（与检索阶段保持一致）====================\n",
        "# 使用与 \"4. 检索阶段\" 相同的南瓜书数据集，确保教程连贯性\n",
        "\n",
        "# 数据路径（相对于当前notebook所在目录）\n",
        "pdf_path = \"../3. 索引阶段/data/pumpkin_book.pdf\"\n",
        "qa_path = '../3. 索引阶段/data/train_dataset.json'\n",
        "\n",
        "# 初始化 Embedding 模型\n",
        "embedding = ZhipuAIEmbeddings(\n",
        "    api_key=os.environ.get('ZHIPUAI_API_KEY'),\n",
        "    model=\"embedding-3\"\n",
        ")\n",
        "print(\"✅ Embedding模型初始化成功\")\n",
        "\n",
        "# 加载问答数据集\n",
        "with open(qa_path, 'r', encoding='utf-8') as file:\n",
        "    qa_pairs = json.load(file)\n",
        "print(f\"✅ 加载了 {len(qa_pairs)} 个问答对\")\n",
        "\n",
        "# ---------- 向量库构建函数（与检索阶段保持一致）----------\n",
        "def clean_text(text: str):\n",
        "    \"\"\"文本清理函数\"\"\"\n",
        "    text = re.sub(r'→_→\\n欢迎去各大电商平台选购纸质版南瓜书《机器学习公式详解》\\n←_←', '', text)\n",
        "    text = re.sub(r'→_→\\n配套视频教程：https://www.bilibili.com/video/BV1Mh411e7VU\\n←_←', '', text)\n",
        "    text = re.sub(r'\\s+', '', text)\n",
        "    text = re.sub(r'\\n+', '', text)\n",
        "    return text\n",
        "\n",
        "def build_vectorstore(pdf_path, embedding, chunk_size=2000, chunk_overlap=100, persist_directory=\"./chroma_db\"):\n",
        "    \"\"\"构建或加载向量数据库\"\"\"\n",
        "    if os.path.exists(persist_directory) and os.listdir(persist_directory):\n",
        "        print(f\"发现已存在的向量数据库: {persist_directory}，正在加载...\")\n",
        "        try:\n",
        "            vectorstore = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
        "            count = vectorstore._collection.count() \n",
        "            print(f\"✅ 加载成功！库中包含 {count} 个文档块。\")\n",
        "            return vectorstore\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ 加载失败 ({e})，将重新构建...\")\n",
        "    \n",
        "    print(\"未发现现有向量库或加载失败，开始重新构建...\")\n",
        "    loader = PyMuPDFLoader(pdf_path)\n",
        "    pdf_pages = loader.load()\n",
        "    data_pages = pdf_pages[13:-13]  # 去掉前后的目录和附录页\n",
        "    for page in data_pages:\n",
        "        page.page_content = clean_text(page.page_content)\n",
        "    \n",
        "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, separator='')\n",
        "    split_docs = text_splitter.split_documents(data_pages)\n",
        "    for split_doc in tqdm(split_docs, desc=\"处理文档\"):\n",
        "        split_doc.metadata['chunk_size'] = chunk_size\n",
        "        split_doc.metadata['chunk_overlap'] = chunk_overlap\n",
        "    \n",
        "    batch_size = 16\n",
        "    vectorstore = None\n",
        "    print(f\"共{len(split_docs)}个文档块，将分{(len(split_docs) + batch_size - 1) // batch_size}批处理...\")\n",
        "    \n",
        "    for i in tqdm(range(0, len(split_docs), batch_size), desc=\"批量构建向量库\"):\n",
        "        batch = split_docs[i:i+batch_size]\n",
        "        if vectorstore is None:\n",
        "            vectorstore = Chroma.from_documents(documents=batch, embedding=embedding, persist_directory=persist_directory)\n",
        "        else:\n",
        "            vectorstore.add_documents(documents=batch)\n",
        "    \n",
        "    print(f\"✅ 向量数据库构建完成并保存至 {persist_directory}！\")\n",
        "    return vectorstore\n",
        "\n",
        "# 构建向量库\n",
        "print(\"正在初始化向量数据库...\")\n",
        "vectorstore = build_vectorstore(pdf_path, embedding)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "# ---------- 辅助函数 ----------\n",
        "def print_docs(docs, title=\"检索结果\"):\n",
        "    \"\"\"打印检索结果\"\"\"\n",
        "    print(f\"{'='*20} {title} {'='*20}\")\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        page = doc.metadata.get('page', 'N/A')\n",
        "        print(f\"Document {i} [页码:{page}]:\")\n",
        "        print(doc.page_content[:150] + \"...\" if len(doc.page_content) > 150 else doc.page_content)\n",
        "        print()\n",
        "\n",
        "# 执行原始检索\n",
        "query = \"在决策树基于“密度”属性进行划分时，南瓜书计算的候选划分点集合 Ta 包含了哪些具体数值？\"\n",
        "original_docs = retriever.invoke(query)\n",
        "\n",
        "print_docs(original_docs, \"原始检索结果 (Top 5)\")\n",
        "\n",
        "# 计算原始上下文总长度\n",
        "original_length = sum(len(doc.page_content) for doc in original_docs)\n",
        "print(f\">>> 原始上下文总长度: {original_length} 字符\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f4e48c1",
      "metadata": {},
      "source": [
        "**2. 实施上下文压缩**\n",
        "\n",
        "从上面的原始检索结果可以看到，返回了 5 个文档块，但并非所有文档块都与查询高度相关。有些文档块可能只是提及了某些相关关键词，但并不能真正回答问题。\n",
        "\n",
        "如果不进行压缩，这些噪音会：\n",
        "\n",
        "1.  **浪费 Token**：不仅增加了成本，还占用了宝贵的上下文窗口。\n",
        "2.  **干扰回答**：过多不相关的信息可能导致模型生成偏离主题的回答。\n",
        "\n",
        "现在我们应用 `ContextualCompressionRetriever` 来过滤掉相关性低的文档块。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "5c36c627",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: 在决策树基于“密度”属性进行划分时，南瓜书计算的候选划分点集合 Ta 包含了哪些具体数值？\n",
            "==================== 压缩后检索结果 ====================\n",
            "Document 1 [页码:51]:\n",
            "式(4.7)可知，此时i依次取1到16，那么“密度”这个属性的候选划分点集合为Ta={0.243+0.2452,0.245+0.3432,0.343+0.3602,0.360+0.4032,0.403+0.4372,0.437+0.4812,0.481+0.5562,0.556+0.5932,0.5...\n",
            "\n",
            "\n",
            ">>> 压缩后上下文总长度: 1454 字符\n",
            ">>> 压缩率: 73.4%\n"
          ]
        }
      ],
      "source": [
        "# 1. 定义过滤器\n",
        "# similarity_threshold 是核心参数，设定为 0.6 意味着过滤掉相关性低于 0.6 的文档\n",
        "embeddings_filter = EmbeddingsFilter(embeddings=embedding, similarity_threshold=0.6)\n",
        "\n",
        "# 2. 构建压缩检索器\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=embeddings_filter,  # 使用 EmbeddingsFilter 作为压缩器\n",
        "    base_retriever=retriever  # 使用原始检索器\n",
        ")\n",
        "\n",
        "# 3. 执行压缩检索\n",
        "print(f\"Query: {query}\")\n",
        "compressed_docs = compression_retriever.invoke(query)\n",
        "print_docs(compressed_docs, \"压缩后检索结果\")\n",
        "\n",
        "# 计算压缩后上下文总长度\n",
        "compressed_length = sum(len(doc.page_content) for doc in compressed_docs)\n",
        "print(f\"\\n>>> 压缩后上下文总长度: {compressed_length} 字符\")\n",
        "print(f\">>> 压缩率: {(1 - compressed_length/original_length)*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab0f0a46",
      "metadata": {},
      "source": [
        "可以看出压缩后，检索器只返回了与查询高度相关的文档块，过滤掉了相关性较低的内容。这不仅减少了 Token 消耗，还提高了后续生成的质量。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcab4c84",
      "metadata": {},
      "source": [
        "**3. 生成效果对比**\n",
        "\n",
        "接下来，我们对比使用原始上下文和压缩后上下文进行生成的效果差异。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "c627f285",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: 在决策树基于“密度”属性进行划分时，南瓜书计算的候选划分点集合 Ta 包含了哪些具体数值？\n",
            "\n",
            "==================== 基于 [原始] 上下文的回答 ====================\n",
            "在决策树基于“密度”属性进行划分时，南瓜书计算的候选划分点集合 Ta 包含以下具体数值：\n",
            "\n",
            "0.243 + 0.2452\n",
            "0.245 + 0.3432\n",
            "0.343 + 0.3602\n",
            "0.360 + 0.4032\n",
            "0.403 + 0.4372\n",
            "0.437 + 0.4812\n",
            "0.481 + 0.5562\n",
            "0.556 + 0.5932\n",
            "0.593 + 0.6082\n",
            "0.608 + 0.6342\n",
            "0.634 + 0.6392\n",
            "0.639 + 0.6572\n",
            "0.657 + 0.6662\n",
            "0.666 + 0.6972\n",
            "0.697 + 0.7192\n",
            "0.719 + 0.7742\n",
            "\n",
            "==================== 基于 [压缩后] 上下文的回答 ====================\n",
            "南瓜书计算的候选划分点集合 Ta 包含了以下具体数值：\n",
            "0.243+0.2452, 0.245+0.3432, 0.343+0.3602, 0.360+0.4032, 0.403+0.4372, 0.437+0.4812, 0.481+0.5562, 0.556+0.5932, 0.593+0.6082, 0.608+0.6342, 0.634+0.6392, 0.639+0.6572, 0.657+0.6662, 0.666+0.6972, 0.697+0.7192, 0.719+0.7742。\n"
          ]
        }
      ],
      "source": [
        "# 定义生成链\n",
        "template = \"\"\"仅根据以下背景回答问题。如果背景中没有答案，请回答不知道。\n",
        "背景信息：\n",
        "{context}\n",
        "\n",
        "问题: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "\n",
        "chain = (\n",
        "    {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(f\"Query: {query}\")\n",
        "\n",
        "# 1. 使用原始文档生成\n",
        "print(\"\\n\" + \"=\" * 20 + \" 基于 [原始] 上下文的回答 \" + \"=\" * 20)\n",
        "try:\n",
        "    original_response = chain.invoke({\"context\": format_docs(original_docs), \"question\": query})\n",
        "    print(original_response)\n",
        "except Exception as e:\n",
        "    print(f\"生成失败: {e}\")\n",
        "\n",
        "# 2. 使用压缩后的文档生成\n",
        "print(\"\\n\" + \"=\" * 20 + \" 基于 [压缩后] 上下文的回答 \" + \"=\" * 20)\n",
        "try:\n",
        "    compressed_response = chain.invoke({\"context\": format_docs(compressed_docs), \"question\": query})\n",
        "    print(compressed_response)\n",
        "except Exception as e:\n",
        "    print(f\"生成失败: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11eb45bd",
      "metadata": {},
      "source": [
        "通过上下文压缩，我们显著减少了输入 Token 数量，同时大模型依然能够生成准确的回答。这在实际应用中可以有效降低 API 调用成本并提升响应速度。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb503df4",
      "metadata": {},
      "source": [
        "2）**提示压缩（Prompt Compression）案例**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "333daad4",
      "metadata": {},
      "source": [
        "- 提示压缩：一种从粗到细的提示语压缩策略，作为 Token 预算控制器，它能在高压缩率下保持语义完整性。这里我们使用 [LLMLingua](https://github.com/microsoft/LLMLingua) 工具包，这是一种轻量且强大的提示压缩方法。它利用从 GPT-4 蒸馏的数据训练 BERT 级编码器进行 token 分类，在任务无关的压缩场景中表现出色，能有效识别并剔除 Prompt 中的冗余 token。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "a57f40c8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading Model from https://www.modelscope.cn to directory: /Users/zhihu123/.cache/modelscope/hub/models/microsoft/llmlingua-2-xlm-roberta-large-meetingbank\n"
          ]
        }
      ],
      "source": [
        "# 我们先使用 ModelScope 下载模型需要用到的模型\n",
        "llmlingua_model_dir = snapshot_download('microsoft/llmlingua-2-xlm-roberta-large-meetingbank')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "d7a66642",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "原始 Prompt 长度: 5511 字符\n",
            "\n",
            ">>> 原始回答:\n",
            "根据您提供的文本内容，式(4.7)用于计算连续属性“密度”的候选划分点集合 Ta。该集合是通过取每两个相邻取值的中点来生成的。以下是根据文本内容计算出的 Ta 集合中的具体数值：\n",
            "\n",
            "1. 0.243 + 0.245 / 2 = 0.244\n",
            "2. 0.245 + 0.343 / 2 = 0.279\n",
            "3. 0.343 + 0.360 / 2 = 0.357\n",
            "4. 0.360 + 0.403 / 2 = 0.381\n",
            "5. 0.403 + 0.437 / 2 = 0.415\n",
            "6. 0.437 + 0.481 / 2 = 0.454\n",
            "7. 0.481 + 0.556 / 2 = 0.518\n",
            "8. 0.556 + 0.593 / 2 = 0.574\n",
            "9. 0.593 + 0.608 / 2 = 0.601\n",
            "10. 0.608 + 0.634 / 2 = 0.621\n",
            "11. 0.634 + 0.639 / 2 = 0.636\n",
            "12. 0.639 + 0.657 / 2 = 0.647\n",
            "13. 0.657 + 0.666 / 2 = 0.663\n",
            "14. 0.666 + 0.697 / 2 = 0.672\n",
            "15. 0.697 + 0.719 / 2 = 0.703\n",
            "16. 0.719 + 0.774 / 2 = 0.748\n",
            "\n",
            "因此，候选划分点集合 Ta 包含以下具体数值：\n",
            "\n",
            "{0.244, 0.279, 0.357, 0.381, 0.415, 0.454, 0.518, 0.574, 0.601, 0.621, 0.636, 0.647, 0.663, 0.672, 0.703, 0.748}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1057 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "压缩后 Prompt 长度: 3727 字符\n",
            "\n",
            ">>> 压缩后回答:\n",
            "根据您提供的信息，在决策树基于“密度”属性进行划分时，候选划分点集合 \\( T_a \\) 包含了以下具体数值：\n",
            "\n",
            "{0.243, 0.245, 0.343, 0.360, 0.403, 0.437, 0.481, 0.556, 0.593, 0.608, 0.634, 0.657, 0.666, 0.697, 0.719, 0.774}\n",
            "\n",
            "这些数值是“密度”属性在数据集中已观测到的可能取值。在计算候选划分点时，通常会使用这些数值的中点作为候选划分点，但根据您提供的信息，这里直接列出了所有已观测到的取值。\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from llmlingua import PromptCompressor\n",
        "from modelscope import snapshot_download\n",
        "\n",
        "# 1. 使用检索到的南瓜书文档作为上下文\n",
        "# question = \"什么是决策树？它的基本原理是什么？\"\n",
        "question = \"在决策树基于“密度”属性进行划分时，南瓜书计算的候选划分点集合 Ta 包含了哪些具体数值？\"\n",
        "\n",
        "# 使用之前检索到的压缩后文档作为上下文\n",
        "context = [doc.page_content for doc in original_docs]\n",
        "\n",
        "# 2. 测试原始 Prompt 效果\n",
        "original_prompt = \"\\n\".join(context) + \"\\n\" + question\n",
        "print(f\"原始 Prompt 长度: {len(original_prompt)} 字符\")\n",
        "response_original = llm.invoke(original_prompt)\n",
        "print(f\"\\n>>> 原始回答:\\n{response_original.content}\")\n",
        "\n",
        "# 3. 初始化压缩模型\n",
        "\n",
        "# 初始化压缩器\n",
        "llm_lingua = PromptCompressor(\n",
        "    model_name=llmlingua_model_dir,\n",
        "    use_llmlingua2=True,\n",
        "    device_map=\"cpu\"\n",
        ")\n",
        "\n",
        "# 4. 执行压缩\n",
        "compressed_result = llm_lingua.compress_prompt(\n",
        "    context,\n",
        "    instruction=\"\",\n",
        "    question=question,\n",
        "    rate=0.7,  # 压缩到原始的 30%\n",
        "    force_tokens=[\"\\n\", \"。\", \"，\", \"：\"]\n",
        ")\n",
        "compressed_prompt_str = compressed_result['compressed_prompt']\n",
        "final_prompt = f\"{compressed_prompt_str}\\n\\n基于以上信息，回答问题：{question}\"\n",
        "\n",
        "print(f\"压缩后 Prompt 长度: {len(final_prompt)} 字符\")\n",
        "\n",
        "response_compressed = llm.invoke(final_prompt)\n",
        "print(f\"\\n>>> 压缩后回答:\\n{response_compressed.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86e0fcc0",
      "metadata": {},
      "source": [
        "`compress_prompt` 返回的信息中包含了压缩后的 prompt 以及压缩的统计信息。可以看出它节省了大量的 Token，同时保持了语义的完整性。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "4dddd3e9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"compressed_prompt\": \"，，={0.243+0.2452,0.245+0.3432,0.343+0.3602,0.360+0.4032,0.403+0.4372,0.437+0.4812,0.481+0.5562,0.556+0.5932,0.593+0.6082,0.608+0.6342,0.634+0.6392,0.639+0.6572,0.657+0.6662,0.666+0.6972,0.697+0.7192,0.719+0.7742}4.4.2式(4(4.2，， λ∈{−,+，=−时有Dλt=Datt，=+时有Dλt=Da>tt。 4.4.3式(4(4.2，(4.2|Dv||改为式(4.11)的 ̃rv，(4.1.10)的 ̃pk(4.9)的ρ。(4.9)(4.10)(4.11)中的权重wx， 初始化为1。，， 除编号为8、10的两个样本在此属性缺失之外，， 而编号为8、10的两个样本则要按比例同时划入三个子集。， 稍糊子集包含样本7、9、13、14、17共5个样本，，，， 而此时各样本的权重wx初始化为1， 因此编号为8、10的两个样本分到稍糊、清晰、模糊三个子集的权重分别为515,715和315。 4.5多变量决策树本节内容也通俗易懂，。 4.5.1图(4， 离散属性不可以重复使用，。 4.5.2图(4.11)的解释对照“西瓜书”中图4.10的决策树， 下面给出图4.11中的划分边界产出过程。 在下图4-2中， 斜纹阴影部分表示已确定标记为坏瓜的样本， 点状阴影部分表示已确定标记为好瓜的样本， 空白部分表示需要进一步划分的样本。 第一次划分条件是“含糖率0.126?”，(a，(a。(a)空白部分继续进行划分， 第二次划分条件是“密度0.381?”，(b)新增斜纹阴影部分所示，(b。(b)空白部分继续进行划分， 第三次划分条件是“含糖率0.205?”， 不满足此条件的样本直接标记为好瓜(c，(c。 在第三次划分的基础上对图4-2(c)空白部分继续进行划分， 第四次划分的条件是“密度0.560?”， 满足此条件的样本直接标记为好瓜(d)新增点状阴影部分所示， 而不满足此条件的样本直接标记为坏瓜(d。\\n\\n_index(D,敲声=浊响)=0(D,敲声=沉闷)=0=清脆)=0(D,纹理=清晰),纹理=稍稀)=0=模糊)=0.403Gini(D,脐部=凹陷),脐部=稍凹)=0=平坦)(D,触感=硬挺)=0=软粘)=0.， 对于属性“触感”，，。， Gini_index(D,纹理=清晰)=0.286最小， 所以选择属性“纹理”为最优划分属性并生成根节点，=清晰”为最优划分点生成D1)、D2)两个子节点，， 直至满足停止条件。 以上便是CART决策树的构建过程，， CART决策树最终构造出来的是一棵二叉树。， 回归树还可以处理回归问题，。 假设给定数据集D={(x1,y1),(x2,y2),···(xN,yN)}其中x∈Rd为d维特征向量， y∈R是连续型随机变量。，， 而每个d维特征向量x就对应了d维的特征空间中的一个数据点。 CART回归树的目标是将特征空间划分成若干个子空间，，，，。。 (1)任意选择一个属性a， 遍历其所有可能取值， 根据下式找出属性a最优划分点v∗:v∗=argminvminc1Xxi∈R1(a,v)(yi−c1)2+minc2Xxi∈R2(a,v)(yi−c2)2其中， R1(a,v)={x|x∈Dav},R2(a,v)={x|x， c1和c2分别为集合R1，=ave(yi|x∈R1)=1|Xxi∈R1)yic2=ave(yi|x∈R2)=1|Xxi∈R2\\n\\n(2)遍历所有属性， 找到最优划分属性a∗，，， 直至满足停止条件， 假设最终将特征空间划分为M个子空间R1,R2，(x)=MXm=1cmI(x∈Rm， 其中的cm表示的也是集合Rm中的样本xi对应的输出值yi的均值。，，，。 4.3剪枝处理本节内容通俗易懂，，。。 图4.5与图4.4均是基于信息增益生成的决策树， 不同在于图4.4基于表4.1， 而图4.5基于表4.2的训练集。 结点3包含训练集“脐部”为稍凹的样本(编号6、7、15、17)，，(编号6、7、15、17)含两个好瓜和两个坏瓜， 因此叶结点硬挺的类别随机从类别好瓜和坏瓜中选择其一。 结点5包含训练集“脐部”为稍凹且“根蒂”为稍蜷的样本(编号6、7、15)，， 因此叶结点浅白类别标记为好瓜(编号6、7、15样本中，，。 结点6包含训练集“脐部”为稍凹、“根蒂”为稍蜷、“色泽”为乌黑的样本(编号7、15)， 当根据“纹理”再次进行划分时不含有“纹理”为模糊的样本)，(编号、15)含好瓜和坏瓜各一个，。 图4.5两次随机选择均选为好瓜，(参见第1章1.4节)。。 有些分类器只能使用离散属性，， 有兴趣可以通过关键词“连续属性离散化”或者“Discretization”查阅更多处理方法。 结合第11章11.2节至11.4节分别介绍的“过滤式”算法、“包裹式”算法、“嵌入式”算法的概念， 若先使用某个离散化算法对连续属性离散化后再调用C4.5决策树生成算法，，.4.1节所述，，，，。， 有些分类器不能使用含有缺失值的样本，。 常用的缺失值填充方法是，;对于离散属性，。。， 一般缺失值仅指样本的属性值有缺失， 若类别标记有缺失，。， 也可以尝试根据第11章11.6节的式(11.24)， 在低秩假设下对数据集缺失值进行填充。 4.4.1式(4， 就是以每两个相邻取值的中点作为划分点。 下面以“西瓜书”中表4.3中西瓜数据集3.0为例来说明此式的用法。 对于“密度”这个连续属性， 已观测到的可能取值为{0.243,0.245,0.343,0.360,0.403,0.,0.556,0.593,0.608,0.657,0.666,0.697,0.719,0.774，\\n\\n，|Y|=3Xk=1p2k|=3Xk=1Xk′=kpkpk′=(p1p1+p2p2+p3p3)+(p1p2+p1p3+p2p1+p2p3+p3p1+p3p2)=(p1p1+p1p2+p1p3)+(p2p1+p2p2+p2p3)+(p3p1+p3p2+p3p3)=p1(p1+p2+p3)+p2(p1+p2+p3)+p3(p1+p2+p3)=p1+p2(4.5)Gini(D)=|Y|Xk=1Xk′=kpkpk′=1−|Xk=1p2k从数据集中D任取两个样本， 类别标记一致的概率越大表示其纯度越高，。 4.2.5式(4.6)的解释此为数据集D中属性a的基尼指数的定义，， 数据集D按照属性a的所有可能取值划分后的纯度。，，，。:(1)考虑每个属性a的每个可能取值v，=v和a=v两部分来计算基尼指数，_index(D,a)=|Da=v||D|Gini(Da=v)+=v)(2)选择基尼指数最小的属性及其对应取值作为， 直至满足停止条件。 下面以“西瓜书”中表4.2中西瓜数据集2.0为例来构造CART决策树，:以属性“色泽”为例，{青绿， 乌黑， 浅白}，，，(色泽=青绿),D2(色泽=青绿)。 子集D1包含编号{1,4,6,10,13,17}共6个样例，=36， 反例占p2=36;子集D2包含编号{2,3,5,7,16}共11个样例， 其中正例占p1=511， 反例占p2=611， 根据式(4.5=青绿”划分之后得到基尼指数为Gini_index(D,色泽=青绿)=617×1−362−362!+1117×1−5112−6112!=0，_index(D,色泽=乌黑)=0=浅白)=0=蜷缩=稍蜷)=0=硬挺)=0.439\",\n",
            "  \"compressed_prompt_list\": [\n",
            "    \"，，={0.243+0.2452,0.245+0.3432,0.343+0.3602,0.360+0.4032,0.403+0.4372,0.437+0.4812,0.481+0.5562,0.556+0.5932,0.593+0.6082,0.608+0.6342,0.634+0.6392,0.639+0.6572,0.657+0.6662,0.666+0.6972,0.697+0.7192,0.719+0.7742}4.4.2式(4(4.2，， λ∈{−,+，=−时有Dλt=Datt，=+时有Dλt=Da>tt。 4.4.3式(4(4.2，(4.2|Dv||改为式(4.11)的 ̃rv，(4.1.10)的 ̃pk(4.9)的ρ。(4.9)(4.10)(4.11)中的权重wx， 初始化为1。，， 除编号为8、10的两个样本在此属性缺失之外，， 而编号为8、10的两个样本则要按比例同时划入三个子集。， 稍糊子集包含样本7、9、13、14、17共5个样本，，，， 而此时各样本的权重wx初始化为1， 因此编号为8、10的两个样本分到稍糊、清晰、模糊三个子集的权重分别为515,715和315。 4.5多变量决策树本节内容也通俗易懂，。 4.5.1图(4， 离散属性不可以重复使用，。 4.5.2图(4.11)的解释对照“西瓜书”中图4.10的决策树， 下面给出图4.11中的划分边界产出过程。 在下图4-2中， 斜纹阴影部分表示已确定标记为坏瓜的样本， 点状阴影部分表示已确定标记为好瓜的样本， 空白部分表示需要进一步划分的样本。 第一次划分条件是“含糖率0.126?”，(a，(a。(a)空白部分继续进行划分， 第二次划分条件是“密度0.381?”，(b)新增斜纹阴影部分所示，(b。(b)空白部分继续进行划分， 第三次划分条件是“含糖率0.205?”， 不满足此条件的样本直接标记为好瓜(c，(c。 在第三次划分的基础上对图4-2(c)空白部分继续进行划分， 第四次划分的条件是“密度0.560?”， 满足此条件的样本直接标记为好瓜(d)新增点状阴影部分所示， 而不满足此条件的样本直接标记为坏瓜(d。\",\n",
            "    \"_index(D,敲声=浊响)=0(D,敲声=沉闷)=0=清脆)=0(D,纹理=清晰),纹理=稍稀)=0=模糊)=0.403Gini(D,脐部=凹陷),脐部=稍凹)=0=平坦)(D,触感=硬挺)=0=软粘)=0.， 对于属性“触感”，，。， Gini_index(D,纹理=清晰)=0.286最小， 所以选择属性“纹理”为最优划分属性并生成根节点，=清晰”为最优划分点生成D1)、D2)两个子节点，， 直至满足停止条件。 以上便是CART决策树的构建过程，， CART决策树最终构造出来的是一棵二叉树。， 回归树还可以处理回归问题，。 假设给定数据集D={(x1,y1),(x2,y2),···(xN,yN)}其中x∈Rd为d维特征向量， y∈R是连续型随机变量。，， 而每个d维特征向量x就对应了d维的特征空间中的一个数据点。 CART回归树的目标是将特征空间划分成若干个子空间，，，，。。 (1)任意选择一个属性a， 遍历其所有可能取值， 根据下式找出属性a最优划分点v∗:v∗=argminvminc1Xxi∈R1(a,v)(yi−c1)2+minc2Xxi∈R2(a,v)(yi−c2)2其中， R1(a,v)={x|x∈Dav},R2(a,v)={x|x， c1和c2分别为集合R1，=ave(yi|x∈R1)=1|Xxi∈R1)yic2=ave(yi|x∈R2)=1|Xxi∈R2\",\n",
            "    \"(2)遍历所有属性， 找到最优划分属性a∗，，， 直至满足停止条件， 假设最终将特征空间划分为M个子空间R1,R2，(x)=MXm=1cmI(x∈Rm， 其中的cm表示的也是集合Rm中的样本xi对应的输出值yi的均值。，，，。 4.3剪枝处理本节内容通俗易懂，，。。 图4.5与图4.4均是基于信息增益生成的决策树， 不同在于图4.4基于表4.1， 而图4.5基于表4.2的训练集。 结点3包含训练集“脐部”为稍凹的样本(编号6、7、15、17)，，(编号6、7、15、17)含两个好瓜和两个坏瓜， 因此叶结点硬挺的类别随机从类别好瓜和坏瓜中选择其一。 结点5包含训练集“脐部”为稍凹且“根蒂”为稍蜷的样本(编号6、7、15)，， 因此叶结点浅白类别标记为好瓜(编号6、7、15样本中，，。 结点6包含训练集“脐部”为稍凹、“根蒂”为稍蜷、“色泽”为乌黑的样本(编号7、15)， 当根据“纹理”再次进行划分时不含有“纹理”为模糊的样本)，(编号、15)含好瓜和坏瓜各一个，。 图4.5两次随机选择均选为好瓜，(参见第1章1.4节)。。 有些分类器只能使用离散属性，， 有兴趣可以通过关键词“连续属性离散化”或者“Discretization”查阅更多处理方法。 结合第11章11.2节至11.4节分别介绍的“过滤式”算法、“包裹式”算法、“嵌入式”算法的概念， 若先使用某个离散化算法对连续属性离散化后再调用C4.5决策树生成算法，，.4.1节所述，，，，。， 有些分类器不能使用含有缺失值的样本，。 常用的缺失值填充方法是，;对于离散属性，。。， 一般缺失值仅指样本的属性值有缺失， 若类别标记有缺失，。， 也可以尝试根据第11章11.6节的式(11.24)， 在低秩假设下对数据集缺失值进行填充。 4.4.1式(4， 就是以每两个相邻取值的中点作为划分点。 下面以“西瓜书”中表4.3中西瓜数据集3.0为例来说明此式的用法。 对于“密度”这个连续属性， 已观测到的可能取值为{0.243,0.245,0.343,0.360,0.403,0.,0.556,0.593,0.608,0.657,0.666,0.697,0.719,0.774，\",\n",
            "    \"，|Y|=3Xk=1p2k|=3Xk=1Xk′=kpkpk′=(p1p1+p2p2+p3p3)+(p1p2+p1p3+p2p1+p2p3+p3p1+p3p2)=(p1p1+p1p2+p1p3)+(p2p1+p2p2+p2p3)+(p3p1+p3p2+p3p3)=p1(p1+p2+p3)+p2(p1+p2+p3)+p3(p1+p2+p3)=p1+p2(4.5)Gini(D)=|Y|Xk=1Xk′=kpkpk′=1−|Xk=1p2k从数据集中D任取两个样本， 类别标记一致的概率越大表示其纯度越高，。 4.2.5式(4.6)的解释此为数据集D中属性a的基尼指数的定义，， 数据集D按照属性a的所有可能取值划分后的纯度。，，，。:(1)考虑每个属性a的每个可能取值v，=v和a=v两部分来计算基尼指数，_index(D,a)=|Da=v||D|Gini(Da=v)+=v)(2)选择基尼指数最小的属性及其对应取值作为， 直至满足停止条件。 下面以“西瓜书”中表4.2中西瓜数据集2.0为例来构造CART决策树，:以属性“色泽”为例，{青绿， 乌黑， 浅白}，，，(色泽=青绿),D2(色泽=青绿)。 子集D1包含编号{1,4,6,10,13,17}共6个样例，=36， 反例占p2=36;子集D2包含编号{2,3,5,7,16}共11个样例， 其中正例占p1=511， 反例占p2=611， 根据式(4.5=青绿”划分之后得到基尼指数为Gini_index(D,色泽=青绿)=617×1−362−362!+1117×1−5112−6112!=0，_index(D,色泽=乌黑)=0=浅白)=0=蜷缩=稍蜷)=0=硬挺)=0.439\"\n",
            "  ],\n",
            "  \"origin_tokens\": 5263,\n",
            "  \"compressed_tokens\": 2938,\n",
            "  \"ratio\": \"1.8x\",\n",
            "  \"rate\": \"55.8%\",\n",
            "  \"saving\": \", Saving $0.1 in GPT-4.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(json.dumps(compressed_result, indent=2, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "cfa97127",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            ">>> 压缩统计:\n",
            "原始 Token 数: 5263\n",
            "压缩后 Token 数: 2375\n",
            "压缩率: 45.1%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(f\"\\n>>> 压缩统计:\")\n",
        "print(f\"原始 Token 数: {compressed_result['origin_tokens']}\")\n",
        "print(f\"压缩后 Token 数: {compressed_result['compressed_tokens']}\")\n",
        "# 计算压缩率（rate 可能是字符串或浮点数）\n",
        "rate = compressed_result.get('rate', 0)\n",
        "print(f\"压缩率: {rate}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c3474c3",
      "metadata": {},
      "source": [
        "通过提示压缩，我们将上下文和用户查询整合为 prompt 后，显著减少了整体字符数，从而降低了调用大模型接口的成本。且压缩后的 prompt 仍然能保持较好的语义完整性，大模型依然能够生成准确的回答。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "376ac0e6",
      "metadata": {},
      "source": [
        "### 5.1.2 重排序 (Reranking)\n",
        "\n",
        "检索问答（RAG）系统中，**重排序（Reranking）** 是提升检索准确率的关键步骤，通常位于“两阶段检索”流程的第二阶段。\n",
        "\n",
        "#### 1. 为什么要引入重排序？\n",
        "\n",
        "<img src=\"reranker2.png\" width=\"680px\">\n",
        "\n",
        "在传统的向量检索（Vector Search）中，我们主要依赖 Embedding 模型计算查询与文档的余弦相似度。虽然这种方法速度快，但存在局限性：\n",
        "- **语义压缩损失**：Embedding 将丰富的文本信息压缩为固定维度的向量，不可避免地会丢失细节。\n",
        "- **缺乏交互与上下文**：向量检索通常基于预先计算好的文档嵌入（Index），这些嵌入在创建时并未考虑特定查询的上下文。这种“双编码器”模式下，查询和文档是独立编码的，无法捕捉它们之间细微的词汇或逻辑交互。\n",
        "\n",
        "因此，仅靠向量检索往往难以保证送入的文档都是最相关的，这就需要重排序来进行“精修”。\n",
        "\n",
        "#### 2. 工作原理：两阶段检索与交叉编码器\n",
        "\n",
        "为了兼顾**速度**和**精度**，工业界通常采用“漏斗”式的两阶段检索架构：\n",
        "\n",
        "1.  **检索（Retrieval）**：使用向量检索或关键词检索（BM25），从海量文档中快速筛选出 Top-N（如前 100 个）候选文档。这一步注重**召回率**，目的是“不遗漏”。\n",
        "2.  **重排（Reranking）**：使用**重排序模型**对这 Top-N 个文档进行逐一打分，作为**数据过滤**机制，精确计算它们与查询的相关性，并重新排序，最终只保留 Top-K（如前 5 个）给大模型。这一步注重**准确率**，目的是“去伪存真”。\n",
        "\n",
        "**为什么重排序更准？**\n",
        "\n",
        "重排序模型通常基于**交叉编码器（Cross-Encoder） **实现。不同于向量检索中查询与文档独立编码的**双编码器（Bi-Encoder）**，交叉编码器将查询和文档文本拼接后一起输入语言模型（如 BERT），通过深层的注意力机制直接计算两者的相关性得分（通常为 0~1 之间的概率值）。\n",
        "\n",
        "- **优势**：能精准捕捉语义细节（如否定、因果关系），准确率远高于向量检索。\n",
        "- **劣势**：无法预先构建文档索引，必须在查询时实时计算，计算开销大、速度慢，因此仅适用于对少量候选文档进行精细排序。\n",
        "\n",
        "#### 3. 模型选择与效果\n",
        "引入重排序可以显著提升 RAG 系统的最终效果。如下图所示，使用 `bge-reranker` 或 `cohere-reranker` 后，检索准确率明显优于未重排序（WithoutReranker）的情况 *「以下内容为在私有数据集上的效果，不同数据集效果可能不同」*。\n",
        "\n",
        "<img src=\"reranker_perform.png\" width=\"680px\">\n",
        "\n",
        "为客观评估与选择模型，建议参考 [MTEB/CMTEB](https://huggingface.co/spaces/mteb/leaderboard)（MTEB 是多任务基准测试集，CMTEB 是其中文版本） 等公共权威榜单。这些榜单提供了不同模型在标准化任务上的性能比较，可作为选型的重要依据。\n",
        "\n",
        "目前常见的重排序方案主要包括：\n",
        "- **开源模型**：[BAAI/bge-reranker](https://huggingface.co/BAAI/bge-reranker-base) 系列（bge-reranker-base/base/large），支持本地私有化部署，效果在开源界领先。\n",
        "- **商业 API**：[Cohere Rerank](https://docs.cohere.com/docs/rerank-2) 和 [Jina Reranker](https://jina.ai/reranker/)，提供高性能的 API 服务，无需自行维护基础设施。\n",
        "\n",
        "接下来，我们将演示如何使用 `bge-reranker-large` 模型在本地进行重排序。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e2a5e56",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading Model from https://www.modelscope.cn to directory: /Users/zhihu123/.cache/modelscope/hub/models/BAAI/bge-reranker-large\n"
          ]
        }
      ],
      "source": [
        "# 这里我们用 modelscope 下载模型\n",
        "model_dir = snapshot_download('BAAI/bge-reranker-large') # 也可以下载 base 「bge-reranker-base」模型，更轻量级一些，当时效果稍差"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92e95d85",
      "metadata": {},
      "source": [
        "我们使用 `LangChain` 提供的抽象组件来实现重排序。这在构建 RAG 管道时会更加方便，因为可以无缝集成到 `ContextualCompressionRetriever` 中。\n",
        "\n",
        "由于 LangChain 的 `HuggingFaceCrossEncoder` 封装有时需要配合特定的依赖，或者为了更灵活的控制，我们可以自定义一个简单的 CrossEncoder 类，或者直接使用 `HuggingFaceCrossEncoder`（如果环境中安装了 `langchain_community`）。\n",
        "\n",
        "以下是基于 LangChain 的实现示例："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "8c90d044",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: 南瓜书中对模型评估中的“代价矩阵”进行了怎样的公式变形？请结合 cost01 和 cost10 说明。\n",
            "\n",
            "==================== 原始检索结果 (Top 5) ====================\n",
            "\n",
            "[1] 中将cost01记为cost+−，cost10记为cost−+。本公式还可以作如下恒等变形E(f;D;cost)=1mm+×1m+Xxi∈D+I(f(xi̸=yi))×cost+−+m−×1m−Xxi∈D−I(f(xi̸=yi))×cost−+=m+m×1m+Xxi∈D+I(f(xi̸=yi))×cost+−+m−m×1m−Xxi∈D−I(f(xi̸=yi))×cost−+其中m+和m−分别表示正例子集D+和反例子集D−的样本个数。1m+Pxi∈D+I(f(xi̸=yi))表示正例子集D+预测错误样本所占比例，即假反例率FNR。1m−Pxi∈D−I(f(xi̸=yi))表示反例子集D−...\n",
            "\n",
            "[2] 由书中上下文可知，式(10.28)是如下优化问题的解。minw1,w2,...,wmmXi=1xi−Xj∈Qiwijxj22s.t.Xj∈Qiwij=1若令xi∈Rd×1,Qi={q1i,q2i,...,qni}，则上述优化问题的目标函数可以进行如下恒等变形mXi=1xi−Xj∈Qiwijxj22=mXi=1Xj∈Qiwijxi−Xj∈Qiwijxj22=mXi=1Xj∈Qiwij(xi−xj)22=mXi=1∥Xiwi∥22=mXi=1wiTXTiXiwi其中wi=(wiq1i,wiq2i,...,wiqni)∈Rn×1，Xi=\u0000xi−xq1i,xi−xq2i,...,xi−xqni\u0001∈Rd...\n",
            "\n",
            "[3] 所以ˆµ0=1m0K10=12κ(x1,x1)+κ(x1,x3)κ(x2,x1)+κ(x2,x3)κ(x3,x1)+κ(x3,x3)κ(x4,x1)+κ(x4,x3)∈R4×1ˆµ1=1m1K11=12κ(x1,x2)+κ(x1,x4)κ(x2,x2)+κ(x2,x4)κ(x3,x2)+κ(x3,x4)κ(x4,x2)+κ(x4,x4)∈R4×1根据此结果易得ˆµ0,ˆµ1的一般形式为ˆµ0=1m0K10=1m0Px∈X0κ(x1,x)Px∈X0κ(x2,x)...Px∈X0κ(xm,x)∈Rm×1ˆµ1=1m1K11...\n",
            "\n",
            "[4] 2.3.12式(2.25)的解释对于包含m个样本的样例集D，可以算出学习器f(x)总的代价是costse=m×p×FNR×cost+−+m×(1−p)×FPR×cost−++m×p×TPR×cost+++m×(1−p)×TNR×cost−−其中p是正例在样例集中所占的比例（或严格地称为样例为正例的概率），costse下标中的“se”表示sensitive，即代价敏感，根据前面讲述的FNR、FPR、TPR、TNR的定义可知：m×p×FNR表示正例被预测为反例（正例预测错误）的样本个数；m×(1−p)×FPR表示反例被预测为正例（反例预测错误）的样本个数；m×p×TPR表示正例被预测为正例（正例预...\n",
            "\n",
            "[5] 13.4.8式(13.20)的解释F∗=limt→∞F(t)=(1−α)(I−αS)−1Y[解析]：由式(13.19)F(t+1)=αSF(t)+(1−α)Y当t取不同的值时，有：t=0:F(1)=αSF(0)+(1−α)Y=αSY+(1−α)Yt=1:F(2)=αSF(1)+(1−α)Y=αS(αSY+(1−α)Y)+(1−α)Y=(αS)2Y+(1−α)1Xi=0(αS)i!Yt=2:F(3)=αSF(2)+(1−α)Y=αS(αS)2Y+(1−α)1Xi=0(αS)i!Y!+(1−α)Y=(αS)3Y+(1−α)2Xi=0(αS)i!Y可以观察到规律F(t)=(αS)tY+(1−α)t−...\n",
            "\n",
            "==================== 重排序后结果 (Top 3) ====================\n",
            "\n",
            "[1] 中将cost01记为cost+−，cost10记为cost−+。本公式还可以作如下恒等变形E(f;D;cost)=1mm+×1m+Xxi∈D+I(f(xi̸=yi))×cost+−+m−×1m−Xxi∈D−I(f(xi̸=yi))×cost−+=m+m×1m+Xxi∈D+I(f(xi̸=yi))×cost+−+m−m×1m−Xxi∈D−I(f(xi̸=yi))×cost−+其中m+和m−分别表示正例子集D+和反例子集D−的样本个数。1m+Pxi∈D+I(f(xi̸=yi))表示正例子集D+预测错误样本所占比例，即假反例率FNR。1m−Pxi∈D−I(f(xi̸=yi))表示反例子集D−...\n",
            "\n",
            "[2] 由书中上下文可知，式(10.28)是如下优化问题的解。minw1,w2,...,wmmXi=1xi−Xj∈Qiwijxj22s.t.Xj∈Qiwij=1若令xi∈Rd×1,Qi={q1i,q2i,...,qni}，则上述优化问题的目标函数可以进行如下恒等变形mXi=1xi−Xj∈Qiwijxj22=mXi=1Xj∈Qiwijxi−Xj∈Qiwijxj22=mXi=1Xj∈Qiwij(xi−xj)22=mXi=1∥Xiwi∥22=mXi=1wiTXTiXiwi其中wi=(wiq1i,wiq2i,...,wiqni)∈Rn×1，Xi=\u0000xi−xq1i,xi−xq2i,...,xi−xqni\u0001∈Rd...\n",
            "\n",
            "[3] 2.3.12式(2.25)的解释对于包含m个样本的样例集D，可以算出学习器f(x)总的代价是costse=m×p×FNR×cost+−+m×(1−p)×FPR×cost−++m×p×TPR×cost+++m×(1−p)×TNR×cost−−其中p是正例在样例集中所占的比例（或严格地称为样例为正例的概率），costse下标中的“se”表示sensitive，即代价敏感，根据前面讲述的FNR、FPR、TPR、TNR的定义可知：m×p×FNR表示正例被预测为反例（正例预测错误）的样本个数；m×(1−p)×FPR表示反例被预测为正例（反例预测错误）的样本个数；m×p×TPR表示正例被预测为正例（正例预...\n"
          ]
        }
      ],
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "\n",
        "# 1. 初始化 CrossEncoder 重排序模型\n",
        "rerank_model = HuggingFaceCrossEncoder(model_name=model_dir)\n",
        "\n",
        "# 2. 创建重排序器 (Compressor)\n",
        "# top_n 控制最终保留最相关的多少个文档\n",
        "reranker = CrossEncoderReranker(model=rerank_model, top_n=3)\n",
        "\n",
        "# 3. 使用之前构建的 retriever 进行检索，然后重排序\n",
        "# 构建带重排序的压缩检索器\n",
        "rerank_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=reranker,\n",
        "    base_retriever=retriever  # 使用之前构建的南瓜书文档检索器\n",
        ")\n",
        "\n",
        "# 4. 执行检索+重排序\n",
        "rerank_query = \"南瓜书中对模型评估中的“代价矩阵”进行了怎样的公式变形？请结合 cost01 和 cost10 说明。\"\n",
        "\n",
        "print(f\"Query: {rerank_query}\")\n",
        "print(\"\\n\" + \"=\" * 20 + \" 原始检索结果 (Top 5) \" + \"=\" * 20)\n",
        "original_results = retriever.invoke(rerank_query)\n",
        "for i, doc in enumerate(original_results, 1):\n",
        "    print(f\"\\n[{i}] {doc.page_content[:300]}...\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 20 + \" 重排序后结果 (Top 3) \" + \"=\" * 20)\n",
        "reranked_results = rerank_retriever.invoke(rerank_query)\n",
        "for i, doc in enumerate(reranked_results, 1):\n",
        "    print(f\"\\n[{i}] {doc.page_content[:300]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f07c2bd0",
      "metadata": {},
      "source": [
        "可以看到，重排序后果与原始检索结果的排序有所不同。Cross-Encoder 模型能够更精确地计算查询与文档的相关性，从而将最相关的文档排在前面。检索回来的都是公式内容，而经过 reranker 后，将 cost 相关的公式内容排在前面。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ecaca85",
      "metadata": {},
      "source": [
        "有了重排序，我们可以扩大检索范围，尽可能召回相关文档，同时保证送给模型的内容质量，以确保生成效果。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3181cb5f",
      "metadata": {},
      "source": [
        "## 5.2 参考引用\n",
        "\n",
        "**为什么要采用参考引用？**\n",
        "参考引用指示生成的内容来自于检索的哪些文档。\n",
        "\n",
        "1. **提供可解释性**：通过参考引用，可以明确知道生成的答案中的不同部分来自于哪些源文档。这提供了一种可解释性，让用户或开发者能够追踪答案的来源，并评估答案的可靠性。\n",
        "\n",
        "1. **增强答案的准确性**：参考引用可以帮助生成模型更准确地生成答案。通过引用相关文档中的内容，模型可以更好地利用检索到的知识来回答问题，减少生成不相关或错误信息的可能性。\n",
        "\n",
        "1. **促进知识的整合**：参考引用允许模型将来自不同文档的信息进行整合，生成更全面和连贯的答案。通过引用多个相关文档，模型可以综合不同来源的知识，生成更加完善的答案。\n",
        "\n",
        "1. **便于答案的后处理**：参考引用提供了一种结构化的方式来表示答案中的知识来源。这种结构化信息可以方便地进行后处理，如将文档 ID 替换为实际的文档标题、链接或摘要等，使答案更加人性化和易于理解。\n",
        "\n",
        "1. **支持答案的评估和改进**：通过参考引用，可以方便地评估生成答案的质量。可以比较生成的答案与引用文档的相关性和一致性，从而识别模型生成答案的优缺点。这有助于发现模型的局限性，并为进一步改进模型提供指导。\n",
        "\n",
        "下面将通过案例帮助大家理解 _参考引用_ 及其实现，面对多个文档，如何分 chunks 方便索引。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75d3affa",
      "metadata": {},
      "source": [
        "接下来我们演示如何使用 LangChain 实现参考引用功能。我们将使用南瓜书（机器学习公式详解）作为知识库，让模型在回答问题时标注信息来源（包括页码）。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b803de36",
      "metadata": {},
      "source": [
        "这里我们用两种方式实现，第一种是直接使用 `Prompt` 的方式，第二种是使用 `Pydantic` 进行结构化输出。\n",
        "首先演示第一种方式，这里通过指令模板引导模型在回答中直接标注引用，实现简单直观。输出为纯文本，易于解析和展示。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eff9e57f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: 南瓜书中对支持向量机（SVM）的对率损失函数式(6.33)是如何推导的？\n",
            "\n",
            "==================== 检索到的文档 ====================\n",
            "\n",
            "[来源1] 例，则上式可改写为ℓ(w,b)=Pmi=1ln\u00101+e−(wTxi+b)\u0011,yi=+1Pmi=1ln\u00101+ewTxi+b\u0011,yi=−1=mXi=1ln\u00101+e−yi(wTxi+b)\u0011此时上式的求和项正是式(6.33)所表述的对率损失。6.4.6式(6.41)的解释参见式(6.13)的解释6.5...\n",
            "\n",
            "[来源2] 第6章支持向量机在深度学习流行之前，支持向量机及其核方法一直是机器学习领域中的主流算法，尤其是核方法至今都仍有相关学者在持续研究。6.1间隔与支持向量6.1.1图6.1的解释回顾第5章5.2节的感知机模型可知，图6.1中的黑色直线均可作为感知机模型的解，因为感知机模型求解的是能将正负样本完全正确划分...\n",
            "\n",
            "[来源3] 6.4.4式(6.40)将式(6.37)、式(6.38)和(6.39)代入式(6.36)可以得到式(6.35)的对偶问题，有12∥w∥2+CmXi=1ξi+mXi=1αi\u00001−ξi−yi\u0000wTxi+b\u0001\u0001−mXi=1µiξi=12∥w∥2+mXi=1αi\u00001−yi\u0000wTxi+b\u0001\u0001+CmXi=1ξi...\n",
            "\n",
            "[来源4] (1)式(6.6)中的未知数是w和b，式(6.11)中的未知数是α，w的维度d对应样本特征个数，α的维度m对应训练样本个数，通常m≪d，所以求解式(6.11)更高效，反之求解式(6.6)更高效；(2)式(6.11)中有样本内积xTxji这一项，后续可以很自然地引入核函数，进而使得支持向量机也能对在原...\n",
            "\n",
            "[来源5] 式(3.11)的推导思路如下：首先根据定理3.1推导出Eˆw是ˆw的凸函数，接着根据定理3.2推导出式(3.11)。下面按照此思路进行推导。由于式(3.10)已推导出Eˆw关于ˆw的一阶导数，接着基于此进一步推导出二阶导数，即Hessian矩阵。推导过程如下：∇2Eˆw=∂∂ˆwT\u0012∂Eˆw∂ˆw\u0013...\n"
          ]
        }
      ],
      "source": [
        "# 利用之前构建的南瓜书向量存储和检索器\n",
        "\n",
        "# 定义带引用的生成模板\n",
        "citation_template = \"\"\"基于以下背景信息回答问题。请在回答中标注信息来源，格式为 [来源X]。\n",
        "\n",
        "背景信息：\n",
        "{context}\n",
        "\n",
        "问题：{question}\n",
        "\n",
        "请按以下格式回答：\n",
        "1. 先给出答案，在相关陈述后标注 [来源1]、[来源2] 等\n",
        "2. 最后列出所有引用的来源\n",
        "\"\"\"\n",
        "\n",
        "citation_prompt = ChatPromptTemplate.from_template(citation_template)\n",
        "\n",
        "# 检索相关文档\n",
        "# citation_query = \"什么是支持向量机(SVM)？它的核心思想是什么？\"\n",
        "citation_query = \"南瓜书中对支持向量机（SVM）的对率损失函数式(6.33)是如何推导的？\"\n",
        "citation_docs = retriever.invoke(citation_query)\n",
        "\n",
        "print(f\"Query: {citation_query}\")\n",
        "print(\"\\n\" + \"=\" * 20 + \" 检索到的文档 \" + \"=\" * 20)\n",
        "for i, doc in enumerate(citation_docs, 1):\n",
        "    print(f\"\\n[来源{i}] {doc.page_content[:300]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3929c172",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================== 带引用的回答 ====================\n",
            "1. 南瓜书中对支持向量机（SVM）的对率损失函数式(6.33)的推导如下：首先，将式(6.37)、式(6.38)和(6.39)代入式(6.36)可以得到式(6.35)的对偶问题。然后，通过一系列代数变换和条件约束，最终得到式(6.33)。具体推导过程涉及对数几率回归与支持向量机的关系，以及对数几率损失函数的变形。这一推导过程在“西瓜书”的第6章中有所描述 [来源1]。\n",
            "\n",
            "2. 引用来源：\n",
            "   [来源1] (第67页) 例，则上式可改写为ℓ(w,b)=Pmi=1ln(1+e−(wTxi+b)),yi=+1Pmi=1ln(1+ewTxi+b),yi=−1=mXi=1ln(1+e−yi(wTxi+b))此时上式的求和项正是式(6.33)所表述的对率损失。6.4.6式(6.41)的解释参见式(6.13)的解释6.5支持向量回归6.5.1式(6.43)的解释相比于线性回归用一条线来拟合训练样本，支持向量回归而是采用一个以f(x)=wTx+b为中心，宽度为2ϵ的间隔带，来拟合训练样本。落在带子上的样本不计算损失（类比线性回归在线上的点预测误差为0），不在带子上的则以偏离带子的距离作为损失（类比线性回归的均方误差），然后以最小化损失的方式迫使间隔带从样本最密集的地方穿过，进而达到拟合训练样本的目的。因此支持向量回归的优化问题可以写为minw,b12∥w∥2+CmXi=1ℓϵ(f(xi)−yi)其中ℓϵ(z)为“ϵ不敏感损失函数”（类比线性回归的均方误差损失）ℓϵ(z)=(0,if|z|⩽ϵ|z|−ϵ,if|z|>ϵ12∥w∥2为L2正则项，此处引入正则项除了起正则化本身的作用外，也是为了和软间隔支持向量机的优化目标保持形式上的一致，这样就可以导出对偶问题引入核函数，C为用来调节损失权重的正则化常数。6.5.2式(6.45)的推导同软间隔支持向量机，引入松弛变量ξi，令ℓϵ(f(xi)−yi)=ξi显然ξi⩾0，并且当|f(xi)−yi|⩽ϵ时，ξi=0，当|f(xi)−yi|>ϵ时，ξi=|f(xi)−yi|−ϵ，所以|f(xi)−yi|−ϵ⩽ξi|f(xi)−yi|⩽ϵ+ξi−ϵ−ξi⩽f(xi)−yi⩽ϵ+ξi因此支持向量回归的优化问题可以化为minw,b,ξi12∥w∥2+CmXi=1ξis.t.−ϵ−ξi⩽f(xi)−yi⩽ϵ+ξiξi⩾0,i=1,2,...,m\n"
          ]
        }
      ],
      "source": [
        "def format_docs_with_sources(docs):\n",
        "    \"\"\"格式化文档，添加来源编号\"\"\"\n",
        "    formatted = []\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        page = doc.metadata.get('page', 'N/A')\n",
        "        formatted.append(f\"[来源{i}] (第{page}页) {doc.page_content}\")\n",
        "    return \"\\n\\n\".join(formatted)\n",
        "\n",
        "# 生成带引用的回答\n",
        "context_with_sources = format_docs_with_sources(citation_docs)\n",
        "\n",
        "# 构建生成链\n",
        "citation_chain = citation_prompt | llm | StrOutputParser()\n",
        "\n",
        "# 执行生成\n",
        "print(\"\\n\" + \"=\" * 20 + \" 带引用的回答 \" + \"=\" * 20)\n",
        "citation_response = citation_chain.invoke({\n",
        "    \"context\": context_with_sources,\n",
        "    \"question\": citation_query\n",
        "})\n",
        "print(citation_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2478272",
      "metadata": {},
      "source": [
        "第二种方式利用 `Pydantic` 定义结构化输出格式，确保返回结果具有固定的数据结构，以提供置信度等元数据信息，但是需要模型支持结构化输出功能\n",
        "，对指令遵循能力要求较高。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "86b57bb8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: 南瓜书中对支持向量机（SVM）的对率损失函数式(6.33)是如何推导的？\n",
            "\n",
            "==================== 结构化输出 ====================\n",
            "\n",
            "回答: 南瓜书中对支持向量机（SVM）的对率损失函数式(6.33)的推导过程如下：首先，考虑对数几率回归模型，其损失函数为对数损失函数。在支持向量机中，使用对率损失函数来代替对数几率回归中的0/1损失函数，从而得到对率回归模型。具体推导如下：\n",
            "\n",
            "1. 对数几率回归模型的对数损失函数为：\n",
            "   ℓ(β) = ∑_{i=1}^{m} ln(y_i p(y_i|x_i;β) + (1−y_i) p(0|x_i;β))\n",
            "\n",
            "2. 其中，p(y_i|x_i;β) 表示在给定特征 x_i 和参数 β 的情况下，样本 y_i 出现的概率，p(0|x_i;β) 表示在给定特征 x_i 和参数 β 的情况下，样本 y_i 不出现的概率。\n",
            "\n",
            "3. 将 p(y_i|x_i;β) 和 p(0|x_i;β) 分别表示为指数函数的形式：\n",
            "   p(y_i|x_i;β) = e^(β^T x_i) / (1 + e^(β^T x_i))\n",
            "   p(0|x_i;β) = 1 / (1 + e^(β^T x_i))\n",
            "\n",
            "4. 将上述表达式代入对数损失函数中，得到：\n",
            "   ℓ(β) = ∑_{i=1}^{m} ln(y_i e^(β^T x_i) + (1−y_i) / (1 + e^(β^T x_i)))\n",
            "\n",
            "5. 对上述表达式进行化简，得到对率损失函数式(6.33)：\n",
            "   ℓ(β) = ∑_{i=1}^{m} [y_i ln(y_i e^(β^T x_i) + (1−y_i) / (1 + e^(β^T x_i))) + (1−y_i) ln(1 / (1 + e^(β^T x_i)))]\n",
            "\n",
            "6. 在支持向量机中，正例和反例分别用 y_i = +1 和 y_i = −1 表示，因此可以将上述表达式中的 y_i 替换为 +1 和 −1，得到支持向量机的对率损失函数式(6.33)：\n",
            "   ℓ(β) = ∑_{i=1}^{m} [y_i ln(y_i e^(β^T x_i) + (1−y_i) / (1 + e^(β^T x_i))) + (1−y_i) ln(1 / (1 + e^(β^T x_i)))]\n",
            "\n",
            "置信度: 0.9\n",
            "\n",
            "引用:\n",
            "  [来源1]: 例，则上式可改写为ℓ(w,b)=Pmi=1ln(1+e−(wTxi+b)),yi=+1Pmi=...\n",
            "  [来源3]: 如果使用对率损失函数ℓlog来替代式(6.29)中的0/1损失函数，则几乎就得到了对率回归模型(3....\n",
            "  [来源4]: 上式中正例和反例分别用yi=1和yi=0表示，这是对数几率回归常用的方式，而在支持向量机中正例和反例...\n"
          ]
        }
      ],
      "source": [
        "# 结构化输出示例 - 使用 Pydantic 解析引用\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "\n",
        "# 1. 定义结构化输出 Schema\n",
        "class Citation(BaseModel):\n",
        "    \"\"\"单个引用\"\"\"\n",
        "    source_id: int = Field(description=\"来源编号\")\n",
        "    quote: str = Field(description=\"引用的原文片段\")\n",
        "\n",
        "class AnswerWithCitations(BaseModel):\n",
        "    \"\"\"带引用的回答\"\"\"\n",
        "    answer: str = Field(description=\"回答内容\")\n",
        "    citations: List[Citation] = Field(description=\"引用列表\")\n",
        "    confidence: float = Field(description=\"置信度，0-1之间\")\n",
        "\n",
        "# 2. 创建 Pydantic 解析器\n",
        "parser = PydanticOutputParser(pydantic_object=AnswerWithCitations)\n",
        "\n",
        "# 3. 构建带格式指令的 Prompt\n",
        "structured_template = \"\"\"基于以下背景信息回答问题，并提供引用。\n",
        "\n",
        "背景信息：\n",
        "{context}\n",
        "\n",
        "问题：{question}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "structured_prompt = ChatPromptTemplate.from_template(structured_template)\n",
        "\n",
        "# 4. 执行结构化生成\n",
        "citation_query = \"南瓜书中对支持向量机（SVM）的对率损失函数式(6.33)是如何推导的？\"\n",
        "structured_docs = retriever.invoke(citation_query)\n",
        "\n",
        "try:\n",
        "    structured_chain = structured_prompt | llm | parser\n",
        "    result = structured_chain.invoke({\n",
        "        \"context\": format_docs_with_sources(structured_docs),\n",
        "        \"question\": citation_query,\n",
        "        \"format_instructions\": parser.get_format_instructions()\n",
        "    })\n",
        "    \n",
        "    print(f\"Query: {citation_query}\")\n",
        "    print(\"\\n\" + \"=\" * 20 + \" 结构化输出 \" + \"=\" * 20)\n",
        "    print(f\"\\n回答: {result.answer}\")\n",
        "    print(f\"\\n置信度: {result.confidence}\")\n",
        "    print(f\"\\n引用:\")\n",
        "    for cite in result.citations:\n",
        "        print(f\"  [来源{cite.source_id}]: {cite.quote[:50]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"结构化解析失败: {e}\")\n",
        "    print(\"注意：结构化输出对模型能力有一定要求，某些模型可能无法完美遵循格式指令。\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "82f5da35",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[来源1] 对于第一个条件，当超平面满足该条件时，根据超平面的性质(4)可知，若yi=+1的正样本被划分到正空间（当然也可以将其划分到负空间），yi=−1的负样本被划分到负空间，以下不等式成立(wTxi+b⩾0,yi=+1wTxi+b⩽0,yi=−1对于第二个条件，首先设离超平面最近的正样本为x+∗，离超平面最近的负样本为x−∗，由于这两样本是离超平面最近的点，所以其他样本到超平面的距离均大于等于它们，即|wTxi+b|∥w∥⩾|wTx+∗+b|∥w∥,yi=+1|wTxi+b|∥w∥⩾|wTx−∗+b|∥w∥,yi=−1结合第一个条件中推导出的不等式，可将上式中的绝对值符号去掉并推得(wTxi+b∥w∥⩾wTx+∗+b∥w∥,yi=+1wTxi+b∥w∥⩽wTx−∗+b∥w∥,yi=−1基于此再考虑第二个条件，“位于正负样本正中间”等价于要求超平面到x+∗和x−∗这两点的距离相等，即wTx+∗+b∥w∥=wTx−∗+b∥w∥综上，支持向量机所要求的超平面所需要满足的条件如下wTxi+b∥w∥⩾wTx+∗+b∥w∥,yi=+1wTxi+b∥w∥⩽wTx−∗+b∥w∥,yi=−1|wTx+∗+b|∥w∥=|wTx−∗+b|∥w∥但是根据超平面的性质(2)可知，当等倍缩放法向量w和位移项b时，超平面不变，且上式也恒成立，因此会导致所求的超平面的参数w和b有无穷多解。因此为了保证每个超平面的参数只有唯一解，不妨再额外施加一些约束，例如约束x+∗和x−∗代入进超平面方程后的绝对值为1，也就是令wTx+∗+b=1,wTx−∗+b=−1。此时支持向量机所要求的超平面所需要满足的条件变为(wTxi+b∥w∥⩾+1∥w∥,yi=+1wTxi+b∥w∥⩽−1∥w∥,yi=−1由于∥w∥恒大于0，因此上式可进一步化简为(wTxi+b⩾+1,yi=+1wTxi+b⩽−1,yi=−16.1.5式(6.4)的推导根据式(6.3)的推导可知，x+∗和x−∗便是“支持向量”，因此支持向量到超平面的距离已经被约束为1∥w∥，所以两个异类支持向量到超平面的距离之和为2∥w∥。6.1.6式(6.5)的解释式(6.5)是通过“最大化间隔”来保证超平面离正负样本都尽可能远，且该超平面有且仅有一个，因此可以解出唯一解。...\n",
            "\n",
            "[来源2] 为等式，说明µ∗igi(x∗)=0。此时再结合主问题和对偶问题原有的约束条件µ∗i⩾0,gi(x∗)⩽0,hj(x∗)=0便凑齐了KKT条件。6.2.5式(6.9)和式(6.10)的推导L(w,b,α)=12∥w∥2+mXi=1αi(1−yi(wTxi+b))=12∥w∥2+mXi=1(αi−αiyiwTxi−αiyib)=12wTw+mXi=1αi−mXi=1αiyiwTxi−mXi=1αiyib对w和b分别求偏导数并令其为零∂L∂w=12×2×w+0−mXi=1αiyixi−0=0=⇒w=mXi=1αiyixi∂L∂b=0+0−0−mXi=1αiyi=0=⇒mXi=1αiyi=06.2.6式(6.11)的推导因为αi⩾0，且12∥w∥2和1−yi\u0000wTxi+b\u0001均是关于w和b的凸函数，所以式(6.8)也是关于w和b的凸函数。根据凸函数的性质可知，其极值点就是最值点，所以一阶导为零的点就是最小值点，因此将式(6.9)和式(6.10)代入式(6.8)后即可得式(6.8)的最小值（等价于下确界），再根据对偶问题的定义加上约束αi⩾0，就得到了式(6.6)的对偶问题。由于式(6.10)也是αi必须满足的条件，且不含有w和b，因此也需要纳入对偶问题的约束条件。根据以上思路进行推导的过程如下：infw,bL(w,b,α)=12wTw+mXi=1αi−mXi=1αiyiwTxi−mXi=1αiyib=12wTmXi=1αiyixi−wTmXi=1αiyixi+mXi=1αi−bmXi=1αiyi=−12wTmXi=1αiyixi+mXi=1αi−bmXi=1αiyi=−12wTmXi=1αiyixi+mXi=1αi=−12(mXi=1αiyixi)T(mXi=1αiyixi)+mXi=1αi=−12mXi=1αiyixTimXi=1αiyixi+mXi=1αi=mXi=1αi−12mXi=1mXj=1αiαjyiyjxTixj所以maxαinfw,bL(w,b,α)=maxαmXi=1αi−12mXi=1mXj=1αiαjyiyjxTixj最后将αi⩾0和式(6.10)作为约束条件即可得式(6.11)。式(6.6)之所以要转化为式(6.11)来求解，其主要有以下两点理由：...\n",
            "\n",
            "[来源3] 3.2.8式(3.11)的推导首先铺垫讲解接下来以及后续内容将会用到的多元函数相关基础知识[1]。n元实值函数：含n个自变量，值域为实数域R的函数称为n元实值函数，记为f(x)，其中x=(x1;x2;...;xn)为n维向量。“西瓜书”和本书中的多元函数未加特殊说明均为实值函数。凸集：设集合D⊂Rn为n维欧式空间中的子集，如果对D中任意的n维向量x∈D和y∈D与任意的α∈[0,1]，有αx+(1−α)y∈D则称集合D是凸集。凸集的几何意义是：若两个点属于此集合，则这两点连线上的任意一点均属于此集合。常见的凸集有空集∅，整个n维欧式空间Rn。凸函数：设D⊂Rn是非空凸集，f是定义在D上的函数，如果对任意的x1,x2∈D,α∈(0,1)，均有f\u0000αx1+(1−α)x2\u0001⩽αf(x1)+(1−α)f(x2)则称f为D上的凸函数。若其中的⩽改为<也恒成立，则称f为D上的严格凸函数。梯度：若n元函数f(x)对x=(x1;x2;...;xn)中各分量xi的偏导数∂f(x)∂xi(i=1,2,...,n)都存在，则称函数f(x)在x处一阶可导，并称以下列向量∇f(x)=∂f(x)∂x=∂f(x)∂x1∂f(x)∂x2...∂f(x)∂xn为函数f(x)在x处的一阶导数或梯度，易证梯度指向的方向是函数值增大速度最快的方向。∇f(x)也可写成行向量形式∇f(x)=∂f(x)∂xT=\u0014∂f(x)∂x1,∂f(x)∂x2,···,∂f(x)∂xn\u0015我们称列向量形式为“分母布局”，行向量形式为“分子布局”，由于在最优化中习惯采用分母布局，因此“西瓜书”以及本书中也采用分母布局。为了便于区分当前采用何种布局，通常在采用分母布局时偏导符号∂后接的是x，采用分子布局时后接的是xT。Hessian矩阵：若n元函数f(x)对x=(x1;x2;...;xn)中各分量xi的二阶偏导数∂2f(x)∂xi∂xj(i=1,2,...,n;j=1,2,...,n)都存在，则称函数f(x)在x处二阶阶可导，并称以下矩阵∇2f(x)=∂2f(x)∂x∂xT=∂2f(x)∂x21∂2f(x)∂x1∂x2···∂2f(x)∂x1∂xn∂2f(x)∂x2∂x1∂2f(x)∂x22···∂2f(x)∂x2∂xn............∂2f(x)∂xn∂x1∂2f(x)∂xn∂x2···∂2f(x)∂x2n为函数f(x)在x处的二阶导数或Hessian矩阵。若其中的二阶偏导数均连续，则∂2f(x)∂xi∂xj=∂2f(x)∂xj∂xi此时Hessian矩阵为对称矩阵。定理3.1：设D⊂Rn是非空开凸集，f(x)是定义在D上的实值函数，且f(x)在D上二阶连续可微，如果f(x)的Hessian矩阵∇2f(x)在D上是半正定的，则f(x)是D上的凸函数；如果∇2f(x)在D上是正定的，则f(x)是D上的严格凸函数。定理3.2：若f(x)是凸函数，且f(x)一阶连续可微，则x∗是全局解的充分必要条件是其梯度等于零向量，即∇f(x∗)=0。...\n",
            "\n",
            "[来源4] 第6章支持向量机在深度学习流行之前，支持向量机及其核方法一直是机器学习领域中的主流算法，尤其是核方法至今都仍有相关学者在持续研究。6.1间隔与支持向量6.1.1图6.1的解释回顾第5章5.2节的感知机模型可知，图6.1中的黑色直线均可作为感知机模型的解，因为感知机模型求解的是能将正负样本完全正确划分的超平面，因此解不唯一。而支持向量机想要求解的则是离正负样本都尽可能远且刚好位于“正中间”的划分超平面，因为这样的超平面理论上泛化性能更好。6.1.2式(6.1)的解释n维空间的超平面定义为wTx+b=0，其中w,x∈Rn，w=(w1;w2;...;wn)称为法向量，b称为位移项。超平面具有以下性质：(1)法向量w和位移项b确定一个唯一超平面；(2)超平面方程不唯一，因为当等倍缩放w和b时（假设缩放倍数为α），所得的新超平面方程αwTx+αb=0和wTx+b=0的解完全相同，因此超平面不变，仅超平面方程有变；(3)法向量w垂直于超平面；(4)超平面将n维空间切割为两半，其中法向量w指向的那一半空间称为正空间，另一半称为负空间，正空间中的点x+代入进方程wTx++b其计算结果大于0，反之负空间中的点代入进方程其计算结果小于0；(5)n维空间中的任意点x到超平面的距离公式为r=|wTx+b|∥w∥，其中∥w∥表示向量w的模。6.1.3式(6.2)的推导对于任意一点x0=(x01;x02;...;x0n)，设其在超平面wTx+b=0上的投影点为x1=(x11;x12;...;x1n)，则wTx1+b=0。根据超平面的性质(3)可知，此时向量−−−→x1x0与法向量w平行，因此|w·−−−→x1x0|=|∥w∥·cosπ·∥−−−→x1x0∥|=∥w∥·∥−−−→x1x0∥=∥w∥·r又w·−−−→x1x0=w1(x01−x11)+w2(x02−x12)+...+wn(x0n−x1n)=w1x01+w2x02+...+wnx0n−(w1x11+w2x12+...+wnx1n)=wTx0−wTx1=wTx0+b所以|wTx0+b|=∥w∥·rr=wTx+b∥w∥6.1.4式(6.3)的推导支持向量机所要求的超平面需要满足三个条件，第一个是能正确划分正负样本，第二个是要位于正负样本正中间，第三个是离正负样本都尽可能远。式(6.3)仅满足前两个条件，第三个条件由式(6.5)来满足，因此下面仅基于前两个条件来进行推导。...\n",
            "\n",
            "[来源5] 则表示在w>0,b<0范围内寻找目标函数的最小值，“s.t.”是“subjectto”的简写，意思是“受约束于”，即为约束条件。以上介绍的符号都是应用数学领域的一个分支——“最优化”中的内容，若想进一步了解可找一本最优化的教材（例如参考文献[1]）进行系统性地学习。3.2.3式(3.5)的推导“西瓜书”在式(3.5)左侧给出的凸函数的定义是最优化中的定义，与高等数学中的定义不同，本书也默认采用此种定义。由于一元线性回归可以看作是多元线性回归中元的个数为1时的情形，所以此处暂不给出E(w,b)是关于w和b的凸函数的证明，在推导式(3.11)时一并给出，下面开始推导式(3.5)。已知E(w,b)=mPi=1(yi−wxi−b)2，所以∂E(w,b)∂w=∂∂w\"mXi=1(yi−wxi−b)2#=mXi=1∂∂wh(yi−wxi−b)2i=mXi=1[2·(yi−wxi−b)·(−xi)]=mXi=1\u00022·\u0000wx2i−yixi+bxi\u0001\u0003=2·wmXi=1x2i−mXi=1yixi+bmXi=1xi!=2wmXi=1x2i−mXi=1(yi−b)xi!3.2.4式(3.6)的推导已知E(w,b)=mPi=1(yi−wxi−b)2，所以∂E(w,b)∂b=∂∂b\"mXi=1(yi−wxi−b)2#=mXi=1∂∂bh(yi−wxi−b)2i=mXi=1[2·(yi−wxi−b)·(−1)]=mXi=1[2·(b−yi+wxi)]=2·\"mXi=1b−mXi=1yi+mXi=1wxi#=2mb−mXi=1(yi−wxi)!3.2.5式(3.7)的推导推导之前先重点说明一下“闭式解”或称为“解析解”。闭式解是指可以通过具体的表达式解出待解参数，例如可根据式(3.7)直接解得w。机器学习算法很少有闭式解，线性回归是一个特例，接下来推导...\n"
          ]
        }
      ],
      "source": [
        "for i, doc in enumerate(structured_docs, 1):\n",
        "    print(f\"\\n[来源{i}] {doc.page_content[:3000]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c10717f",
      "metadata": {},
      "source": [
        "通过结构化输出，我们可以将模型的回答解析为程序可处理的数据结构，便于后续的自动化处理，如：\n",
        "- 自动验证引用的准确性\n",
        "- 将引用链接到原文\n",
        "- 根据置信度决定是否需要人工审核"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c10717g",
      "metadata": {},
      "source": [
        "**参考链接与文献**\n",
        "\n",
        "1. https://www.pinecone.io/learn/series/rag/rerankers/\n",
        "2. https://towardsdatascience.com/improving-rag-performance-using-rerankers-6adda61b966d\n",
        "3. https://www.llamaindex.ai/blog/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83\n",
        "4. https://python.langchain.ac.cn/docs/how_to/qa_citations/\n",
        "5. Zhang T, Patil S G, Jain N, et al. Raft: Adapting language model to domain specific rag[J]. arXiv preprint arXiv:2403.10131, 2024.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3e2c105",
      "metadata": {},
      "source": [
        "## 总结\n",
        "\n",
        "本章介绍了 RAG 系统中生成阶段的关键优化技术：\n",
        "\n",
        "### 后处理优化\n",
        "\n",
        "**信息压缩**\n",
        "- **上下文压缩**：使用 `EmbeddingsFilter` 过滤掉与查询相关性低的文档，减少 Token 消耗\n",
        "- **提示压缩**：使用 LLMLingua 等工具压缩 Prompt，在保持语义完整性的同时降低成本\n",
        "\n",
        "**重排序 (Reranking)**\n",
        "- 使用 Cross-Encoder 模型（如 bge-reranker）对检索结果进行精排\n",
        "\n",
        "### 参考引用\n",
        "\n",
        "- 在回答中标注信息来源，提高可解释性和可信度\n",
        "- 使用结构化输出（Pydantic）便于后续处理\n",
        "\n",
        "### 最佳实践建议\n",
        "\n",
        "1. **根据场景选择压缩策略**：简单查询用上下文压缩，复杂查询用提示压缩\n",
        "2. **合理设置相似度阈值**：过高会丢失相关信息，过低则无法有效过滤\n",
        "3. **重排序与压缩可组合使用**：先重排序选出 Top-K，再压缩减少 Token\n",
        "4. **始终提供引用来源**：提高用户对 AI 回答的信任度"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
