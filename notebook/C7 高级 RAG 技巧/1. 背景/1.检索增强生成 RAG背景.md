# 检索增强生成 RAG 背景

## 一、什么是 RAG

大型语言模型（LLM）相较于传统的语言模型具有更强大的能力，然而在某些情况下，它们仍可能无法提供准确的答案。目前 LLM 面临的主要问题有：

- **信息偏差/幻觉：** LLM 有时会产生与客观事实不符的信息，导致用户接收到的信息不准确。RAG 通过检索数据源，辅助模型生成过程，确保输出内容的精确性和可信度，减少信息偏差。
- **知识更新滞后性：** LLM 基于静态的数据集训练，这可能导致模型的知识更新滞后，无法及时反映最新的信息动态。RAG 通过实时检索最新数据，保持内容的时效性，确保信息的持续更新和准确性。
- **内容不可追溯：** LLM 生成的内容往往缺乏明确的信息来源，影响内容的可信度。RAG 将生成内容与检索到的原始资料建立链接，增强了内容的可追溯性，从而提升了用户对生成内容的信任度。
- **领域专业知识能力欠缺：** LLM 在处理特定领域的专业知识时，效果可能不太理想，这可能会影响到其在相关领域的回答质量。RAG 通过检索特定领域的相关文档，为模型提供丰富的上下文信息，从而提升了在专业领域内的问题回答质量和深度。
- **推理能力限制：** 面对复杂问题时，LLM 可能缺乏必要的推理能力，这影响了其对问题的理解和回答。RAG 结合检索到的信息和模型的生成能力，通过提供额外的背景知识和数据支持，增强了模型的推理和理解能力。
- **应用场景适应性受限：** LLM 需在多样化的应用场景中保持高效和准确，但单一模型可能难以全面适应所有场景。RAG 使得 LLM 能够通过检索对应应用场景数据的方式，灵活适应问答系统、推荐系统等多种应用场景。
- **长文本处理能力较弱：** LLM 在理解和生成长篇内容时受限于有限的上下文窗口，且必须按顺序处理内容，输入越长，速度越慢。RAG 通过检索和整合长文本信息，强化了模型对长上下文的理解和生成，有效突破了输入长度的限制，同时降低了调用成本，并提升了整体的处理效率。

为了解决大型语言模型在生成文本时面临的一系列挑战，提高模型的性能和输出质量，研究人员提出了一种新的模型架构：**检索增强生成（RAG, Retrieval-Augmented Generation）**。该架构巧妙地**整合了从庞大知识库中检索到的相关信息，并以此为基础，指导大型语言模型生成更为精准的答案**，从而显著提升了回答的准确性与深度，在一定程度上弥补了大模型的缺陷。


## 二、RAG 的工作流程

RAG 是一个完整的系统，其工作流程可以简单地分为数据处理、检索、增强和生成四个阶段：
![RAG 示例图](../../../figures/C1-2-RAG.png)

### 2.1 数据处理阶段

**核心目标：** 对各种格式的数据进行预处理，以适应大模型的输入要求。

- **数据预处理：** 自动识别并解析多种格式（Word、TXT、Excel、PDF等），利用OCR技术从图片和视频中提取文本，确保所有知识源统一转换为高质量、结构化的纯文本数据。

- **数据分块：** 由于大模型处理上下文的能力有限，因此需要根据文本内容和模型处理能力的实时评估，动态调整文本块大小，平衡信息完整性与模型处理效率。

- **索引构建：** 采用先进的词嵌入模型和压缩算法，减少向量存储空间，同时构建快速索引机制，确保检索速度与质量。

### 2.2 检索阶段

**核心目标**：快速准确地从海量数据中检索出最相关信息。

- **问题向量化**： 利用微调后的词嵌入模型，针对用户查询进行更精确的语义向量化。
- **高效计算相似度**： 利用向量数据库提供的强大的语义向量检索能力，计算问题向量与知识库的相似度。
- **选择top K检索结果**： 根据查询复杂度和用户偏好，动态调整返回文本块数量K，确保信息的全面性。

### 2.3 增强阶段

**核心目标：** 构建高质量提示，促进生成模型的理解与生成能力。

将用户提问、筛选后的检索结果以及历史对话信息进行深度融合，构建出一个完整、连贯且富含上下文信息的提示词。这一提示词不仅包含了问题所需的关键信息，还融入了足够的背景知识和上下文关联，有助于生成模型更准确地理解问题并生成高质量的回答。

### 2.4 生成阶段

**核心目标：** 高质量、高效地生成回答。

在生成阶段，LLM运用其强大的语义分析能力，深入剖析提示词所蕴含的丰富语义关联和上下文线索，通过精准捕捉用户提问的核心要点，LLM能够生成与问题高度契合、信息准确无误且表述清晰流畅的回答，从而使用户能够轻松理解并快速吸收关键信息。

## 三、RAG VS Finetune

在提升大语言模型效果中，RAG 和 微调（Finetune）是两种主流的方法。 

RAG 和 微调的对比可以参考下表，数据来源：[1](https://arxiv.org/abs/2312.10997)和[2](https://baoyu.io/translations/ai-paper/2312.10997-retrieval-augmented-generation-for-large-language-models-a-survey)

| 特征比较 | RAG | 微调 |
| --- | --- | --- |
| 知识更新 | 直接更新检索知识库，无需重新训练。信息更新成本低，适合动态变化的数据。 | 通常需要重新训练来保持知识和数据的更新。更新成本高，适合静态数据。 |
| 外部知识 | 擅长利用外部资源，特别适合处理文档或其他结构化/非结构化数据库。 | 将外部知识学习到 LLM 内部。 |
| 数据处理 | 对数据的处理和操作要求极低。 | 依赖于构建高质量的数据集，有限的数据集可能无法显著提高性能。 |
| 模型定制 | 侧重于信息检索和融合外部知识，但可能无法充分定制模型行为或写作风格。 | 可以根据特定风格或术语调整 LLM 行为、写作风格或特定领域知识。 |
| 可解释性 | 可以追溯到具体的数据来源，有较好的可解释性和可追踪性。 | 黑盒子，可解释性相对较低。 |
| 计算资源 | 需要额外的资源来支持检索机制和数据库的维护。 | 依赖高质量的训练数据集和微调目标，对计算资源的要求较高。 |
| 推理延迟 | 增加了检索步骤的耗时 | 单纯 LLM 生成的耗时 |
| 降低幻觉 | 通过检索到的真实信息生成回答，降低了产生幻觉的概率。 | 模型学习特定领域的数据有助于减少幻觉，但面对未见过的输入时仍可能出现幻觉。 |
| 伦理隐私 | 检索和使用外部数据可能引发伦理和隐私方面的问题。 | 训练数据中的敏感信息需要妥善处理，以防泄露。 |

## 四、RAG存在的问题及解决办法

| 问题阶段     | 问题描述                                                     | 解决办法                                                     |
| ------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 数据处理阶段 | 一是文档格式种类多，特别是扫描版pdf或者双栏图片等这种特殊的文件，这些需要进行特殊的处理。<br>二是术语表述不统一，可能会造成歧义，例如“LLM”、“大语言模型”和“大模型”等。<br>三是对文本即进行分块时，如果分块太大，可能包含太多不相关的信息，从而降低了检索的准确性。相反，分块太小可能会丢失必要的上下文信息，导致生成的回应缺乏连贯性或深度。 | **<font style="color:rgb(25, 27, 31);">提升数据质量：</font>**<font style="color:rgb(25, 27, 31);">高性能RAG系统依赖于准确且清洁的原始知识数据，为了保证数据的准确性，需要优化各种格式类型文件的处理方法。另外，也需要去除特殊字符、统一术语表示、根据主题归类存储等。</font> **<font style="color:rgb(25, 27, 31);">采用不同的分块方法：</font>**<font style="color:rgb(25, 27, 31);">在RAG系统中实施合适的文档分块方法，旨在确保信息的完整性和相关性。</font><br>（1）固定大小分块：直接设定块中的字数，并选择块之间是否重复内容，以确保语义上下文不会在块之间丢失。<font style="color:rgb(25, 27, 31);"><br>（2）内容分块：根据文档的具体内容进行分块，例如根据标点符号（如句号）分割，或者直接使用更高级的NLTK或者spaCy库提供的句子分割功能。</font> <br>（3）<font style="color:rgb(25, 27, 31);">递归分块：推荐的方法，通过重复地应用分块规则来递归地分解文本。例如，先通过段落换行符（`\n\n`）进行分割。对于大小超过标准的块，使用单换行符（`\n`）再次分割，否则进行保留。以此类推，不断根据块大小更新更小的分块规则（如空格，句号）。</font><br>（4） <font style="color:rgb(25, 27, 31);">从小到大分块：充分利用大分块和小分块各自的优势，把同一文档进行从大到小所有尺寸的分割，然后把不同大小的分块全部存进向量数据库，并保存每个分块的上下级关系。</font> <br>（5）<font style="color:rgb(25, 27, 31);">特殊文件类型分块：针对特定文件类型的专门分割器，例如Markdown文件，Latex文件等分割器。<br></font>**添加元数据信息**：<font style="color:rgb(15, 23, 42);">将引用的日期和用途（用于筛选）、以及章节标题等元数据嵌入到文本块中，对于提升检索效率和准确性是有益的。<br></font> <font style="color:rgb(15, 23, 42);">**混合检索**：</font><u><font style="color:rgb(15, 23, 42);"> </font></u><font style="color:rgb(15, 23, 42);">通过融合关键词搜索、语义搜索和向量搜索等多种技术，适应不同类型的查询需求，确保能够一致地检索到最相关和内容丰富的信息。</font> |
| 检索阶段     | <br>一是低精度问题，即检索集中的文本块并不都与查询内容相关，这可能导致信息错误或不连贯。<br>二是低召回率问题，即未能检索到所有相关的文档块，使得大语言模型无法获取足够的背景信息来合成答案。<br>三是过时信息问题，因为数据冗余或过时可能导致检索结果不准确。 | **<font style="color:rgb(15, 23, 42);">微调嵌入模型：</font>**<font style="color:rgb(15, 23, 42);"> 微调嵌入模型的调整直接影响到 RAG 的有效性。微调的目的是让检索到的内容与查询之间的相关性更加紧密。微调嵌入模型的作用可以比作在语音生成前对“听觉”进行调整，优化检索内容对最终输出的影响。通常，微调嵌入模型的方法可以分为针对特定领域上下文的嵌入调整和检索步骤的优化。特别是在处理不断变化或罕见术语的专业领域，这些定制化的嵌入模型方法能够显著提高检索的相关性。<br></font>**<font style="color:rgb(15, 23, 42);">动态嵌入（Dynamic Embedding）：</font>**<font style="color:rgb(15, 23, 42);">不同于静态嵌入（static embedding），动态嵌入根据单词出现的上下文进行调整，为每个单词提供不同的向量表示。例如，在 Transformer 模型（如 BERT）中，同一单词根据周围词汇的不同，其嵌入也会有所变化。理想的嵌入应该包含足够的上下文，以保证良好的结果。</font> |
| 增强阶段     | <br>一是当多个检索到的文段包含相似信息时，冗余和重复成为问题，这可能导致生成内容的重复。<br>二是增强过程需要恰当地评估每个文段的价值。<br>三是生成模型可能会过度依赖于增强信息，导致生成的内容仅是重复检索到的信息，而缺乏新的价值或综合信息。 | 在从数据库中检索出有价值的上下文后，将其与查询内容合并输入到大模型会遇到挑战。一次性向大语言模型展示所有相关文档可能会超出其处理的上下文窗口限制。将多个文档拼接成一个冗长的检索提示不仅效率低，还会引入噪声，影响大语言模型聚焦关键信息。因此，需要对检索到的内容进行额外处理，以解决这些问题。<br>**<font style="color:rgb(15, 23, 42);">ReRank：</font>**<font style="color:rgb(15, 23, 42);">重新排序，将最相关的信息置于提示的前后边缘，是一个简单直接的方法。<br></font> **<font style="color:rgb(15, 23, 42);">Prompt压缩：</font>**<font style="color:rgb(15, 23, 42);">研究显示，检索文档中的噪音会对 RAG 性能产生不利影响。在处理的后期阶段，可以计算检索出来的文本块和问题的相关性，将无关的文本块剔除，从而压缩上下文的长度，凸显关键段落。   </font> |
| 生成阶段     | <br>一是制造错误信息，即模型在缺乏足够上下文的情况下虚构答案。<br>二是回答不相关，即模型生成的答案未能针对查询问题。<br>三是生成有害或偏见性回应问题。 | **增强上下文理解**：优化模型，使其能够更准确地理解和处理上下文信息。通过引入更复杂的语义理解和推理机制，模型可以在缺乏直接上下文的情况下，更好地推断出相关信息，从而减少虚构答案的可能性。<br>**引入后处理机制**：在模型生成答案后，引入后处理机制对答案进行验证和修正。这可以包括使用知识图谱、规则库或多智能体来检查答案的准确性和一致性。<br>**增加用户反馈**：允许用户对生成的答案进行反馈和评价，以指导模型在后续生成中更好地匹配用户查询。通过不断学习和优化，模型可以逐渐提高回答问题的准确性和相关性 |

## 五、RAG 的成功案例

RAG 已经在多个领域取得了成功，包括问答系统、对话系统、文档摘要、文档生成等。

我们将在第三部分对 RAG 的应用进行详细介绍。将现有成熟的 RAG 案例进行拆解，和大家一起深入了解 RAG。

1. [Datawhale 知识库助手](https://github.com/logan-zou/Chat_with_Datawhale_langchain) 是结合本课程内容、在由[散步](https://github.com/sanbuphy)打造的 [ChatWithDatawhale](https://github.com/sanbuphy/ChatWithDatawhale)—— Datawhale 内容学习助手的基础上，将架构调整为初学者容易学习的 LangChain 架构，并参考第二章内容对不同源大模型 API 进行封装的 LLM 应用，能够帮助用户与 DataWhale 现有仓库和学习内容流畅对话，从而帮助用户快速找到想学习的内容和可以贡献的内容。


2. [**天机**](https://github.com/SocialAI-tianji/Tianji)是 **SocialAI**（来事儿 AI）制作的一款免费使用、非商业用途的人工智能系统。您可以利用它进行涉及传统人情世故的任务，如如何敬酒、如何说好话、如何会来事儿等，以提升您的情商和核心竞争能力。我们坚信，只有人情世故才是未来AI的核心技术，只有会来事儿的AI才有机会走向AGI，让我们携手见证通用人工智能的来临。 —— "天机不可泄漏。"
---
>【**参考内容**】：

1. [Retrieval-Augmented Generation for Large Language Models: A Survey](https://arxiv.org/abs/2312.10997)
2. [面向大语言模型的检索增强生成技术：综述 [译]](https://baoyu.io/translations/ai-paper/2312.10997-retrieval-augmented-generation-for-large-language-models-a-survey)