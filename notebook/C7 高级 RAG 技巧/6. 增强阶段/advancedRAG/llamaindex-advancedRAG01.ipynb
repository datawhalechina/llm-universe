{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505a940f-80d8-49e2-8be2-2da687b9fc88",
   "metadata": {},
   "source": [
    "# Llama-Index-advancedRAG\n",
    "\n",
    "Llama-Index的核心组件主要由一下这几个组件组成：\n",
    "- 向量存储索引（VectorStoreIndex）**：用于存储文档的向量表示，以便快速检索。\n",
    "- 摘要索引（SummaryIndex）**：存储文档的摘要信息，提供快速预览和检索。\n",
    "- 对象索引（ObjectIndex）**：索引文档中的具体对象或实体，以便进行详细的信息检索。\n",
    "- 查询引擎工具（QueryEngineTool）**：提供强大的查询处理能力，支持复杂的检索任务。\n",
    "- OpenAI代理（OpenAIAgent）**：集成OpenAI技术，增强生成能力和智能交互。\n",
    "- FnRetrieverOpenAI代理（FnRetrieverOpenAIAgent）**：结合检索与生成的功能，提供更丰富的用户体验。\n",
    "\n",
    "- 可以简单跑下如下示例，感受一下llamaindex，\n",
    "  # 以下llamaindex基于当前2024年四月最新版本-V0.10.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "332ca805-6244-4953-a959-863faa71f7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"sk-你的sk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01573f8a-0449-4cf3-ad13-926cdb27faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader('.\\\\data').load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d72a506a-074b-47fa-a9b5-0e20d7b4a05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no information or mention of \"王奇\" in the provided context.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"王奇\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d14f121a-6479-4a0d-aae9-8320fdbc4b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "王奇是本书的作者之一，他在书中提到了乔布斯的例子，强调了学习强化学习的重要性，并分享了乔布斯在大学学习书法课的经历。\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# Either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"说一下王奇\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e5fac1-1328-4361-93ca-b7ca57dd825b",
   "metadata": {},
   "source": [
    "# advanced RAG\n",
    "\n",
    "## 一、Rag fusion\n",
    "\n",
    "### 1、背景\n",
    "\n",
    "​       传统Rag技术能够降低大模型的幻觉并且结合最新的文档知识来进行回答,这种技术的出现，为人工智能和搜索领域带来了一场革命，它通过结合向量检索与生成算法的能力，实现了一种全新的交互方式。尽管如此，这项创新技术也存在一些挑战。因此需要引入高级Rag技术\n",
    "\n",
    "​       在RAG的框架下，搜索环节不可或缺，这一环节本身却面临着诸多难题：提升向量召回的精确度可以结合传统的BM25算法和向量检索技术来共同完成搜索任务。然而，即便如此，有时期望的结果仍可能未被检索到。\n",
    "\n",
    "​        在这样的背景下，我们可以从传统搜广推技术中汲取灵感，寻求可能的改进空间。在实际的检索操作之前，有一个关键步骤是查询词处理与文本理解，这一步也可以运用众多自然语言处理（NLP）技术，如分词、命名实体识别（NER）、同义词处理、词项权重分配、意图识别以及查询词的改写等一系列复杂的NLP任务，现在通过大模型，我们可以相对方便的运用上述方式来增强检索信息的丰富性，从而提高搜索的准确性。\n",
    "\n",
    "​        那么怎么利用大语言模型来优化查询词处理与文本理解这一关键步骤呢，接下来，我会先介绍的基本的RAG-Fusion技术，这种技术正是基于llm对查询query进行改写，来增强检索信息。并且后面还会着重介绍llamaindex实现的几种改进的RAG-Fusion技术。\n",
    "\n",
    "### 2.Rag fusion\n",
    "\n",
    "​       RAG-fusion 简而言之就是生成多个用户查询并对结果重新排序利用倒数排名融合(RRF)和自定义向量得分加权来获得全面、准确的结果。首先利用 LLM 把用户原始查询query翻译成成N个不同但是语义相似的查询query，然后分别进行检索，接着将这多路召回的结果重排生成结果。\n",
    "如下图所示：\n",
    "\n",
    "![rrfsample](.\\rrfsample.png)\n",
    "\n",
    "\n",
    "\n",
    "下面再简单介绍一下倒数排序融合：\n",
    "\n",
    "倒数排序融合（Reciprocal Rank Fusion, RRF）是一种用于融合多个搜索系统或检索系统结果的方法，旨在通过综合多个排名列表来提高信息检索的准确性和效果。这种方法基于一个简单的原理，即假设所有系统的输出都是有价值的，而且对每个系统的贡献给予平等的重视。RRF特别适合于那些需要从不同数据源或搜索引擎中获取信息并将其整合为单一结果列表的场景，例如学术文献搜索、元搜索引擎等。\n",
    "\n",
    "### RRF的工作原理\n",
    "\n",
    "倒数排序融合通过为每个检索结果分配一个基于其在各个系统中排名的倒数得分来工作。然后，对同一结果的所有得分进行汇总，得到一个综合得分，最后根据这些综合得分对所有结果重新排序，以产生最终的融合结果列表。\n",
    "\n",
    "RRF的基本公式为：\n",
    "$$\n",
    "RRF(S) = \\sum_{i=1}^{N} \\frac{1}{k + rank_i(S)}\n",
    "$$\n",
    "\n",
    "- 其中，*S*代表某个具体的搜索结果。\n",
    "- rank*i*(*S*)是搜索结果S*在第i*个系统中的排名。\n",
    "- *N*是系统的总数。\n",
    "- *k*是一个常数，用于调整排名的权重（通常设置为60）。\n",
    "\n",
    "### 示例\n",
    "\n",
    "假设我们有两个搜索系统（A和B），并对同一个查询请求产生了结果。我们想要使用RRF方法来融合这两个系统的结果。以下是每个系统返回的前三个结果及其排名：\n",
    "\n",
    "- **系统A的结果**:\n",
    "  - 文档1: 排名1\n",
    "  - 文档2: 排名2\n",
    "  - 文档3: 排名3\n",
    "- **系统B的结果**:\n",
    "  - 文档2: 排名1\n",
    "  - 文档1: 排名2\n",
    "  - 文档4: 排名3\n",
    "\n",
    "我们假设*k*=60，根据RRF公式计算每个文档的融合得分。\n",
    "\n",
    "- **文档1的得分**: 1/(60+1)+1/(60+2)==0.03265\n",
    "- **文档2的得分**: 1/(60+2)+1/(60+1)==0.03265\n",
    "- **文档3的得分**: 1/(60+3)==0.01613\n",
    "- **文档4的得分**: 1/(60+3)==0.01613\n",
    "\n",
    "最后，我们根据这些融合得分对文档进行重新排序。在这个例子中，文档1和文档2有相同的最高得分，因此它们将共享最高排名位置。文档3和文档4的得分较低，它们将排在后面。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db5a896-2c80-461a-8091-48ced84d8f73",
   "metadata": {},
   "source": [
    "## 01.下面会用llama-index进行实操，首先构建一个字典，包含文件的八个标准问答对，key是问题，value是期望rag的答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "887f555b-42e5-4c7c-a45d-25e3b6a32947",
   "metadata": {},
   "outputs": [],
   "source": [
    "qna_dict = {\n",
    "        \"介绍下SVM算法\": \"是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机\",\n",
    "        \"用通俗的语言介绍下强化学习\": \"监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略\",\n",
    "        \"BN 和 LN 区别\": \"Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。\",\n",
    "        \"讲讲self attention\": \"Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系\",\n",
    "        \"Bert 的预训练过程\": \"Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。\",\n",
    "        \"GPT 与 Bert 的区别\": \"GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。\",\n",
    "        \"pca 属于有监督还是无监督\": \"PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。\",\n",
    "        \"介绍 transformer 算法\": \"Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f4580-fb34-4936-87b1-61cd0af1e491",
   "metadata": {},
   "source": [
    "## 02.然后构建评估prompt和评估函数：\n",
    "### 意在让大模型判断生成的结果和期望的结果是否含义一致，在下面使用各种advanecd-rag方法时会经常调用， 主要使用Llama Packs，Llama Packs是对llamaindex的更加高级的封装，几行代码实现各种高级RAG，如果想看具体的流程也可以直接进入github查看源码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f1fcd4c-75dc-4c77-aeb2-3a99241d7392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "import re\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "eval_template = PromptTemplate(\"\"\"\n",
    "            输入:\n",
    "            Question: {question}\n",
    "            LLM-Answer: {llm_answer}\n",
    "            Expected-Answer: {expected_answer}\n",
    "\n",
    "            任务:\n",
    "            比较LLM-Answer和Expected-Answer，确定它们是否传达类似的含义。\n",
    "\n",
    "            输出:\n",
    "            意思一致：如果LLM-Answer和Expected-Answer传达类似的含义。 \n",
    "            意思不同：如果LLM-Answer和Expected-Answer传达完全不同的含义。\n",
    "\n",
    "            示例： \n",
    "            Question：法国的首都是什么？ \n",
    "            LLM-Answer: 巴黎 \n",
    "            Expected-Answer: 光之城\n",
    "\n",
    "            输出:\n",
    "            意思一致\"\"\")\n",
    "    \n",
    "def eval_llm_output(row, llm):\n",
    "    ''' Eval llm_answer and expected_answer for the question with llm '''\n",
    "    question = row['question']\n",
    "    llm_answer = row['llm_answer']\n",
    "    expected_answer = row['expected_answer']\n",
    "    prompt_message = eval_template.format(question=question, llm_answer=llm_answer, expected_answer=expected_answer)\n",
    "    answer = llm.complete(prompt_message).text\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4522679-4c1e-4ed1-b0d7-0590c626b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93ffde0a-6e17-4a7c-b034-6e33ffe13692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PDFReader\n",
    "from pathlib import Path\n",
    "\n",
    "loader = PDFReader()\n",
    "documents = loader.load_data(\n",
    "        file=Path(f\"face.pdf\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "470b7f4b-900e-469c-b344-70967e16fa6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pdf的页数\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "305c0f3d-b8ac-4307-9888-4fc67a8b092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "node_parser = SimpleNodeParser.from_defaults()\n",
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21a75793-9d7b-4f4c-8373-59462e3e2658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b115a79-a6e3-42f0-85e0-096451f69426",
   "metadata": {},
   "source": [
    "## 混合融合检索HybridFusionRetriever: 原理类似原本的Rag-fusion\n",
    "## 1 根据输入问题Query重写翻译为多个相似语义的Query\n",
    "## 2-接着进行向量Embeding(vector)+关键词搜索(bm25 search)\n",
    "    这里简单说一下bm25：\n",
    "    BM25 是一种在信息检索领域广泛使用的排名函数，主要用于评估文档与用户查询之间的相关性。它是基于概率检索框架的一种改进，通过考虑查询中的词项在文档中的频率（Term Frequency, TF）以及在整个文档集合中的逆文档频率（Inverse Document Frequency, IDF）来计算。\n",
    "### bm25\n",
    "#### 词项频率 (TF)\n",
    "词项在文档中出现的次数。BM25 对 TF 进行了饱和处理，以避免过分强调在文档中频繁出现的词项。\n",
    "#### 逆文档频率 (IDF)\n",
    "衡量一个词项在文档集合中的普遍重要性。如果一个词项在很多文档中都出现，则其 IDF 值会降低，反映其区分文档的能力较弱。\n",
    "#### 文档长度归一化\n",
    "文档的长度会影响词项频率的计算。BM25 通过文档长度归一化来防止长文档被过高估计相关性。\n",
    "#### BM25 得分公式\n",
    "BM25 的得分公式如下所示：\n",
    "$$ \\text{Score}(D, Q) = \\sum_{i=1}^{n} IDF(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}})} $$\n",
    "其中：\n",
    "- $ D $ 是文档，\n",
    "- $ Q $ 是查询，\n",
    "- $ q_i $ 是查询 $ Q $ 中的第 $ i $ 个词项，\n",
    "- $ f(q_i, D) $ 是词项 $ q_i $ 在文档 $ D $ 中的频率，\n",
    "- $ |D| $ 是文档 $ D $ 的长度，\n",
    "- $ \\text{avgdl} $ 是文档集合中文档平均长度，\n",
    "- $ k_1 $ 和 $ b $ 是可调的参数。\n",
    "#### BM25 的优点\n",
    "- **简单高效**：BM25 不需要复杂的预处理，计算速度快。\n",
    "- **广泛应用**：由于其高效性，BM25 在工业界和学术界都有广泛应用。\n",
    "## 3-接着用Reciprocal Rank Fusion(倒数排序融合)对搜索结果进行重排，选择topk结果输入llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4426c5aa-ea7f-466b-bb2e-fff2de5e0f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "\n",
    "HybridFusionRetrieverPack = download_llama_pack(\n",
    "    \"HybridFusionRetrieverPack\",\n",
    "    \"./hybrid_fusion_pack\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2492bb01-6602-428a-a6c6-2c9e7b75a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_fusion_pack = HybridFusionRetrieverPack(\n",
    "    nodes, chunk_size=256, vector_similarity_top_k=2, bm25_similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b57074c5-d14c-4916-bbfc-9afdbdf269ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. SVM算法原理及应用领域介绍\n",
      "2. SVM算法与其他机器学习算法的比较分析\n",
      "3. SVM算法在数据挖掘和模式识别中的应用案例展示\n",
      "Generated queries:\n",
      "1. What is reinforcement learning explained in simple terms?\n",
      "2. Easy explanation of reinforcement learning\n",
      "3. Introduction to reinforcement learning in layman's terms\n",
      "Generated queries:\n",
      "1. BN vs LN comparison\n",
      "2. What are the differences between BN and LN?\n",
      "3. Features of BN and LN compared\n",
      "Generated queries:\n",
      "1. What is self attention in neural networks?\n",
      "2. How does self attention improve natural language processing models?\n",
      "3. Examples of self attention mechanisms in deep learning architectures.\n",
      "Generated queries:\n",
      "1. Bert 预训练模型的优势和劣势\n",
      "2. Bert 预训练过程中使用的数据集\n",
      "3. Bert 预训练模型的应用领域和效果\n",
      "Generated queries:\n",
      "1. GPT vs Bert: 什么是GPT和Bert，它们之间有什么区别？\n",
      "2. GPT和Bert的不同之处是什么？\n",
      "3. GPT和Bert在自然语言处理中有何异同？\n",
      "Generated queries:\n",
      "1. What is the difference between supervised and unsupervised learning in machine learning?\n",
      "2. Examples of supervised learning algorithms in machine learning.\n",
      "3. How does principal component analysis (PCA) work in unsupervised learning?\n",
      "Generated queries:\n",
      "1. Transformer算法原理介绍\n",
      "2. Transformer算法应用领域\n",
      "3. Transformer算法与其他深度学习算法比较\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import os\n",
    "from getpass import getpass\n",
    "answers = {'question':[], 'llm_answer': [], 'expected_answer': []}\n",
    "\n",
    "for question in qna_dict.keys():\n",
    "    expected_answer = qna_dict[question]\n",
    "    # result\n",
    "    response = hybrid_fusion_pack.run(question)\n",
    "    answers['question'].append(question)\n",
    "    answers['llm_answer'].append(response.response)\n",
    "    answers['expected_answer'].append(expected_answer)\n",
    "\n",
    "answers_df = pd.DataFrame(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "34ffd6e2-7196-432f-87bc-0fed28ed85e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>llm_answer</th>\n",
       "      <th>expected_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>介绍下SVM算法</td>\n",
       "      <td>SVM is a binary classification model that defines the maximum margin linear classifier in the feature space. It distinguishes itself from the perceptron by maximizing the margin. SVM can be categorized into three types: linear separable SVM for linearly separable training data, linear SVM for data that is not linearly separable but approximately so, and non-linear SVM for data that is not linearly separable. The learning strategy of SVM involves maximizing the margin, which can be formalized as solving a convex quadratic programming problem or minimizing the regularized hinge loss function. The learning algorithm of SVM focuses on solving convex quadratic programming optimization. Additionally, the choice of kernel function in SVM includes Linear kernel for linearly separable cases and RBF kernel for non-linearly separable cases, with the latter requiring parameter tuning through methods like cross-validation.</td>\n",
       "      <td>是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>用通俗的语言介绍下强化学习</td>\n",
       "      <td>强化学习是一种机器学习方法，通过代理程序在与环境的交互中学习如何做出一系列决策，以使得获得的累积奖励最大化。在这个过程中，代理程序会根据环境的反馈调整自己的行为，从而逐渐学会选择最优的行动策略。强化学习的目标是使代理程序能够在不断尝试和学习中获得最大的长期回报。</td>\n",
       "      <td>监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BN 和 LN 区别</td>\n",
       "      <td>BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本的所有维度特征做归一化。LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。因此，LN不依赖于batch的大小和输入sequence的长度，可以用于batchsize为1和RNN中sequence的normalize操作。</td>\n",
       "      <td>Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>讲讲self attention</td>\n",
       "      <td>SelfAttention is a mechanism that differs significantly from traditional Attention. While traditional Attention calculates dependencies between hidden states of the source and target ends, SelfAttention operates separately on the source and target ends. It focuses solely on the self-relevance of the source or target input, capturing dependencies between words within the source or target end. The self-attention results from the source end are then incorporated into the attention obtained from the target end, capturing dependencies between words in both the source and target ends. This unique approach allows SelfAttention to capture dependencies not only between words in the source and target ends but also within the source or target end itself, making it more effective than traditional Attention mechanisms.</td>\n",
       "      <td>Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bert 的预训练过程</td>\n",
       "      <td>BERT的预训练过程使用了LAMB优化器，该优化器适用于小批量和大批量训练，并且不需要调整除学习率以外的其他超参数。通过LAMB优化器，BERT的预训练批量大小可以扩展到64K，而不会损失准确率，从而使BERT的训练过程可以在76分钟内完成。</td>\n",
       "      <td>Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GPT 与 Bert 的区别</td>\n",
       "      <td>GPT是单向模型，只能利用上文信息，而BERT是双向模型，可以同时利用上下文信息。</td>\n",
       "      <td>GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pca 属于有监督还是无监督</td>\n",
       "      <td>PCA belongs to unsupervised learning.</td>\n",
       "      <td>PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>介绍 transformer 算法</td>\n",
       "      <td>Transformer算法是一个典型的encoder-decoder模型，其中包括Encoder端和Decoder端。Encoder端由多个Block组成，包括多头self-attention模块和前馈神经网络模块；Decoder端也由多个Block组成，包括多头self-attention模块、多头Encoder-Decoder attention交互模块和前馈神经网络模块。在Transformer中，每个模块都包含残差层和Layer Normalization层，用于提高模型的训练效果和准确性。</td>\n",
       "      <td>Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            question  \\\n",
       "0           介绍下SVM算法   \n",
       "1      用通俗的语言介绍下强化学习   \n",
       "2         BN 和 LN 区别   \n",
       "3   讲讲self attention   \n",
       "4        Bert 的预训练过程   \n",
       "5     GPT 与 Bert 的区别   \n",
       "6     pca 属于有监督还是无监督   \n",
       "7  介绍 transformer 算法   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    llm_answer  \\\n",
       "0  SVM is a binary classification model that defines the maximum margin linear classifier in the feature space. It distinguishes itself from the perceptron by maximizing the margin. SVM can be categorized into three types: linear separable SVM for linearly separable training data, linear SVM for data that is not linearly separable but approximately so, and non-linear SVM for data that is not linearly separable. The learning strategy of SVM involves maximizing the margin, which can be formalized as solving a convex quadratic programming problem or minimizing the regularized hinge loss function. The learning algorithm of SVM focuses on solving convex quadratic programming optimization. Additionally, the choice of kernel function in SVM includes Linear kernel for linearly separable cases and RBF kernel for non-linearly separable cases, with the latter requiring parameter tuning through methods like cross-validation.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          强化学习是一种机器学习方法，通过代理程序在与环境的交互中学习如何做出一系列决策，以使得获得的累积奖励最大化。在这个过程中，代理程序会根据环境的反馈调整自己的行为，从而逐渐学会选择最优的行动策略。强化学习的目标是使代理程序能够在不断尝试和学习中获得最大的长期回报。   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本的所有维度特征做归一化。LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。因此，LN不依赖于batch的大小和输入sequence的长度，可以用于batchsize为1和RNN中sequence的normalize操作。   \n",
       "3                                                                                                            SelfAttention is a mechanism that differs significantly from traditional Attention. While traditional Attention calculates dependencies between hidden states of the source and target ends, SelfAttention operates separately on the source and target ends. It focuses solely on the self-relevance of the source or target input, capturing dependencies between words within the source or target end. The self-attention results from the source end are then incorporated into the attention obtained from the target end, capturing dependencies between words in both the source and target ends. This unique approach allows SelfAttention to capture dependencies not only between words in the source and target ends but also within the source or target end itself, making it more effective than traditional Attention mechanisms.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    BERT的预训练过程使用了LAMB优化器，该优化器适用于小批量和大批量训练，并且不需要调整除学习率以外的其他超参数。通过LAMB优化器，BERT的预训练批量大小可以扩展到64K，而不会损失准确率，从而使BERT的训练过程可以在76分钟内完成。   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    GPT是单向模型，只能利用上文信息，而BERT是双向模型，可以同时利用上下文信息。   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        PCA belongs to unsupervised learning.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Transformer算法是一个典型的encoder-decoder模型，其中包括Encoder端和Decoder端。Encoder端由多个Block组成，包括多头self-attention模块和前馈神经网络模块；Decoder端也由多个Block组成，包括多头self-attention模块、多头Encoder-Decoder attention交互模块和前馈神经网络模块。在Transformer中，每个模块都包含残差层和Layer Normalization层，用于提高模型的训练效果和准确性。   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          expected_answer  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机  \n",
       "1                                                                                                                                                               监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略  \n",
       "2                                                                                                                                                                                                                                                                                        Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。  \n",
       "3  Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系  \n",
       "4                                                                                                                                                                                                                                                                                                             Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                               GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。  \n",
       "7                                                                                                                                                                                                                                                                Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(answers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "27bcc5a3-5c9b-4ec2-8337-25c43f6d10a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval llm-answer with expected-answer\n",
    "answers_df['rag_eval_results'] = answers_df.apply(lambda r: eval_llm_output(r, llm), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4d2baf19-c2f3-4b62-a3eb-77984f6feb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>llm_answer</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>rag_eval_results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>介绍下SVM算法</td>\n",
       "      <td>SVM is a binary classification model that defines the maximum margin linear classifier in the feature space. It distinguishes itself from the perceptron by maximizing the margin. SVM can be categorized into three types: linear separable SVM for linearly separable training data, linear SVM for data that is not linearly separable but approximately so, and non-linear SVM for data that is not linearly separable. The learning strategy of SVM involves maximizing the margin, which can be formalized as solving a convex quadratic programming problem or minimizing the regularized hinge loss function. The learning algorithm of SVM focuses on solving convex quadratic programming optimization. Additionally, the choice of kernel function in SVM includes Linear kernel for linearly separable cases and RBF kernel for non-linearly separable cases, with the latter requiring parameter tuning through methods like cross-validation.</td>\n",
       "      <td>是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都在介绍SVM算法是一种二分类模型，定义了特征空间上的最大间隔线性分类器。两者都提到了SVM通过最大化间隔来区分自己与感知机的不同之处。因此，它们传达的含义是类似的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>用通俗的语言介绍下强化学习</td>\n",
       "      <td>强化学习是一种机器学习方法，通过代理程序在与环境的交互中学习如何做出一系列决策，以使得获得的累积奖励最大化。在这个过程中，代理程序会根据环境的反馈调整自己的行为，从而逐渐学会选择最优的行动策略。强化学习的目标是使代理程序能够在不断尝试和学习中获得最大的长期回报。</td>\n",
       "      <td>监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BN 和 LN 区别</td>\n",
       "      <td>BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本的所有维度特征做归一化。LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。因此，LN不依赖于batch的大小和输入sequence的长度，可以用于batchsize为1和RNN中sequence的normalize操作。</td>\n",
       "      <td>Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer传达的含义是一致的，只是表达方式略有不同。都在解释BatchNormalization和LayerNormalization的区别，以及它们各自的特点。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>讲讲self attention</td>\n",
       "      <td>SelfAttention is a mechanism that differs significantly from traditional Attention. While traditional Attention calculates dependencies between hidden states of the source and target ends, SelfAttention operates separately on the source and target ends. It focuses solely on the self-relevance of the source or target input, capturing dependencies between words within the source or target end. The self-attention results from the source end are then incorporated into the attention obtained from the target end, capturing dependencies between words in both the source and target ends. This unique approach allows SelfAttention to capture dependencies not only between words in the source and target ends but also within the source or target end itself, making it more effective than traditional Attention mechanisms.</td>\n",
       "      <td>Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系</td>\n",
       "      <td>意思不同。LLM-Answer和Expected-Answer虽然都在讲Self Attention与传统Attention的区别，但表达方式和详细内容有所不同。LLM-Answer更加强调Self Attention的独特之处和优势，而Expected-Answer则更加详细地解释了Self Attention的工作原理和优势。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bert 的预训练过程</td>\n",
       "      <td>BERT的预训练过程使用了LAMB优化器，该优化器适用于小批量和大批量训练，并且不需要调整除学习率以外的其他超参数。通过LAMB优化器，BERT的预训练批量大小可以扩展到64K，而不会损失准确率，从而使BERT的训练过程可以在76分钟内完成。</td>\n",
       "      <td>Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GPT 与 Bert 的区别</td>\n",
       "      <td>GPT是单向模型，只能利用上文信息，而BERT是双向模型，可以同时利用上下文信息。</td>\n",
       "      <td>GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都指出了GPT是单向模型，无法利用上下文信息，只能利用上文；而BERT是双向模型。同时也提到了GPT是基于自回归模型，可以应用在NLU和NLG两大任务，而BERT是基于自编码模型，只能完成NLU任务，无法直接应用在文本生成上面。因此，它们传达的含义是相似的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pca 属于有监督还是无监督</td>\n",
       "      <td>PCA belongs to unsupervised learning.</td>\n",
       "      <td>PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都表明PCA属于无监督学习。虽然LLM-Answer没有提到数据集中y的重要性，但两者都强调了PCA是用来改变样本X的属性维度的。因此，它们传达的含义是类似的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>介绍 transformer 算法</td>\n",
       "      <td>Transformer算法是一个典型的encoder-decoder模型，其中包括Encoder端和Decoder端。Encoder端由多个Block组成，包括多头self-attention模块和前馈神经网络模块；Decoder端也由多个Block组成，包括多头self-attention模块、多头Encoder-Decoder attention交互模块和前馈神经网络模块。在Transformer中，每个模块都包含残差层和Layer Normalization层，用于提高模型的训练效果和准确性。</td>\n",
       "      <td>Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都在介绍Transformer算法的结构和组成部分，虽然表达方式略有不同，但传达的含义是相似的。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            question  \\\n",
       "0           介绍下SVM算法   \n",
       "1      用通俗的语言介绍下强化学习   \n",
       "2         BN 和 LN 区别   \n",
       "3   讲讲self attention   \n",
       "4        Bert 的预训练过程   \n",
       "5     GPT 与 Bert 的区别   \n",
       "6     pca 属于有监督还是无监督   \n",
       "7  介绍 transformer 算法   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    llm_answer  \\\n",
       "0  SVM is a binary classification model that defines the maximum margin linear classifier in the feature space. It distinguishes itself from the perceptron by maximizing the margin. SVM can be categorized into three types: linear separable SVM for linearly separable training data, linear SVM for data that is not linearly separable but approximately so, and non-linear SVM for data that is not linearly separable. The learning strategy of SVM involves maximizing the margin, which can be formalized as solving a convex quadratic programming problem or minimizing the regularized hinge loss function. The learning algorithm of SVM focuses on solving convex quadratic programming optimization. Additionally, the choice of kernel function in SVM includes Linear kernel for linearly separable cases and RBF kernel for non-linearly separable cases, with the latter requiring parameter tuning through methods like cross-validation.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          强化学习是一种机器学习方法，通过代理程序在与环境的交互中学习如何做出一系列决策，以使得获得的累积奖励最大化。在这个过程中，代理程序会根据环境的反馈调整自己的行为，从而逐渐学会选择最优的行动策略。强化学习的目标是使代理程序能够在不断尝试和学习中获得最大的长期回报。   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本的所有维度特征做归一化。LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。因此，LN不依赖于batch的大小和输入sequence的长度，可以用于batchsize为1和RNN中sequence的normalize操作。   \n",
       "3                                                                                                            SelfAttention is a mechanism that differs significantly from traditional Attention. While traditional Attention calculates dependencies between hidden states of the source and target ends, SelfAttention operates separately on the source and target ends. It focuses solely on the self-relevance of the source or target input, capturing dependencies between words within the source or target end. The self-attention results from the source end are then incorporated into the attention obtained from the target end, capturing dependencies between words in both the source and target ends. This unique approach allows SelfAttention to capture dependencies not only between words in the source and target ends but also within the source or target end itself, making it more effective than traditional Attention mechanisms.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    BERT的预训练过程使用了LAMB优化器，该优化器适用于小批量和大批量训练，并且不需要调整除学习率以外的其他超参数。通过LAMB优化器，BERT的预训练批量大小可以扩展到64K，而不会损失准确率，从而使BERT的训练过程可以在76分钟内完成。   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    GPT是单向模型，只能利用上文信息，而BERT是双向模型，可以同时利用上下文信息。   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        PCA belongs to unsupervised learning.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Transformer算法是一个典型的encoder-decoder模型，其中包括Encoder端和Decoder端。Encoder端由多个Block组成，包括多头self-attention模块和前馈神经网络模块；Decoder端也由多个Block组成，包括多头self-attention模块、多头Encoder-Decoder attention交互模块和前馈神经网络模块。在Transformer中，每个模块都包含残差层和Layer Normalization层，用于提高模型的训练效果和准确性。   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          expected_answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机   \n",
       "1                                                                                                                                                               监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略   \n",
       "2                                                                                                                                                                                                                                                                                        Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。   \n",
       "3  Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系   \n",
       "4                                                                                                                                                                                                                                                                                                             Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                               GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。   \n",
       "7                                                                                                                                                                                                                                                                Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。   \n",
       "\n",
       "                                                                                                                                                         rag_eval_results  \n",
       "0                                                      意思一致。LLM-Answer和Expected-Answer都在介绍SVM算法是一种二分类模型，定义了特征空间上的最大间隔线性分类器。两者都提到了SVM通过最大化间隔来区分自己与感知机的不同之处。因此，它们传达的含义是类似的。  \n",
       "1                                                                                                                                                                    意思不同  \n",
       "2                                                             意思一致。LLM-Answer和Expected-Answer传达的含义是一致的，只是表达方式略有不同。都在解释BatchNormalization和LayerNormalization的区别，以及它们各自的特点。  \n",
       "3  意思不同。LLM-Answer和Expected-Answer虽然都在讲Self Attention与传统Attention的区别，但表达方式和详细内容有所不同。LLM-Answer更加强调Self Attention的独特之处和优势，而Expected-Answer则更加详细地解释了Self Attention的工作原理和优势。  \n",
       "4                                                                                                                                                                    意思不同  \n",
       "5        意思一致。LLM-Answer和Expected-Answer都指出了GPT是单向模型，无法利用上下文信息，只能利用上文；而BERT是双向模型。同时也提到了GPT是基于自回归模型，可以应用在NLU和NLG两大任务，而BERT是基于自编码模型，只能完成NLU任务，无法直接应用在文本生成上面。因此，它们传达的含义是相似的。  \n",
       "6                                                         意思一致。LLM-Answer和Expected-Answer都表明PCA属于无监督学习。虽然LLM-Answer没有提到数据集中y的重要性，但两者都强调了PCA是用来改变样本X的属性维度的。因此，它们传达的含义是类似的。  \n",
       "7                                                                                         意思一致。LLM-Answer和Expected-Answer都在介绍Transformer算法的结构和组成部分，虽然表达方式略有不同，但传达的含义是相似的。  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(answers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "96f99ecc-3d5a-44fe-a29c-6e4e0416025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_df.to_excel('hybrid_fusion_pack_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc671db-14aa-410c-9314-1d74150762ee",
   "metadata": {},
   "source": [
    "## 重写查询检索Query Rewriting Retriever\n",
    "### 1 根据输入问题Query重写翻译为多个相似语义的Query，默认生成多个\n",
    "### 2-接着进行向量Embeding(vector)搜索 \n",
    "### 3-接着用Reciprocal Rank Fusion(倒数排序融合)对搜索结果进行重排，选择topk结果输入llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "772ba757-123a-4db8-a0a1-afcc5d83629d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and Initialize Pack\n",
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "\n",
    "QueryRewritingRetrieverPack = download_llama_pack(\n",
    "    \"QueryRewritingRetrieverPack\",\n",
    "    \"./query_rewriting_pack\",\n",
    "    # leave the below commented out (was for testing purposes)\n",
    "    # llama_hub_url=\"https://raw.githubusercontent.com/run-llama/llama-hub/jerry/add_llama_packs/llama_hub\",\n",
    ")\n",
    "\n",
    "query_rewriting_pack = QueryRewritingRetrieverPack(\n",
    "    nodes,\n",
    "    chunk_size=256,\n",
    "    vector_similarity_top_k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7e40205a-78d2-4a35-bb5a-5daa595e197b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. SVM算法原理及应用领域介绍\n",
      "2. SVM算法与其他机器学习算法的比较分析\n",
      "3. SVM算法在数据挖掘和模式识别中的实际应用案例\n",
      "Generated queries:\n",
      "1. What is reinforcement learning explained in simple terms?\n",
      "2. Explain reinforcement learning in layman's terms\n",
      "3. Introduction to reinforcement learning for beginners\n",
      "Generated queries:\n",
      "1. BN vs LN comparison\n",
      "2. Key differences between BN and LN\n",
      "3. What sets BN apart from LN?\n",
      "Generated queries:\n",
      "1. What is self attention in deep learning?\n",
      "2. How does self attention improve natural language processing models?\n",
      "3. Examples of self attention mechanisms in neural networks.\n",
      "Generated queries:\n",
      "1. Bert 预训练模型有哪些？\n",
      "2. Bert 预训练算法是如何工作的？\n",
      "3. Bert 预训练的优势和应用领域是什么？\n",
      "Generated queries:\n",
      "1. GPT vs Bert: 语言模型的不同之处\n",
      "2. GPT 和 Bert 在自然语言处理中的应用比较\n",
      "3. GPT 和 Bert 的性能对比研究结果\n",
      "Generated queries:\n",
      "1. 什么是主成分分析（PCA）？\n",
      "2. PCA如何在有监督学习中使用？\n",
      "3. PCA和无监督学习的区别是什么？\n",
      "Generated queries:\n",
      "1. Transformer算法原理介绍\n",
      "2. Transformer算法应用领域\n",
      "3. Transformer算法与其他深度学习算法比较\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "answers = {'question':[], 'llm_answer': [], 'expected_answer': []}\n",
    "\n",
    "for question in qna_dict.keys():\n",
    "    expected_answer = qna_dict[question]\n",
    "    # result\n",
    "    response = query_rewriting_pack.run(question)\n",
    "    answers['question'].append(question)\n",
    "    answers['llm_answer'].append(response.response)\n",
    "    answers['expected_answer'].append(expected_answer)\n",
    "\n",
    "answers_df = pd.DataFrame(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2e153ff7-88f4-4e8b-a432-c3adce0dc606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>llm_answer</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>rag_eval_results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>介绍下SVM算法</td>\n",
       "      <td>SVM算法是一种二分类模型，其基本模型是定义在特征空间上的间隔最大的线性分类器。SVM可分为三种：线性可分SVM、线性SVM和非线性SVM。线性可分SVM适用于训练数据线性可分的情况，通过最大化硬间隔学习得到线性分类器；线性SVM适用于训练数据不能线性可分但近似线性可分的情况，通过最大化软间隔学习得到线性分类器；非线性SVM适用于训练数据线性不可分的情况，通过核技巧和最大化软间隔学习得到非线性分类器。</td>\n",
       "      <td>是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>用通俗的语言介绍下强化学习</td>\n",
       "      <td>强化学习是一种机器学习方法，就像教小孩子学习一样。在强化学习中，模型通过尝试不同的行动来学习，然后根据行动的结果来做出调整。这种学习方式就像小孩子通过尝试不同的行为来学习什么是对的，什么是错的一样。强化学习的目标是让模型在特定环境中学会采取最优的行动，以获得最大的奖励。</td>\n",
       "      <td>监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BN 和 LN 区别</td>\n",
       "      <td>BatchNormalization是对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差；LayerNormalization是对单个样本的所有维度特征做归一化，同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差。 LN不依赖于batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操作。</td>\n",
       "      <td>Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>讲讲self attention</td>\n",
       "      <td>Transformer模型中的self-attention机制是一种注意力机制，用于计算输入序列中每个位置的表示与其他位置的关联程度。在self-attention中，每个输入位置都会与其他位置进行加权组合，以便更好地捕捉输入序列内部的依赖关系。这种机制允许模型在处理序列数据时聚焦于不同位置之间的交互，有助于提高模型对长距离依赖关系的建模能力。</td>\n",
       "      <td>Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bert 的预训练过程</td>\n",
       "      <td>During the pre-training process of Bert, two main objectives are utilized: Masked Language Model (MLM) and Next Sentence Prediction (NSP). The model is trained to predict masked words within a sentence using the MLM task, where random tokens are masked and the model is tasked with predicting them. Additionally, the NSP task involves determining whether a given pair of sentences appear consecutively in the original text or not. These two objectives are jointly trained to minimize a combined loss function, enhancing the model's ability to understand and generate language.</td>\n",
       "      <td>Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都在描述Bert的预训练过程，提到了MLM和NSP这两个主要任务，以及它们的具体实现方式和训练过程。两者传达的信息是相似的，只是表达方式略有不同。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GPT 与 Bert 的区别</td>\n",
       "      <td>GPT是单向模型，只能利用上文信息，而BERT是双向模型。 GPT基于自回归模型，适用于NLU和NLG任务，而BERT基于自编码模型，主要用于NLU任务，无法直接应用在文本生成上。</td>\n",
       "      <td>GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都提到了GPT是单向模型，无法利用上下文信息，只能利用上文；而BERT是双向模型。同时也提到了GPT基于自回归模型，适用于NLU和NLG任务，而BERT基于自编码模型，主要用于NLU任务，无法直接应用在文本生成上。因此它们传达的含义是相似的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pca 属于有监督还是无监督</td>\n",
       "      <td>PCA belongs to unsupervised learning.</td>\n",
       "      <td>PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都表明PCA属于无监督学习。虽然LLM-Answer没有提到数据集中y的重要性，但两者都强调PCA是用来改变样本X的属性维度的。因此，它们传达的含义是类似的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>介绍 transformer 算法</td>\n",
       "      <td>Transformer算法是一个典型的encoder-decoder模型，其中包括Encoder端和Decoder端，每个端都由多个Block组成。Encoder端包括多头self-attention模块和前馈神经网络模块，而Decoder端包括多头self-attention模块、多头Encoder-Decoder attention交互模块和前馈神经网络模块。在Transformer中，每个模块都包含残差连接和Layer Normalization层，这些组件共同构成了Transformer模型的核心结构。</td>\n",
       "      <td>Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。</td>\n",
       "      <td>：Transformer算法的LLM-Answer和Expected-Answer传达的含义是一致的，都在介绍Transformer模型的结构和组成部分。因此，输出为意思一致。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            question  \\\n",
       "0           介绍下SVM算法   \n",
       "1      用通俗的语言介绍下强化学习   \n",
       "2         BN 和 LN 区别   \n",
       "3   讲讲self attention   \n",
       "4        Bert 的预训练过程   \n",
       "5     GPT 与 Bert 的区别   \n",
       "6     pca 属于有监督还是无监督   \n",
       "7  介绍 transformer 算法   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         llm_answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                        SVM算法是一种二分类模型，其基本模型是定义在特征空间上的间隔最大的线性分类器。SVM可分为三种：线性可分SVM、线性SVM和非线性SVM。线性可分SVM适用于训练数据线性可分的情况，通过最大化硬间隔学习得到线性分类器；线性SVM适用于训练数据不能线性可分但近似线性可分的情况，通过最大化软间隔学习得到线性分类器；非线性SVM适用于训练数据线性不可分的情况，通过核技巧和最大化软间隔学习得到非线性分类器。   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                           强化学习是一种机器学习方法，就像教小孩子学习一样。在强化学习中，模型通过尝试不同的行动来学习，然后根据行动的结果来做出调整。这种学习方式就像小孩子通过尝试不同的行为来学习什么是对的，什么是错的一样。强化学习的目标是让模型在特定环境中学会采取最优的行动，以获得最大的奖励。   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                        BatchNormalization是对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差；LayerNormalization是对单个样本的所有维度特征做归一化，同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差。 LN不依赖于batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操作。   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                     Transformer模型中的self-attention机制是一种注意力机制，用于计算输入序列中每个位置的表示与其他位置的关联程度。在self-attention中，每个输入位置都会与其他位置进行加权组合，以便更好地捕捉输入序列内部的依赖关系。这种机制允许模型在处理序列数据时聚焦于不同位置之间的交互，有助于提高模型对长距离依赖关系的建模能力。   \n",
       "4  During the pre-training process of Bert, two main objectives are utilized: Masked Language Model (MLM) and Next Sentence Prediction (NSP). The model is trained to predict masked words within a sentence using the MLM task, where random tokens are masked and the model is tasked with predicting them. Additionally, the NSP task involves determining whether a given pair of sentences appear consecutively in the original text or not. These two objectives are jointly trained to minimize a combined loss function, enhancing the model's ability to understand and generate language.   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        GPT是单向模型，只能利用上文信息，而BERT是双向模型。 GPT基于自回归模型，适用于NLU和NLG任务，而BERT基于自编码模型，主要用于NLU任务，无法直接应用在文本生成上。   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             PCA belongs to unsupervised learning.   \n",
       "7                                                                                                                                                                                                                                                                                                                                Transformer算法是一个典型的encoder-decoder模型，其中包括Encoder端和Decoder端，每个端都由多个Block组成。Encoder端包括多头self-attention模块和前馈神经网络模块，而Decoder端包括多头self-attention模块、多头Encoder-Decoder attention交互模块和前馈神经网络模块。在Transformer中，每个模块都包含残差连接和Layer Normalization层，这些组件共同构成了Transformer模型的核心结构。   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          expected_answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机   \n",
       "1                                                                                                                                                               监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略   \n",
       "2                                                                                                                                                                                                                                                                                        Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。   \n",
       "3  Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系   \n",
       "4                                                                                                                                                                                                                                                                                                             Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                               GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。   \n",
       "7                                                                                                                                                                                                                                                                Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。   \n",
       "\n",
       "                                                                                                                                           rag_eval_results  \n",
       "0                                                                                                                                                      意思不同  \n",
       "1                                                                                                                                                      意思不同  \n",
       "2                                                                                                                                                      意思不同  \n",
       "3                                                                                                                                                      意思不同  \n",
       "4                                                  意思一致。LLM-Answer和Expected-Answer都在描述Bert的预训练过程，提到了MLM和NSP这两个主要任务，以及它们的具体实现方式和训练过程。两者传达的信息是相似的，只是表达方式略有不同。  \n",
       "5  意思一致。LLM-Answer和Expected-Answer都提到了GPT是单向模型，无法利用上下文信息，只能利用上文；而BERT是双向模型。同时也提到了GPT基于自回归模型，适用于NLU和NLG任务，而BERT基于自编码模型，主要用于NLU任务，无法直接应用在文本生成上。因此它们传达的含义是相似的。  \n",
       "6                                            意思一致。LLM-Answer和Expected-Answer都表明PCA属于无监督学习。虽然LLM-Answer没有提到数据集中y的重要性，但两者都强调PCA是用来改变样本X的属性维度的。因此，它们传达的含义是类似的。  \n",
       "7                                                                  ：Transformer算法的LLM-Answer和Expected-Answer传达的含义是一致的，都在介绍Transformer模型的结构和组成部分。因此，输出为意思一致。  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eval llm-answer with expected-answer\n",
    "answers_df['rag_eval_results'] = answers_df.apply(lambda r: eval_llm_output(r, llm), axis=1)\n",
    "\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(answers_df)\n",
    "    \n",
    "answers_df.to_excel('query_rewriting_pack_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1122fcb-3981-4039-bedc-b1641786f457",
   "metadata": {},
   "source": [
    "# AutoMergingRetriever自动合并检索\n",
    "   先说一下文档层次结构概念：这是一种在切割文档时按照特定层次进行的方法。\n",
    "   层次结构包括广度和深度两个维度，例如，一个长文档可以看作是由多个章节组成，每个章节由多个段落组成，每个段落进一步分割成多个句子。在这里，章节、段落和句子之间的相互关系分别代表了不同的层次关系。\n",
    "## 自动合并检索\n",
    "   自动合并检索技术将文档按照层次结构切割成不同的块，然后在检索时，将最小的结构单元（叶子节点）与查询问题进行相似度匹配。如果一个父节点上的多数叶子节点与问题匹配，则父节点文档作为上下文（context）返回给大型语言模型（LLM）。\n",
    "   自动合并检索技术通过文档层次结构和自动合并的方法提高了检索的准确性和效率。\n",
    "## 1-加载文档建立分层节点图（父节点+子节点),会把大块文档放在父节点，并且将大块文档分割之后的小文档放在子节点\n",
    "## 2查询的时候检索子节点，当查询的子节点数量足够多的时候即同一个父节点下检索子节点达到给定阈值则返回此父文档，否则不返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2962bddd-d644-40d6-9d20-b8ae376b12a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "\n",
    "AutoMergingRetrieverPack = download_llama_pack(\n",
    "    \"AutoMergingRetrieverPack\",\n",
    "    \"./auto_merging_retriever_pack\",\n",
    "    # leave the below commented out (was for testing purposes)\n",
    "    # llama_hub_url=\"https://raw.githubusercontent.com/run-llama/llama-hub/jerry/add_llama_packs/llama_hub\"\n",
    ")\n",
    "auto_merging_pack = AutoMergingRetrieverPack(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ef1fdce1-a638-4452-9c7b-16078c094add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Merging 3 nodes into parent node.\n",
      "> Parent node id: 0349a78c-e46c-4eb0-a93f-980ca83956cd.\n",
      "> Parent node text: 第4页共46页第十七篇：2022年4月10日百度机器学习方向暑期实习面试题6道\n",
      "1、介绍下SVM算法\n",
      "是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感...\n",
      "\n",
      "> Merging 2 nodes into parent node.\n",
      "> Parent node id: 5d6a3c28-7aeb-4ee0-8d51-fd30107080e0.\n",
      "> Parent node text: 3、介绍svm算法\n",
      "是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感\n",
      "知机。\n",
      "SVM可分为三种：\n",
      "线性可分SVM\n",
      "当训练数据线性可分时，通过最大化硬间...\n",
      "\n",
      "> Merging 1 nodes into parent node.\n",
      "> Parent node id: 9c3f958c-7739-4b5f-98df-217ed9a30f0c.\n",
      "> Parent node text: 4、传统机器学习方法了解多少\n",
      "这个可以说逻辑回归模型，逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度\n",
      "下降法来求解参数，从而达到将数据二分类的目的。\n",
      "逻辑回归的优缺点\n",
      "优点：\n",
      "\n",
      "> Merging 1 nodes into parent node.\n",
      "> Parent node id: 5b4a0423-6125-4399-954a-33b99380e478.\n",
      "> Parent node text: LAMB可将BERT预训练的批量大小扩展到64K，且不会造成准确率损失，76分钟就可以完成BERT的训\n",
      "练。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "answers = {'question':[], 'llm_answer': [], 'expected_answer': []}\n",
    "\n",
    "for question in qna_dict.keys():\n",
    "    expected_answer = qna_dict[question]\n",
    "    # result\n",
    "    response = auto_merging_pack.run(question)\n",
    "    answers['question'].append(question)\n",
    "    answers['llm_answer'].append(response.response)\n",
    "    answers['expected_answer'].append(expected_answer)\n",
    "\n",
    "answers_df = pd.DataFrame(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6fe2a8bd-4e41-401b-acc7-10e8cdfff345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>llm_answer</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>rag_eval_results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>介绍下SVM算法</td>\n",
       "      <td>SVM算法是一种二分类模型，其基本模型是定义在特征空间上的间隔最大的线性分类器，通过最大化硬间隔可以学习得到一个线性分类器，即硬间隔SVM。当训练数据不能线性可分但是近似线性可分时，可以通过最大化软间隔学习到一个线性分类器，即软间隔SVM。此外，当训练数据线性不可分时，可以通过使用核技巧和最大化软间隔学习到一个非线性SVM。</td>\n",
       "      <td>是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都在介绍SVM算法是一种二分类模型，其基本模型是定义在特征空间上的间隔最大的线性分类器。虽然LLM-Answer中提到了硬间隔SVM、软间隔SVM和非线性SVM的概念，但整体来看，它们传达的含义是相似的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>用通俗的语言介绍下强化学习</td>\n",
       "      <td>Reinforcement Learning can be explained in simple terms as a learning process where an agent learns to make decisions by receiving feedback in the form of rewards or penalties based on its actions. It is like a trial-and-error method where the agent learns through experience by interacting with its environment.</td>\n",
       "      <td>监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BN 和 LN 区别</td>\n",
       "      <td>LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。</td>\n",
       "      <td>Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都在解释BN和LN的区别，提到了LN是对单个样本的所有维度特征做归一化，而BN是对不同神经元输入计算均值和方差。两者都强调了LN中同层神经元输入拥有相同的均值和方差，而BN中同一个batch中的输入拥有相同的均值和方差。因此，虽然表达方式有些许不同，但传达的含义是一致的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>讲讲self attention</td>\n",
       "      <td>Self attention is a mechanism that differs from traditional attention in how it operates. While traditional attention calculates dependencies between hidden states of the source and target sides, self attention focuses on capturing dependencies within the source or target input itself. By separately considering the source and target sides, self attention captures relationships between words within each side before incorporating this information into the overall attention mechanism. This approach allows self attention to effectively model dependencies within each side and between the source and target sides of the input.</td>\n",
       "      <td>Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bert 的预训练过程</td>\n",
       "      <td>Bert的预训练过程主要包含两个任务，即MLM和NSP。MLM任务涉及对句子中15%的词进行随机mask，并利用上下文来预测这些被mask的词；而NSP任务则是预测两个句子是否在原始文档中是相邻的。BERT在预训练阶段同时进行这两个任务，并将它们的Loss相加。</td>\n",
       "      <td>Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都描述了Bert的预训练过程包含MLM和NSP两个任务，以及任务的具体操作方式。两者传达的含义是相似的，只是表达方式略有不同。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GPT 与 Bert 的区别</td>\n",
       "      <td>GPT是单向模型，只能利用上文信息，而BERT是双向模型，可以同时利用上下文信息。</td>\n",
       "      <td>GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pca 属于有监督还是无监督</td>\n",
       "      <td>PCA belongs to unsupervised learning.</td>\n",
       "      <td>PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer传达了相同的含义，都表明PCA属于无监督学习。虽然表达方式略有不同，但传达的核心信息是一致的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>介绍 transformer 算法</td>\n",
       "      <td>Transformer算法是一个典型的encoder-decoder模型，其中包括Encoder端和Decoder端，每个端都由6个Block组成。Encoder端的Block包括两个模块，多头self-attention模块和前馈神经网络模块，同时每个模块都包含残差层和Layer Normalization层。在功能上，Transformer中的encoder是双向的，用于编码；而decoder是单向的，用于解码，可用于生成任务。</td>\n",
       "      <td>Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            question  \\\n",
       "0           介绍下SVM算法   \n",
       "1      用通俗的语言介绍下强化学习   \n",
       "2         BN 和 LN 区别   \n",
       "3   讲讲self attention   \n",
       "4        Bert 的预训练过程   \n",
       "5     GPT 与 Bert 的区别   \n",
       "6     pca 属于有监督还是无监督   \n",
       "7  介绍 transformer 算法   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            llm_answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  SVM算法是一种二分类模型，其基本模型是定义在特征空间上的间隔最大的线性分类器，通过最大化硬间隔可以学习得到一个线性分类器，即硬间隔SVM。当训练数据不能线性可分但是近似线性可分时，可以通过最大化软间隔学习到一个线性分类器，即软间隔SVM。此外，当训练数据线性不可分时，可以通过使用核技巧和最大化软间隔学习到一个非线性SVM。   \n",
       "1                                                                                                                                                                                                                                                                                                                             Reinforcement Learning can be explained in simple terms as a learning process where an agent learns to make decisions by receiving feedback in the form of rewards or penalties based on its actions. It is like a trial-and-error method where the agent learns through experience by interacting with its environment.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。   \n",
       "3  Self attention is a mechanism that differs from traditional attention in how it operates. While traditional attention calculates dependencies between hidden states of the source and target sides, self attention focuses on capturing dependencies within the source or target input itself. By separately considering the source and target sides, self attention captures relationships between words within each side before incorporating this information into the overall attention mechanism. This approach allows self attention to effectively model dependencies within each side and between the source and target sides of the input.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Bert的预训练过程主要包含两个任务，即MLM和NSP。MLM任务涉及对句子中15%的词进行随机mask，并利用上下文来预测这些被mask的词；而NSP任务则是预测两个句子是否在原始文档中是相邻的。BERT在预训练阶段同时进行这两个任务，并将它们的Loss相加。   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            GPT是单向模型，只能利用上文信息，而BERT是双向模型，可以同时利用上下文信息。   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                PCA belongs to unsupervised learning.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                           Transformer算法是一个典型的encoder-decoder模型，其中包括Encoder端和Decoder端，每个端都由6个Block组成。Encoder端的Block包括两个模块，多头self-attention模块和前馈神经网络模块，同时每个模块都包含残差层和Layer Normalization层。在功能上，Transformer中的encoder是双向的，用于编码；而decoder是单向的，用于解码，可用于生成任务。   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          expected_answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机   \n",
       "1                                                                                                                                                               监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略   \n",
       "2                                                                                                                                                                                                                                                                                        Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。   \n",
       "3  Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系   \n",
       "4                                                                                                                                                                                                                                                                                                             Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                               GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。   \n",
       "7                                                                                                                                                                                                                                                                Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。   \n",
       "\n",
       "                                                                                                                                                          rag_eval_results  \n",
       "0                                    意思一致。LLM-Answer和Expected-Answer都在介绍SVM算法是一种二分类模型，其基本模型是定义在特征空间上的间隔最大的线性分类器。虽然LLM-Answer中提到了硬间隔SVM、软间隔SVM和非线性SVM的概念，但整体来看，它们传达的含义是相似的。  \n",
       "1                                                                                                                                                                     意思不同  \n",
       "2  意思一致。LLM-Answer和Expected-Answer都在解释BN和LN的区别，提到了LN是对单个样本的所有维度特征做归一化，而BN是对不同神经元输入计算均值和方差。两者都强调了LN中同层神经元输入拥有相同的均值和方差，而BN中同一个batch中的输入拥有相同的均值和方差。因此，虽然表达方式有些许不同，但传达的含义是一致的。  \n",
       "3                                                                                                                                                                     意思不同  \n",
       "4                                                                           意思一致。LLM-Answer和Expected-Answer都描述了Bert的预训练过程包含MLM和NSP两个任务，以及任务的具体操作方式。两者传达的含义是相似的，只是表达方式略有不同。  \n",
       "5                                                                                                                                                                     意思不同  \n",
       "6                                                                                           意思一致。LLM-Answer和Expected-Answer传达了相同的含义，都表明PCA属于无监督学习。虽然表达方式略有不同，但传达的核心信息是一致的。  \n",
       "7                                                                                                                                                                     意思不同  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eval llm-answer with expected-answer\n",
    "answers_df['rag_eval_results'] = answers_df.apply(lambda r: eval_llm_output(r, llm), axis=1)\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(answers_df)\n",
    "answers_df.to_excel('auto_merging_pack_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b83f743-c9a4-4319-b3ef-4cdd9705f18c",
   "metadata": {},
   "source": [
    "# Small-to-big Retrieval小到大块检索:\n",
    "### 1输入文档分成初始父分块,将父块细分成子块,\n",
    "### 2子文档和父文档建链接,并为子块建立索引。\n",
    "### 3检索时中用子块,但提供对应父块给llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "476756bd-1ac5-4b68-abe7-c5570353038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install llama-index-embeddings-huggingface==0.2.0\n",
    "\n",
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "\n",
    "RecursiveRetrieverSmallToBigPack = download_llama_pack(\n",
    "    \"RecursiveRetrieverSmallToBigPack\",\n",
    "    \"./recursive_retriever_stb_pack\",\n",
    "    # leave the below commented out (was for testing purposes)\n",
    "    # llama_hub_url=\"https://raw.githubusercontent.com/run-llama/llama-hub/jerry/add_llama_packs/llama_hub\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3e62518-fd2f-478b-bb61-109d6b67398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_retriever_stb_pack = RecursiveRetrieverSmallToBigPack(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad1d1c95-9401-4032-9136-e7979529cfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieving with query id None: 介绍下SVM算法\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-18\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-18: 介绍下SVM算法\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id None: 用通俗的语言介绍下强化学习\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-35\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-35: 用通俗的语言介绍下强化学习\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-0\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-0: 用通俗的语言介绍下强化学习\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id None: BN 和 LN 区别\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-28\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-28: BN 和 LN 区别\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id None: 讲讲self attention\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-7\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-7: 讲讲self attention\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id None: Bert 的预训练过程\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-37\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-37: Bert 的预训练过程\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-1\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-1: Bert 的预训练过程\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id None: GPT 与 Bert 的区别\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-8\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-8: GPT 与 Bert 的区别\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id None: pca 属于有监督还是无监督\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-17\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-17: pca 属于有监督还是无监督\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id None: 介绍 transformer 算法\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-18\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-18: 介绍 transformer 算法\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieved node with id, entering: node-0\n",
      "\u001b[0m\u001b[1;3;34mRetrieving with query id node-0: 介绍 transformer 算法\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "answers = {'question':[], 'llm_answer': [], 'expected_answer': []}\n",
    "\n",
    "for question in qna_dict.keys():\n",
    "    expected_answer = qna_dict[question]\n",
    "    # result\n",
    "    response = recursive_retriever_stb_pack.run(question)\n",
    "    answers['question'].append(question)\n",
    "    answers['llm_answer'].append(response.response)\n",
    "    answers['expected_answer'].append(expected_answer)\n",
    "\n",
    "answers_df = pd.DataFrame(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "750866bd-dfc2-4e33-90e0-0ffd1209a451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>llm_answer</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>rag_eval_results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>介绍下SVM算法</td>\n",
       "      <td>The SVM algorithm's learning strategy focuses on maximizing the margin, which can be formalized as solving a convex quadratic programming problem or equivalently minimizing the regularized hinge loss function. The learning algorithm for SVM involves solving the convex quadratic programming optimization problem. SVM selects kernel functions based on the data separability - Linear kernel is primarily used for linearly separable cases with fewer parameters and faster speed, while RBF kernel is suitable for linearly inseparable cases but relies heavily on parameters, often requiring parameter tuning through techniques like cross-validation.</td>\n",
       "      <td>是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>用通俗的语言介绍下强化学习</td>\n",
       "      <td>In simple terms, reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with its environment. It works on a reward-based system, where the agent receives feedback in the form of rewards or penalties for its actions. The goal is for the agent to learn the best sequence of actions that will lead to the highest cumulative reward over time.</td>\n",
       "      <td>监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略</td>\n",
       "      <td>意思不同。LLM-Answer介绍了强化学习的基本概念和原理，而Expected-Answer则是在比喻的方式下解释了强化学习与监督学习的区别，强调了强化学习通过实践来获取知识的特点。两者虽然都在介绍强化学习，但从不同角度和方式进行了阐述。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BN 和 LN 区别</td>\n",
       "      <td>BN 和 LN 区别在于，BN 是指二叉树的节点，通常表示二叉树的左节点，而 LN 则是指二维列表中的元素，通常表示二维列表中的行或列。</td>\n",
       "      <td>Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>讲讲self attention</td>\n",
       "      <td>SelfAttention is a mechanism that differs significantly from traditional Attention. While traditional Attention calculates dependencies between hidden states of the source and target ends, SelfAttention focuses on the source or target input itself. It captures dependencies between words within the source or target input. The self-attention results from the source end are then incorporated into the attention obtained from the target end to capture dependencies between words in both the source and target ends. This approach allows SelfAttention to effectively capture dependencies not only between words in the source and target ends but also within the source or target input itself.</td>\n",
       "      <td>Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系</td>\n",
       "      <td>意思不同。LLM-Answer和Expected-Answer虽然都在讲Self Attention，但表达方式和详细内容有所不同。LLM-Answer更加技术化和简洁，而Expected-Answer更加详细和易懂。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bert 的预训练过程</td>\n",
       "      <td>During the pre-training process of Bert, a portion of words in the sentence encoding are masked. This allows the model to predict the masked words based on the surrounding words. By artificially masking these words, the computer knows the correct values of the masked words and can assess the accuracy of the model's predictions. This process helps enhance Bert's bidirectional encoding capabilities.</td>\n",
       "      <td>Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GPT 与 Bert 的区别</td>\n",
       "      <td>GPT是单向模型，只能利用上文信息，无法利用上下文信息；而BERT是双向模型，可以同时利用上文和下文信息。</td>\n",
       "      <td>GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。</td>\n",
       "      <td>意思不同。LLM-Answer提到了GPT是单向模型，无法利用上下文信息，只能利用上文；而BERT是双向模型。而Expected-Answer则提到了GPT是基于自回归模型，可以应用在NLU和NLG两大任务，而BERT是基于自编码模型，只能完成NLU任务，无法直接应用在文本生成上面。两者虽然都在讨论GPT和BERT的区别，但具体提到的内容有所不同。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pca 属于有监督还是无监督</td>\n",
       "      <td>PCA belongs to unsupervised learning.</td>\n",
       "      <td>PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都表达了PCA属于无监督学习这一事实，只是表达方式略有不同。LLM-Answer直接指出了PCA属于无监督学习，而Expected-Answer则解释了为什么PCA属于无监督学习。因此，它们传达的含义是相似的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>介绍 transformer 算法</td>\n",
       "      <td>Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个Block，Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder端的Block包括三个模块，多头self-attention模块，多头Encoder-Decoder attention交互模块，以及一个前馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和LayerNormalization层。</td>\n",
       "      <td>Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。</td>\n",
       "      <td>：是的，意思一致。LLM-Answer和Expected-Answer传达的含义非常相似，描述了Transformer算法的结构和组成部分。虽然表达方式有些许差异，但整体内容是一致的。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            question  \\\n",
       "0           介绍下SVM算法   \n",
       "1      用通俗的语言介绍下强化学习   \n",
       "2         BN 和 LN 区别   \n",
       "3   讲讲self attention   \n",
       "4        Bert 的预训练过程   \n",
       "5     GPT 与 Bert 的区别   \n",
       "6     pca 属于有监督还是无监督   \n",
       "7  介绍 transformer 算法   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         llm_answer  \\\n",
       "0                                              The SVM algorithm's learning strategy focuses on maximizing the margin, which can be formalized as solving a convex quadratic programming problem or equivalently minimizing the regularized hinge loss function. The learning algorithm for SVM involves solving the convex quadratic programming optimization problem. SVM selects kernel functions based on the data separability - Linear kernel is primarily used for linearly separable cases with fewer parameters and faster speed, while RBF kernel is suitable for linearly inseparable cases but relies heavily on parameters, often requiring parameter tuning through techniques like cross-validation.   \n",
       "1                                                                                                                                                                                                                                                                                                           In simple terms, reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with its environment. It works on a reward-based system, where the agent receives feedback in the form of rewards or penalties for its actions. The goal is for the agent to learn the best sequence of actions that will lead to the highest cumulative reward over time.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              BN 和 LN 区别在于，BN 是指二叉树的节点，通常表示二叉树的左节点，而 LN 则是指二维列表中的元素，通常表示二维列表中的行或列。   \n",
       "3  SelfAttention is a mechanism that differs significantly from traditional Attention. While traditional Attention calculates dependencies between hidden states of the source and target ends, SelfAttention focuses on the source or target input itself. It captures dependencies between words within the source or target input. The self-attention results from the source end are then incorporated into the attention obtained from the target end to capture dependencies between words in both the source and target ends. This approach allows SelfAttention to effectively capture dependencies not only between words in the source and target ends but also within the source or target input itself.   \n",
       "4                                                                                                                                                                                                                                                                                                  During the pre-training process of Bert, a portion of words in the sentence encoding are masked. This allows the model to predict the masked words based on the surrounding words. By artificially masking these words, the computer knows the correct values of the masked words and can assess the accuracy of the model's predictions. This process helps enhance Bert's bidirectional encoding capabilities.   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             GPT是单向模型，只能利用上文信息，无法利用上下文信息；而BERT是双向模型，可以同时利用上文和下文信息。   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             PCA belongs to unsupervised learning.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                   Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个Block，Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder端的Block包括三个模块，多头self-attention模块，多头Encoder-Decoder attention交互模块，以及一个前馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和LayerNormalization层。   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          expected_answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机   \n",
       "1                                                                                                                                                               监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略   \n",
       "2                                                                                                                                                                                                                                                                                        Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。   \n",
       "3  Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系   \n",
       "4                                                                                                                                                                                                                                                                                                             Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                               GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。   \n",
       "7                                                                                                                                                                                                                                                                Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。   \n",
       "\n",
       "                                                                                                                                                                  rag_eval_results  \n",
       "0                                                                                                                                                                             意思不同  \n",
       "1                                                         意思不同。LLM-Answer介绍了强化学习的基本概念和原理，而Expected-Answer则是在比喻的方式下解释了强化学习与监督学习的区别，强调了强化学习通过实践来获取知识的特点。两者虽然都在介绍强化学习，但从不同角度和方式进行了阐述。  \n",
       "2                                                                                                                                                                             意思不同  \n",
       "3                                                                    意思不同。LLM-Answer和Expected-Answer虽然都在讲Self Attention，但表达方式和详细内容有所不同。LLM-Answer更加技术化和简洁，而Expected-Answer更加详细和易懂。  \n",
       "4                                                                                                                                                                             意思不同  \n",
       "5  意思不同。LLM-Answer提到了GPT是单向模型，无法利用上下文信息，只能利用上文；而BERT是双向模型。而Expected-Answer则提到了GPT是基于自回归模型，可以应用在NLU和NLG两大任务，而BERT是基于自编码模型，只能完成NLU任务，无法直接应用在文本生成上面。两者虽然都在讨论GPT和BERT的区别，但具体提到的内容有所不同。  \n",
       "6                                         意思一致。LLM-Answer和Expected-Answer都表达了PCA属于无监督学习这一事实，只是表达方式略有不同。LLM-Answer直接指出了PCA属于无监督学习，而Expected-Answer则解释了为什么PCA属于无监督学习。因此，它们传达的含义是相似的。  \n",
       "7                                                                                     ：是的，意思一致。LLM-Answer和Expected-Answer传达的含义非常相似，描述了Transformer算法的结构和组成部分。虽然表达方式有些许差异，但整体内容是一致的。  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eval llm-answer with expected-answer\n",
    "answers_df['rag_eval_results'] = answers_df.apply(lambda r: eval_llm_output(r, llm), axis=1)\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(answers_df)\n",
    "answers_df.to_excel('recursive_retriever_stb_pack_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc43c51-18a5-42c9-86f2-7eb8e5f9ff33",
   "metadata": {},
   "source": [
    "# Sentence Window Retrieval句子窗口检索\n",
    "### 1类似小到大块检索，先将文档解析为每个子块中的句子，\n",
    "### 2对单句进行检索。并将检索到的句子连同句子窗口(当前句子的左边的句子和右边的句子)一起传递llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a05032a3-59c6-4ddb-b6e6-fabf8ee93569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d041a666dc41089b74ebebdebb6709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AAAAAAAAAAAIGC\\miniconda\\envs\\llamaindex\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Alienware\\AppData\\Local\\llama_index\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e0079896aa4b06ab6141052f3da74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a685e24a9d475fb46441c1ee23a8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d778ecafddc4ec7b92f8589d84c62eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c431a71ad134651a756f4409fca80ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4483c9a578342b2a53dfa70200b3540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e7a89a360343578cfd29b1db7c3e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c037a0ff302485da496eaf95293b3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3032b7f843cb4510875151138afb8769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "\n",
    "SentenceWindowRetrieverPack = download_llama_pack(\n",
    "    \"SentenceWindowRetrieverPack\",\n",
    "    \"./sentence_window_retriever_pack\",\n",
    "    # leave the below commented out (was for testing purposes)\n",
    "    # llama_hub_url=\"https://raw.githubusercontent.com/run-llama/llama-hub/jerry/add_llama_packs/llama_hub\",\n",
    ")\n",
    "sentence_window_retriever_pack = SentenceWindowRetrieverPack(\n",
    "    documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ea219d6-a94e-4ca3-9f07-d5f7188e3af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "answers = {'question':[], 'llm_answer': [], 'expected_answer': []}\n",
    "\n",
    "for question in qna_dict.keys():\n",
    "    expected_answer = qna_dict[question]\n",
    "    # result\n",
    "    response = sentence_window_retriever_pack.run(question)\n",
    "    answers['question'].append(question)\n",
    "    answers['llm_answer'].append(response.response)\n",
    "    answers['expected_answer'].append(expected_answer)\n",
    "\n",
    "answers_df = pd.DataFrame(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5b54025-15e7-4fd2-b947-ae3c68ca557b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>llm_answer</th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>rag_eval_results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>介绍下SVM算法</td>\n",
       "      <td>SVM is a binary classification model that defines the maximum margin linear classifier in the feature space, distinguishing it from the perceptron. SVM can be categorized into three types: linear separable SVM for linearly separable training data, linear SVM for data that is not linearly separable but approximately so, and nonlinear SVM for data that is not linearly separable, utilizing kernel tricks and maximizing soft margins. SVM's learning strategy focuses on maximizing the margin, which can be formalized as solving a convex quadratic programming problem or minimizing the regularized hinge loss function. The selection of the kernel function in SVM depends on the data characteristics, with Linear kernel suitable for linearly separable cases and RBF kernel for linearly inseparable cases, although parameter tuning for RBF kernel often involves cross-validation due to its dependency on parameters.</td>\n",
       "      <td>是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都在介绍SVM算法的基本原理和特点，虽然表达方式略有不同，但传达的含义是一致的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>用通俗的语言介绍下强化学习</td>\n",
       "      <td>Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with its environment. It works on a reward system, where the agent receives positive or negative feedback based on its actions. The goal is for the agent to learn the best way to achieve a specific objective by maximizing rewards over time through trial and error.</td>\n",
       "      <td>监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BN 和 LN 区别</td>\n",
       "      <td>Batch Normalization (BN) is applied to each batch of training data by normalizing based on the batch's mean and variance, while Layer Normalization (LN) normalizes across the features for each sample. LN does not depend on batch size and is used in scenarios like recurrent neural networks.</td>\n",
       "      <td>Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。</td>\n",
       "      <td>意思一致</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>讲讲self attention</td>\n",
       "      <td>SelfAttention is a mechanism that differs significantly from traditional Attention. While traditional Attention calculates dependencies between hidden states of the source and target ends, SelfAttention operates independently on the source and target ends. It focuses solely on the self-related dependencies within the source or target input, capturing the relationships between words within the source or target end. The self-attention results from the source end are then incorporated into the attention obtained from the target end, capturing dependencies between words in both the source and target ends. This approach is more effective than traditional Attention mechanisms because it considers not only the dependencies between words in the source and target ends but also effectively captures dependencies between words within the source or target end themselves.</td>\n",
       "      <td>Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都在讲述Self Attention与传统Attention机制的区别，以及Self Attention的工作原理和优势。虽然表达方式有所不同，但传达的含义是一致的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bert 的预训练过程</td>\n",
       "      <td>During the pre-training process of Bert, a portion of words in the sentence encoding is masked. This allows the model to predict the masked words based on the surrounding words. Since the computer knows the correct values of the masked words, it can assess the accuracy of the model's predictions. This process helps enhance Bert's bidirectional encoding capabilities.</td>\n",
       "      <td>Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GPT 与 Bert 的区别</td>\n",
       "      <td>GPT是单向模型，只能利用上文信息，而BERT是双向模型，可以同时利用上下文信息。 GPT基于自回归模型，适用于NLU和NLG任务，而BERT基于自编码模型，主要用于NLU任务，无法直接应用在文本生成任务上。</td>\n",
       "      <td>GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。</td>\n",
       "      <td>意思不同。LLM-Answer提到了GPT是单向模型，无法利用上下文信息，只能利用上文；而BERT是双向模型。而Expected-Answer则提到了GPT是基于自回归模型，可以应用在NLU和NLG两大任务，而原生的BERT采用的基于自编码模型，只能完成NLU任务，无法直接应用在文本生成上面。两者虽然都提到了GPT和BERT的区别，但具体描述的重点和细节有所不同。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pca 属于有监督还是无监督</td>\n",
       "      <td>PCA belongs to unsupervised learning.</td>\n",
       "      <td>PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。</td>\n",
       "      <td>意思一致。LLM-Answer和Expected-Answer都表明PCA属于无监督学习。虽然表达方式略有不同，但传达的含义是一致的。LLM-Answer指出PCA属于无监督学习，而Expected-Answer则进一步解释了PCA的特点。因此，它们传达的含义是相似的。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>介绍 transformer 算法</td>\n",
       "      <td>The algorithm described in the context is related to printing elements of a matrix in a spiral order. It involves moving in a clockwise direction along the matrix borders and adding the elements to a result list. The algorithm tracks the boundaries of the matrix and the current position to determine the movement direction.</td>\n",
       "      <td>Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。</td>\n",
       "      <td>意思不同</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            question  \\\n",
       "0           介绍下SVM算法   \n",
       "1      用通俗的语言介绍下强化学习   \n",
       "2         BN 和 LN 区别   \n",
       "3   讲讲self attention   \n",
       "4        Bert 的预训练过程   \n",
       "5     GPT 与 Bert 的区别   \n",
       "6     pca 属于有监督还是无监督   \n",
       "7  介绍 transformer 算法   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       llm_answer  \\\n",
       "0  SVM is a binary classification model that defines the maximum margin linear classifier in the feature space, distinguishing it from the perceptron. SVM can be categorized into three types: linear separable SVM for linearly separable training data, linear SVM for data that is not linearly separable but approximately so, and nonlinear SVM for data that is not linearly separable, utilizing kernel tricks and maximizing soft margins. SVM's learning strategy focuses on maximizing the margin, which can be formalized as solving a convex quadratic programming problem or minimizing the regularized hinge loss function. The selection of the kernel function in SVM depends on the data characteristics, with Linear kernel suitable for linearly separable cases and RBF kernel for linearly inseparable cases, although parameter tuning for RBF kernel often involves cross-validation due to its dependency on parameters.   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Reinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with its environment. It works on a reward system, where the agent receives positive or negative feedback based on its actions. The goal is for the agent to learn the best way to achieve a specific objective by maximizing rewards over time through trial and error.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Batch Normalization (BN) is applied to each batch of training data by normalizing based on the batch's mean and variance, while Layer Normalization (LN) normalizes across the features for each sample. LN does not depend on batch size and is used in scenarios like recurrent neural networks.   \n",
       "3                                          SelfAttention is a mechanism that differs significantly from traditional Attention. While traditional Attention calculates dependencies between hidden states of the source and target ends, SelfAttention operates independently on the source and target ends. It focuses solely on the self-related dependencies within the source or target input, capturing the relationships between words within the source or target end. The self-attention results from the source end are then incorporated into the attention obtained from the target end, capturing dependencies between words in both the source and target ends. This approach is more effective than traditional Attention mechanisms because it considers not only the dependencies between words in the source and target ends but also effectively captures dependencies between words within the source or target end themselves.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                During the pre-training process of Bert, a portion of words in the sentence encoding is masked. This allows the model to predict the masked words based on the surrounding words. Since the computer knows the correct values of the masked words, it can assess the accuracy of the model's predictions. This process helps enhance Bert's bidirectional encoding capabilities.   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        GPT是单向模型，只能利用上文信息，而BERT是双向模型，可以同时利用上下文信息。 GPT基于自回归模型，适用于NLU和NLG任务，而BERT基于自编码模型，主要用于NLU任务，无法直接应用在文本生成任务上。   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           PCA belongs to unsupervised learning.   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The algorithm described in the context is related to printing elements of a matrix in a spiral order. It involves moving in a clockwise direction along the matrix borders and adding the elements to a result list. The algorithm tracks the boundaries of the matrix and the current position to determine the movement direction.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          expected_answer  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机   \n",
       "1                                                                                                                                                               监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略   \n",
       "2                                                                                                                                                                                                                                                                                        Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。   \n",
       "3  Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系   \n",
       "4                                                                                                                                                                                                                                                                                                             Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                               GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。   \n",
       "7                                                                                                                                                                                                                                                                Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。   \n",
       "\n",
       "                                                                                                                                                                          rag_eval_results  \n",
       "0                                                                                                                  意思一致。LLM-Answer和Expected-Answer都在介绍SVM算法的基本原理和特点，虽然表达方式略有不同，但传达的含义是一致的。  \n",
       "1                                                                                                                                                                                     意思不同  \n",
       "2                                                                                                                                                                                     意思一致  \n",
       "3                                                                       意思一致。LLM-Answer和Expected-Answer都在讲述Self Attention与传统Attention机制的区别，以及Self Attention的工作原理和优势。虽然表达方式有所不同，但传达的含义是一致的。  \n",
       "4                                                                                                                                                                                     意思不同  \n",
       "5  意思不同。LLM-Answer提到了GPT是单向模型，无法利用上下文信息，只能利用上文；而BERT是双向模型。而Expected-Answer则提到了GPT是基于自回归模型，可以应用在NLU和NLG两大任务，而原生的BERT采用的基于自编码模型，只能完成NLU任务，无法直接应用在文本生成上面。两者虽然都提到了GPT和BERT的区别，但具体描述的重点和细节有所不同。  \n",
       "6                                                  意思一致。LLM-Answer和Expected-Answer都表明PCA属于无监督学习。虽然表达方式略有不同，但传达的含义是一致的。LLM-Answer指出PCA属于无监督学习，而Expected-Answer则进一步解释了PCA的特点。因此，它们传达的含义是相似的。  \n",
       "7                                                                                                                                                                                     意思不同  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eval llm-answer with expected-answer\n",
    "answers_df['rag_eval_results'] = answers_df.apply(lambda r: eval_llm_output(r, llm), axis=1)\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(answers_df)\n",
    "answers_df.to_excel('sentence_window_retriever_pack_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63183e45-e087-47ee-afd3-f718cb9037ae",
   "metadata": {},
   "source": [
    "# Self-RAG高级检索\n",
    "\n",
    "Self-RAG代表了一种创新的框架，它利用自我反思的标记来对语言模型（LM）进行训练和调控。这个框架的核心流程可以划分为三个主要环节：信息检索、内容生成和内容评估。\n",
    "\n",
    "![selfrag](.\\selfrag.png)\n",
    "\n",
    "\n",
    "1.在信息检索环节，Self-RAG首先对检索标记进行解码，以判断当前情境下是否有必要进行信息的检索，并据此控制检索机制。如果确定需要检索，LM便会激活外部的检索模块，以便搜寻与当前任务相关的文档资料。\n",
    "\n",
    "2.进入内容生成阶段，若无需额外检索，LM将直接预测接下来的文本片段。若检索是必要的，LM会先生成一系列批评标记，这些标记的作用是评估已检索到的文档资料的相关性，并基于这些资料来构建后续的内容。\n",
    "\n",
    "3.在内容评估环节，LM进一步评价检索到的文档对生成内容的支持程度。最终，通过一个新的批评标记来对整体的响应效果进行评估。\n",
    "\n",
    "![selftoken](.\\selftoken.jpg)\n",
    "\n",
    "在训练Self-RAG的过程中，涉及到三个关键的模型组件：检索器、批评家和生成器。首先，批评家模型会被训练，它利用检索器找到的文档和增强的反思标记来训练。接着，生成器LM通过标准的下一个标记预测任务进行训练，目的是掌握如何生成自然连贯的文本以及如何使用特殊的标记来检索或评估自身的生成内容。\n",
    "\n",
    "在推理应用中，Self-RAG通过掌握生成反思标记的能力，能够为多种不同的下游任务或特定需求定制模型的行为，而无需对LMs进行额外的训练。它能够根据检索标记灵活地决定何时进行检索，让模型自行判断是否需要进行信息检索。此外，Self-RAG引入了多种细致的批评标记，用于评估生成内容的各种质量维度。在生成过程中，研究者采用了基于期望批评标记概率的线性插值方法，进行分段级别的束搜索（beam search），以便在每个时间节点确定最佳的续写方案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7529b87-4918-47f4-a5bd-3ab8de4e56cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "\n",
    "# download and install dependencies\n",
    "SelfRAGPack=download_llama_pack(\n",
    "    \"SelfRAGPack\",\n",
    "    \"D:/AAAAAAAAAAAIGC/self_rag_pack\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b979583-859b-42aa-9c13-b76d6ec86bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document, VectorStoreIndex\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Setup a simple retriever\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22e76989-8a97-4caa-86ab-5f71d1f7d144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tmpmodel\\selfrag_llama2_7b.q4_k_m.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
      "downloading https://huggingface.co/m4r1/selfrag_llama2_7b-GGUF/resolve/main/selfrag_llama2_7b.q4_k_m.gguf to C:\\Users\\Alienware\\.cache\\huggingface\\hub\\tmpx3m0nk9s\n"
     ]
    }
   ],
   "source": [
    "download_dir = \"./tmpmodel\"  # Replace\n",
    "!pip3 install -q huggingface-hub\n",
    "!huggingface-cli download m4r1/selfrag_llama2_7b-GGUF selfrag_llama2_7b.q4_k_m.gguf --local-dir {download_dir} --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3efe0e4-e8cc-45aa-a99f-f7edf570a35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./tmpmodel/selfrag_llama2_7b.q4_k_m.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 275/32016 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32016\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3891.33 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'LLaMA v2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '11008', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '15', 'llama.attention.head_count_kv': '32', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_path = \"./tmpmodel/selfrag_llama2_7b.q4_k_m.gguf\"\n",
    "agent_pack = SelfRAGPack(\n",
    "    model_path=model_path, retriever=retriever, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e1788f3-5c00-4d0a-8951-c7fc6937fa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      16.82 ms /    40 runs   (    0.42 ms per token,  2378.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2904.54 ms /    22 tokens (  132.02 ms per token,     7.57 tokens per second)\n",
      "llama_print_timings:        eval time =    8948.68 ms /    39 runs   (  229.45 ms per token,     4.36 tokens per second)\n",
      "llama_print_timings:       total time =   12115.75 ms /    61 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieval required\n",
      "\u001b[0m\u001b[1;3;34mReceived: 10 documents\n",
      "\u001b[0m\u001b[1;3;34mStart evaluation\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      21.36 ms /    50 runs   (    0.43 ms per token,  2341.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =   79415.59 ms /   743 tokens (  106.89 ms per token,     9.36 tokens per second)\n",
      "llama_print_timings:        eval time =   11974.64 ms /    49 runs   (  244.38 ms per token,     4.09 tokens per second)\n",
      "llama_print_timings:       total time =   94319.48 ms /   792 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第4页共46页第十七篇：2022年4月10日百度机器学习方向暑期实习面试题6道\n",
      "1、介绍下SVM算法\n",
      "是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感\n",
      "知机。\n",
      "SVM可分为三种：\n",
      "线性可分SVM\n",
      "当训练数据线性可分时，通过最大化硬间隔（hardmargin）可以学习得到一个线性分类器，即硬间隔\n",
      "SVM。\n",
      "线性SVM\n",
      "当训练数据不能线性可分但是近似线性可分时，通过最大化软间隔（softmargin）也可以学习到一个线\n",
      "性分类器，即软间隔SVM。\n",
      "非线性SVM\n",
      "当训练数据线性不可分时，通过使用核技巧（kerneltrick）和最大化软间隔，可以学习到一个非线性\n",
      "SVM。\n",
      "SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损\n",
      "失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。\n",
      "SVM如何选择核函数\n",
      "Linear核：主要用于线性可分的情形。参数少，速度快，对于一般数据，分类效果已经很理想了。\n",
      "RBF核：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交\n",
      "叉验证来寻找合适的参数，不过这个过程比较耗时。\n",
      "2、介绍下逻辑回归算法\n",
      "逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度下降法来求解参数，从而达\n",
      "到将数据二分类的目的。\n",
      "逻辑回归的优缺点</paragraph>\n",
      "Prediction: [Relevant]逻辑回归是一种常见的二分类模型，它的基本模型是定义在特征空间上的一个最大的\n",
      "Score: 0.9285435719763134\n",
      "\u001b[0m\u001b[1;3;34m1/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       4.18 ms /    19 runs   (    0.22 ms per token,  4547.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   89390.15 ms /   861 tokens (  103.82 ms per token,     9.63 tokens per second)\n",
      "llama_print_timings:        eval time =    3332.36 ms /    18 runs   (  185.13 ms per token,     5.40 tokens per second)\n",
      "llama_print_timings:       total time =   95194.60 ms /   879 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第18页共46页第二十一篇：2022年4月14日美团计算机视觉算法暑期实习面试题6道\n",
      "1、简单介绍gbdt算法的原理\n",
      "GBDT是梯度提升决策树，是一种基于Boosting的算法，采用以决策树为基学习器的加法模型，通过不\n",
      "断拟合上一个弱学习器的残差，最终实现分类或回归的模型。关键在于利用损失函数的负梯度在当前模型的值\n",
      "作为残差的近似值，从而拟合一个回归树。对于分类问题：常使用指数损失函数；对于回归问题：常使用平方\n",
      "误差损失函数（此时，其负梯度就是通常意义的残差），对于一般损失函数来说就是残差的近似。\n",
      "无论损失函数是什么形式，每个决策树拟合的都是负梯度。准确的说，不是用负梯度代替残差，而是当损\n",
      "失函数是均方损失时，负梯度刚好是残差，残差只是特例。\n",
      "2、pca属于有监督还是无监督\n",
      "PCA按有监督和无监督划分应该属于无监督学习，所以数据集有无y并不重要，只是改变样本X的属性\n",
      "(特征)维度。\n",
      "3、介绍svm算法\n",
      "是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感\n",
      "知机。\n",
      "SVM可分为三种：\n",
      "线性可分SVM\n",
      "当训练数据线性可分时，通过最大化硬间隔（hardmargin）可以学习得到一个线性分类器，即硬间隔\n",
      "SVM。\n",
      "线性SVM\n",
      "当训练数据不能线性可分但是近似线性可分时，通过最大化软间隔（softmargin）也可以学习到一个线\n",
      "性分类器，即软间隔SVM。\n",
      "非线性SVM\n",
      "当训练数据线性不可分时，通过使用核技巧（kerneltrick）和最大化软间隔，可以学习到一个非线性\n",
      "SVM。</paragraph>\n",
      "Prediction: [Relevant]这里介绍的是线性可分SVM。[Fully supported][Utility:5]\n",
      "Score: 2.3546355045543526\n",
      "\u001b[0m\u001b[1;3;34m2/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       0.92 ms /     4 runs   (    0.23 ms per token,  4362.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   60326.36 ms /   837 tokens (   72.07 ms per token,    13.87 tokens per second)\n",
      "llama_print_timings:        eval time =     576.27 ms /     3 runs   (  192.09 ms per token,     5.21 tokens per second)\n",
      "llama_print_timings:       total time =   62555.82 ms /   840 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第19页共46页SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损\n",
      "失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。\n",
      "SVM如何选择核函数\n",
      "Linear核：主要用于线性可分的情形。参数少，速度快，对于一般数据，分类效果已经很理想了。\n",
      "RBF核：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交\n",
      "叉验证来寻找合适的参数，不过这个过程比较耗时。\n",
      "4、介绍transformer算法\n",
      "Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个Block，\n",
      "Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder端的\n",
      "Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一个前\n",
      "馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和LayerNormalization\n",
      "层。\n",
      "5、layernorm和batchnorm的比较\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "6、Leetcode—两数之和\n",
      "该题较为简单。\n",
      "1.classSolution(object):\n",
      "2. deftwoSum(self,nums,target):\n",
      "3. \"\"\"\n",
      "4. :typenums:List[int]\n",
      "5. :typetarget:int\n",
      "6. :rtype:List[int]\n",
      "7. \"\"\"\n",
      "8. dic={}\n",
      "9. fori,numinenumerate(nums):\n",
      "10. iftarget-numindic:</paragraph>\n",
      "Prediction: [Irrelevant]​[Utility:5]\n",
      "Score: 0.8689226437672928\n",
      "\u001b[0m\u001b[1;3;34m3/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      13.74 ms /    52 runs   (    0.26 ms per token,  3785.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   88326.36 ms /  1148 tokens (   76.94 ms per token,    13.00 tokens per second)\n",
      "llama_print_timings:        eval time =   10331.39 ms /    51 runs   (  202.58 ms per token,     4.94 tokens per second)\n",
      "llama_print_timings:       total time =  101213.51 ms /  1199 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第1页共46页目录\n",
      "第十七篇：2022年4月10日百度机器学习方向暑期实习面试题6道.4\n",
      "1、介绍下SVM算法.4\n",
      "2、介绍下逻辑回归算法.4\n",
      "3、介绍下决策树算法.5\n",
      "4、用通俗的语言介绍下强化学习（ReinforcementLearning）.5\n",
      "5、Leetcode34在排序数组中查找元素的第一个和最后一个位置.5\n",
      "6、Letcode102层序遍历.6\n",
      "第十八篇：2022年4月18日字节跳动机器学习AILab一面面试题6道.8\n",
      "1、BN和LN区别.8\n",
      "2、讲讲selfattention.8\n",
      "3、Bert的预训练过程.8\n",
      "4、PreNorm与PostNorm的区别？8\n",
      "5、GPT与Bert的区别.9\n",
      "6、如何加速Bert模型的训练.9\n",
      "第十九篇：2022年4月8日字节跳动机抖音APP推荐实习面试题8道.11\n",
      "1、AUC是什么？如何计算AUC？11\n",
      "2、AUC线上线下不一致怎么办.11\n",
      "3、召回阶段的负采样是怎么做的？12\n",
      "4、FM，DeepFM跟FFM的对比.12\n",
      "5、手撕FM的训练过程.12\n",
      "6、Leetcode—64.最小路径和.13\n",
      "7、剑指Offer10-I.斐波那契数列.14\n",
      "8、Leetcode—215.数组中的第K个最大元素.14\n",
      "第二十篇：2022年4月携程暑期实习搜索推荐算法岗面试题8道.15\n",
      "1、你所理解的推荐系统是什么样的？大致流程？所用模型？15\n",
      "2、双塔模型优势，缺点，如何改进？15\n",
      "3、粗排的目的是什么？15\n",
      "4、wide&deep模型为什么要有wide层结构，优缺点，如何改进？16\n",
      "5、推荐领域GBDT+LR的做法了解吗？16\n",
      "6、粗排有哪些指标？NDCG了解吗？16\n",
      "7、ROC，PR曲线含义，坐标轴代表什么？17\n",
      "8、AUC怎么求，实际意义？17\n",
      "第二十一篇：2022年4月14日美团计算机视觉算法暑期实习面试题6道.18\n",
      "1、简单介绍gbdt算法的原理.18\n",
      "2、pca属于有监督还是无监督.18\n",
      "3、介绍svm算法.18\n",
      "4、介绍transformer算法.19\n",
      "5、layernorm和batchnorm的比较.19\n",
      "6、Leetcode—两数之和.19\n",
      "第二十二篇：2022年5月18日2023届广联达提前批面试题5道.21\n",
      "1、Transformer中encoder和decoder的区别（结构和功能上）.21\n",
      "2、有哪些防止过拟合的方法.21\n",
      "3、L1和L2正则化为什么可以防止过拟合.21\n",
      "4、传统机器学习方法了解多少.21</paragraph>\n",
      "Prediction: [Relevant]这些问题都是在知识点上相对较广的，尤其是在SVM和transformer算法上，可以通过在线资料进行阅\n",
      "Score: 0.851191696652873\n",
      "\u001b[0m\u001b[1;3;34m4/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.64 ms /    50 runs   (    0.25 ms per token,  3954.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =   59962.88 ms /   800 tokens (   74.95 ms per token,    13.34 tokens per second)\n",
      "llama_print_timings:        eval time =    9394.84 ms /    49 runs   (  191.73 ms per token,     5.22 tokens per second)\n",
      "llama_print_timings:       total time =   71143.90 ms /   849 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第22页共46页形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权\n",
      "重值比较高，那么这个特征最后对结果的影响会比较大。\n",
      "模型效果不错。在工程上是可以接受的（作为baseline），如果特征工程做的好，效果不会太差，并且\n",
      "特征工程可以并行开发，大大加快开发的速度。\n",
      "训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。\n",
      "缺点：\n",
      "准确率欠佳。因为形式非常的简单，而现实中的数据非常复杂，因此，很难达到很高的准确性。很难处理\n",
      "数据不平衡的问题。\n",
      "5、生成式模型和判别式模型的区别并举一些例子\n",
      "生成模型：学习得到联合概率分布P(x,y)，即特征x，共同出现的概率。\n",
      "常见的生成模型：朴素贝叶斯模型，混合高斯模型，HMM模型。\n",
      "判别模型：学习得到条件概率分布P(y|x)，即在特征x出现的情况下标记y出现的概率。\n",
      "常见的判别模型：感知机，决策树，逻辑回归，SVM，CRF等。\n",
      "判别式模型：要确定一个羊是山羊还是绵羊，用判别式模型的方法是从历史数据中学习到模型，然后通过\n",
      "提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。\n",
      "生成式模型：是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模\n",
      "型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，再放到绵羊模型中看概率是多少，哪个大就是\n",
      "哪个。</paragraph>\n",
      "Prediction: [Relevant]这种模型是一种判别模型，因为它能够判断出一个样本是否属于山羊或者是绵羊。[Partially supported]\n",
      "Score: 1.085246139101128\n",
      "\u001b[0m\u001b[1;3;34m5/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      10.65 ms /    42 runs   (    0.25 ms per token,  3942.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =   77830.12 ms /  1038 tokens (   74.98 ms per token,    13.34 tokens per second)\n",
      "llama_print_timings:        eval time =    8314.95 ms /    41 runs   (  202.80 ms per token,     4.93 tokens per second)\n",
      "llama_print_timings:       total time =   88345.31 ms /  1079 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第27页共46页第二十四篇：2022年5月10日美团搜索推荐算法面试题10道\n",
      "1、介绍下推荐系统的流程\n",
      "推荐系统的流程主要包含一下几个阶段。\n",
      "索引&特征:会根据内容特性提前建立若干种类型的索引。\n",
      "召回阶段:用户请求时会从各种索引种取出千/万条item。\n",
      "粗排阶段:针对这上千/万条item，进行第一遍打分，再筛选出几百条或者千条。这个阶段的排序模型一\n",
      "般都比较简单，能够过滤掉一些与用户兴趣明显不相关的。\n",
      "精排阶段:得到几百条item后，精排阶段会建立相对精细的模型，根据用户的画像，偏好，上下文，结\n",
      "合业务目标进行排序。一般精排后返回50-100条给到engine侧。\n",
      "重排阶段:engine侧拿到精排的50条item。还会做很多的人工干预和产品逻辑，比如item之间的多样\n",
      "性，产品策略逻辑，比如热门，置顶，多种内容之间的位置混合等等。最终会返回5-10条左右的item，曝\n",
      "光给客户端。根据业务特性，在线流程还有许多比较细的模块，比如去重服务，避免给用户推荐重复的内容。\n",
      "特征预处理，特征抽取等模块。\n",
      "2、召回和排序的差异？\n",
      "召回的目的在于减少候选的数量（尽量控制在1000以内），方便后续排序环节使用复杂模型精准排序；\n",
      "因为在短时间内评估海量候选，所以召回的关键点是个快字，受限与此与排序相比，召回的算法模型相对简\n",
      "单，使用的特征比较少。而排序模型相对更加复杂，更追求准确性，使用的特征也会较多。\n",
      "3、结果f1提升的1%怎么保证有效性，如何保证置信呢？\n",
      "实验过程中固定随机种子、多次实验取平均\n",
      "4、固定随机种子后，多次实验结果相同吗？\n",
      "还是会有细微差别，因为在梯度传播过程，梯度（浮点数）精度有差异，随着神经网络层数的增加，梯度\n",
      "差异会从小数后面的位置往前跑。只能设置浮点数精度增加来缓解这个问题。</paragraph>\n",
      "Prediction: [Relevant]因此，在实验过程中，需要多次实验取平均，以避免这个问题。[No support / Contradictory][Utility:5]\n",
      "Score: 1.4039067481529837\n",
      "\u001b[0m\u001b[1;3;34m6/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.47 ms /    51 runs   (    0.24 ms per token,  4088.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =   75346.32 ms /  1013 tokens (   74.38 ms per token,    13.44 tokens per second)\n",
      "llama_print_timings:        eval time =    9905.67 ms /    50 runs   (  198.11 ms per token,     5.05 tokens per second)\n",
      "llama_print_timings:       total time =   87609.43 ms /  1063 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第15页共46页第二十篇：2022年4月携程暑期实习搜索推荐算法岗面试题8道\n",
      "1、你所理解的推荐系统是什么样的？大致流程？所用模型？\n",
      "推荐系统一方面是为了帮助消费者发现对自己有价值的商品，另一方面是帮助生产者把用户可能感兴趣的\n",
      "商品展现给用户，实现生产者和消费者的双赢。\n",
      "大致流程主要包括：获取用户特征，召回过程，排序过程（粗排、精排）\n",
      "召回模型：\n",
      "·规则召回（兴趣标签top，热门top，新品top等）\n",
      "·协同召回（基于用户的协同过滤，基于商品的协同过滤）\n",
      "·向量召回（FM召回，Item2vec,YoutubeDNN向量召回，GraphEmbedding召回，DSSM双塔召\n",
      "回）\n",
      "排序模型：GBDT+LR、Xgboost、FM/FFM、Wide&Deep、DeepFM、Deep&Cross、DIN、BST\n",
      "等\n",
      "2、双塔模型优势，缺点，如何改进？\n",
      "双塔模型的优势是速度快，但模型精度还有待提升。\n",
      "速度快是因为将所有Item转化成Embedding，并存储进ANN检索系统，比如FAISS，以供查询。类\n",
      "似FAISS这种ANN检索系统对海量数据的查询效率高。\n",
      "而双塔模型为了速度快，必须对用户和Item进行特征分离，而特征分离，又必然导致上述两个原因产生\n",
      "的效果损失。\n",
      "改进：SENet双塔模型，把SENet放在Embedding层之上，目的是通过SENet网络，动态地学习这些\n",
      "特征的重要性：对于每个特征学会一个特征权重，然后再把学习到的权重乘到对应特征的Embedding里，这\n",
      "样就可以动态学习特征权重，通过小权重抑制噪音或者无效低频特征，通过大权重放大重要特征影响的目的。\n",
      "3、粗排的目的是什么？\n",
      "粗排是用来帮精排模型找到那些它本来就要打高分的item，只不过范围更广一些。.按照上面的例子，如\n",
      "果没有粗排，精排模型自己找出来的某top10的item。而粗排的任务就是要找到包含这10个item的一个更\n",
      "小的候选集，既保证了效果，又减少线上预测的负担。</paragraph>\n",
      "Prediction: [Relevant]4、如何理解协同召回？\n",
      "协同召回的思想是基于用户的协同过滤，基于商品的协\n",
      "Score: 0.719972554975759\n",
      "\u001b[0m\u001b[1;3;34m7/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.60 ms /    50 runs   (    0.25 ms per token,  3968.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45592.07 ms /   616 tokens (   74.01 ms per token,    13.51 tokens per second)\n",
      "llama_print_timings:        eval time =    9832.24 ms /    49 runs   (  200.66 ms per token,     4.98 tokens per second)\n",
      "llama_print_timings:       total time =   56904.70 ms /   665 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第35页共46页beamsearch是介于全局搜索和贪婪搜索之间。使用beamsize参数来限制在每一步保留下来的可能性\n",
      "词的数量。beamsearch是在测试阶段为了获得更好准确性而采取的一种策略，在训练阶段无需使用。\n",
      "相比暴力搜索的全局最优解，降低时间复杂度的方法就是寻找次优解，具体就是把搜索空间中的N减下\n",
      "来，每一步计算完只保留K个(beamsize)最大的取值路径，这样时间复杂度降为O(K*N*T)，K取值一般比\n",
      "N小很多。这样得到的虽然不是最优解，但是在seq2seq模型的推理预测中可以兼顾时间和效果。\n",
      "5、你了解的文本表示方法有哪些\n",
      "基于one-hot、tf-idf、textrank等的bag-of-words；\n",
      "主题模型：LSA（SVD）、pLSA、LDA；\n",
      "基于词向量的固定表征：word2vec、fastText、glove\n",
      "基于词向量的动态表征：elmo、GPT、bert\n",
      "对比如下：\n",
      "One-hot表示：维度灾难、语义鸿沟；\n",
      "矩阵分解（LSA）：利用全局语料特征，但SVD求解计算复杂度大；\n",
      "基于NNLM/RNNLM的词向量：词向量为副产物，存在效率不高等问题；\n",
      "word2vec、fastText：优化效率高，但是基于局部语料；\n",
      "glove：基于全局预料，结合了LSA和word2vec的优点；\n",
      "elmo、GPT、bert：动态特征。</paragraph>\n",
      "Prediction: [Relevant]这里的“N”是一个大的数字，例如10000，“T”也是一个大的数字，例如100，“K”也是一个大的\n",
      "Score: 0.5379866103651348\n",
      "\u001b[0m\u001b[1;3;34m8/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.02 ms /    50 runs   (    0.24 ms per token,  4161.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   66940.73 ms /   879 tokens (   76.16 ms per token,    13.13 tokens per second)\n",
      "llama_print_timings:        eval time =   10033.26 ms /    49 runs   (  204.76 ms per token,     4.88 tokens per second)\n",
      "llama_print_timings:       total time =   79092.65 ms /   928 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第43页共46页好处是这样做大大节约了参数量。\n",
      "5、如何防止过拟合\n",
      "数据的角度：获取和使用更多的数据（数据集增强）；\n",
      "模型角度：降低模型复杂度、L1\\L2\\Dropout正则化、Earlystopping（提前终止）\n",
      "模型融合的角度：使用bagging等模型融合方法。\n",
      "6、BN在训练和测试的时候的区别？可以防止过拟合吗？\n",
      "对于BN，在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。\n",
      "而在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全\n",
      "量训练数据的均值和方差，这个可以通过移动平均法求得。\n",
      "对于BN，当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，gamma和bata。\n",
      "BN算法防止过拟合：在网络的训练中，BN的使用使得一个minibatch中所有样本都被关联在了一起，\n",
      "因此网络不会从某一个训练样本中生成确定的结果，即同样一个样本的输出不再仅仅取决于样本的本身，也取\n",
      "决于跟这个样本同属一个batch的其他样本，而每次网络都是随机取batch，这样就会使得整个网络不会朝\n",
      "这一个方向使劲学习。一定程度上避免了过拟合。\n",
      "7、什么是AUC？\n",
      "AUC：AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。AUC反映的是分\n",
      "类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。\n",
      "8、卷积核计算公式\n",
      "卷积层计算公式如下：\n",
      "其中，W为输入大小，K为卷积核大小，P为padding大小，S为步幅。\n",
      "如果，想保持卷积前后的特征图大小相同，通常会设定padding为：</paragraph>\n",
      "Prediction: [Relevant]输出大小为W-K+1，因此可以得出：\n",
      "W=K+1\n",
      "\n",
      "其中，K为卷积核大小，W为输出大小\n",
      "Score: 0.7874755543086321\n",
      "\u001b[0m\u001b[1;3;34m9/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.30 ms /    50 runs   (    0.25 ms per token,  4064.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45178.53 ms /   603 tokens (   74.92 ms per token,    13.35 tokens per second)\n",
      "llama_print_timings:        eval time =    9311.76 ms /    49 runs   (  190.04 ms per token,     5.26 tokens per second)\n",
      "llama_print_timings:       total time =   55997.96 ms /   652 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍下SVM算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第41页共46页8、LightGBM对于Xgboost有什么改进\n",
      "模型精度：XGBoost和LightGBM相当。\n",
      "训练速度：LightGBM远快于XGBoost。(快百倍以上，跟数据集有关系)\n",
      "内存消耗：LightGBM远小于XGBoost。(大约是xgb的五分之一)\n",
      "缺失值特征：XGBoost和LightGBM都可以自动处理特征缺失值。\n",
      "分类特征：XGBoost不支持类别特征，需要OneHot编码预处理。LightGBM直接支持类别特征。\n",
      "LightGBM在XGBoost上主要有3方面的优化。\n",
      "1，Histogram算法:直方图算法。\n",
      "2，GOSS算法:基于梯度的单边采样算法。\n",
      "3，EFB算法:互斥特征捆绑算法。\n",
      "9、防止过拟合的方式\n",
      "降低模型复杂度\n",
      "增加更多的训练数据：使用更大的数据集训练模型\n",
      "数据增强\n",
      "正则化：L1、L2、添加BN层\n",
      "添加Dropout策略\n",
      "EarlyStopping\n",
      "10、Adam讲一下\n",
      "Adam算法即自适应时刻估计方法（AdaptiveMomentEstimation），能计算每个参数的自适应学习\n",
      "率。这个方法不仅存储了AdaDelta先前平方梯度的指数衰减平均值，而且保持了先前梯度M(t)的指数衰减\n",
      "平均值，这一点与动量类似。\n",
      "Adam实际上就是将Momentum和RMSprop集合在一起，把一阶动量和二阶动量都使用起来了。</paragraph>\n",
      "Prediction: [Relevant]这里是一个示例代码，使用Adam算法训练一个二阶梯度中心偏移的神经网络模型。[No support / Contradictory]\n",
      "Score: 0.5490881291001676\n",
      "\u001b[0m\u001b[1;3;34m10/10 paragraphs done\n",
      "\n",
      "\u001b[0m\u001b[1;3;34mEnd evaluation\n",
      "\u001b[0m\u001b[1;3;34mSelected the best answer: [Relevant]这里介绍的是线性可分SVM。[Fully supported][Utility:5]\n",
      "\u001b[0m\u001b[1;3;32mFinal answer: 这里介绍的是线性可分SVM。\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       1.03 ms /     4 runs   (    0.26 ms per token,  3868.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2147.54 ms /    28 tokens (   76.70 ms per token,    13.04 tokens per second)\n",
      "llama_print_timings:        eval time =     509.85 ms /     3 runs   (  169.95 ms per token,     5.88 tokens per second)\n",
      "llama_print_timings:       total time =    2722.33 ms /    31 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;32mFinal answer: 好的\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       8.15 ms /    32 runs   (    0.25 ms per token,  3928.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1402.07 ms /    18 tokens (   77.89 ms per token,    12.84 tokens per second)\n",
      "llama_print_timings:        eval time =    5417.76 ms /    31 runs   (  174.77 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:       total time =    6950.20 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieval required\n",
      "\u001b[0m\u001b[1;3;34mReceived: 10 documents\n",
      "\u001b[0m\u001b[1;3;34mStart evaluation\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       1.38 ms /     5 runs   (    0.28 ms per token,  3636.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   79081.09 ms /  1073 tokens (   73.70 ms per token,    13.57 tokens per second)\n",
      "llama_print_timings:        eval time =     850.66 ms /     4 runs   (  212.66 ms per token,     4.70 tokens per second)\n",
      "llama_print_timings:       total time =   82007.95 ms /  1077 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第8页共46页第十八篇：2022年4月18日字节跳动机器学习AILab一面面试题6道\n",
      "1、BN和LN区别\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "2、讲讲selfattention\n",
      "SelfAttention与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的\n",
      "隐变量（hiddenstate）计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但\n",
      "SelfAttention不同，它分别在source端和target端进行，仅与sourceinput或者targetinput自身相关\n",
      "的SelfAttention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的\n",
      "selfAttention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系。\n",
      "因此，selfAttentionAttention比传统的Attentionmechanism效果要好，主要原因之一是，传统的\n",
      "Attention机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，selfAttention可以不仅可以得\n",
      "到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系。\n",
      "3、Bert的预训练过程\n",
      "Bert的预训练主要包含两个任务，MLM和NSP，MaskedLanguageModel任务可以理解为完形填\n",
      "空，随机mask每一个句子中15%的词，用其上下文来做预测；NextSentencePrediction任务选择一些句\n",
      "子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中\n",
      "的相关性。BERT预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的Loss相加。\n",
      "4、PreNorm与PostNorm的区别？\n",
      "参考：https://kexue.fm/archives/9009\n",
      "在同一设置下，PreNorm（也就是Normandadd）的效果是要优于PostNorm（AddandNorm）\n",
      "的，但是单独调整的话，PostNorm的效果是更好的，PreNorm结构无形地增加了模型的宽度而降低了模</paragraph>\n",
      "Prediction: [Relevant]​[No support / Contradictory][Utility:5]\n",
      "Score: 1.1204651873525742\n",
      "\u001b[0m\u001b[1;3;34m1/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.57 ms /    50 runs   (    0.25 ms per token,  3978.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   72783.27 ms /   865 tokens (   84.14 ms per token,    11.88 tokens per second)\n",
      "llama_print_timings:        eval time =    9716.23 ms /    49 runs   (  198.29 ms per token,     5.04 tokens per second)\n",
      "llama_print_timings:       total time =   84667.85 ms /   914 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第39页共46页第二十八篇：京东科技NLP实习面试题10道\n",
      "1、Bert里面为什么用layernormalization，而不用batchnormalization，分别讲一下这\n",
      "两个啥意思。\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "2.、Bert里面为什么Q，K，V要用三个不同的矩阵，用一个不是也行吗\n",
      "如果使用相同的矩阵，相同量级的情况下，q和k进行点积的值会是最大的，进行softmax的加权平均\n",
      "后，该词所占的比重会最大，使得其他词的比重很少，无法有效利用上下文信息来增强当前词的语义表示，而\n",
      "使用不同的QKV后，会很大程度减轻上述的影响。\n",
      "3、Bert和transformer讲一下\n",
      "1bert只有transformer的encode结构，是生成语言模型\n",
      "2bert加入了输入句子的mask机制，在输入的时候会随机mask\n",
      "3模型接收两个句子作为输入，并且预测其中第二个句子是否在原始文档中也是后续句子可以做对话机\n",
      "制的应答。\n",
      "4在训练BERT模型时，MaskedLM和NextSentencePrediction是一起训练的，目标就是要最小化\n",
      "两种策略的组合损失函数。\n",
      "4、AUC指标讲一下\n",
      "AUC：AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。AUC反映的是分\n",
      "类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。</paragraph>\n",
      "Prediction: [Relevant]AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "Score: 0.8590345020754301\n",
      "\u001b[0m\u001b[1;3;34m2/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      13.02 ms /    52 runs   (    0.25 ms per token,  3994.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   49882.44 ms /   663 tokens (   75.24 ms per token,    13.29 tokens per second)\n",
      "llama_print_timings:        eval time =   10088.16 ms /    51 runs   (  197.81 ms per token,     5.06 tokens per second)\n",
      "llama_print_timings:       total time =   61603.50 ms /   714 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第9页共46页型的深度，PostNorm每Norm一次就削弱一次恒等分支的权重，所以PostNorm反而是更突出残差分支\n",
      "的。\n",
      "参考：https://zhuanlan.zhihu.com/p/474988236\n",
      "post-norm在残差之后做归一化，对参数正则化的效果更强，进而模型的鲁棒性也会更好；pre-norm\n",
      "相对于post-norm，因为有一部分参数直接加在了后面，不需要对这部分参数进行正则化，正好可以防止模\n",
      "型的梯度爆炸或者梯度消失，因此，如果层数少post-norm的效果其实要好一些，如果要把层数加大，为了\n",
      "保证模型的训练，pre-norm显然更好一些。\n",
      "5、GPT与Bert的区别\n",
      "1）GPT是单向模型，无法利用上下文信息，只能利用上文；而BERT是双向模型。\n",
      "2）GPT是基于自回归模型，可以应用在NLU和NLG两大任务，而原生的BERT采用的基于自编码模\n",
      "型，只能完成NLU任务，无法直接应用在文本生成上面。\n",
      "6、如何加速Bert模型的训练\n",
      "BERT基线模型的训练使用Adamwithweightdecay（Adam优化器的变体）作为优化器，LAMB是一\n",
      "款通用优化器，它适用于小批量和大批量，且除了学习率以外其他超参数均无需调整。LAMB优化器支持自\n",
      "适应元素级更新（adaptiveelement-wiseupdating）和准确的逐层修正（layer-wisecorrection）。\n",
      "LAMB可将BERT预训练的批量大小扩展到64K，且不会造成准确率损失，76分钟就可以完成BERT的训\n",
      "练。</paragraph>\n",
      "Prediction: [Relevant]参考：https://zhuanlan.zhihu.com/p/474988236\n",
      "LAMB优化器可以将BERT预训练\n",
      "Score: 0.7307614573140608\n",
      "\u001b[0m\u001b[1;3;34m3/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       1.12 ms /     5 runs   (    0.22 ms per token,  4460.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =   75082.76 ms /   978 tokens (   76.77 ms per token,    13.03 tokens per second)\n",
      "llama_print_timings:        eval time =     789.96 ms /     4 runs   (  197.49 ms per token,     5.06 tokens per second)\n",
      "llama_print_timings:       total time =   77871.34 ms /   982 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第36页共46页第二十七篇：京东广告NLP实习面试题7道\n",
      "1、BeamSearch生成的句子基本都一样，是否有方法扩展生成句子的多样性\n",
      "解决方法：通过分组加入相似性惩罚，具体可以参考论文Diversebeamsearch：\n",
      "https://arxiv.org/pdf/1610.02424.pdf。\n",
      "具体方法：选择Beamsize为B，然后将其分为G组，每一组就有B/G个beam，每个单独的组内跟\n",
      "beamsearch很像，不断延展序列，同时引入一个dissimilarity项来保证组与组之间有差异。\n",
      "组内与beamsearch很像：从t-1到t时刻，不断的减少搜索空间（如同beamsearch一样）。\n",
      "组间差异：对于t=4时刻，我们先对第一组输出y（t=4），然后我们开始对第二组输出y（t=4），但\n",
      "是第二组y（t=4）的score不仅取决于第二组之前的y（t=3），也取决于其与第一组的相似程度。以此类\n",
      "推，在t=4时刻对于第三组的输出，我们从上图可以看到其score的打分标准。这儿对于其dissimilarity项\n",
      "的计算采用的办法是hammingdiversity，这个理解起来很简单，比如这个时刻可能输出的词在上面的组出\n",
      "现过，我们就对这个词的分数-1，如果这个时刻可能输出的词在上面组没有出现过，我们就对这个词的分数不\n",
      "惩罚。\n",
      "2、LayerNormalization和BatchNormalization的区别，padding对这两者有影响吗，\n",
      "对哪一维有影响\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样\n",
      "本的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "padding会对BatchNormalization的seq_len这个维度有影响，计算的时候会把padding也算进去。\n",
      "3、pytorch.Dataloader报错出现数据维度不一样怎么解决。\n",
      "在构建dataset重写的__getitem__方法中要返回相同长度的tensor。\n",
      "可以使用向量补全的方法来解决这个问题，把不同长度的向量补全成等长的。</paragraph>\n",
      "Prediction: [Relevant]​[No support / Contradictory][Utility:5]\n",
      "Score: 1.2103178809228488\n",
      "\u001b[0m\u001b[1;3;34m4/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      13.63 ms /    50 runs   (    0.27 ms per token,  3668.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =   73166.46 ms /   957 tokens (   76.45 ms per token,    13.08 tokens per second)\n",
      "llama_print_timings:        eval time =   10614.21 ms /    49 runs   (  216.62 ms per token,     4.62 tokens per second)\n",
      "llama_print_timings:       total time =   85914.47 ms /  1006 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第34页共46页第二十六篇：京东健康NLP实习面试题5道\n",
      "1、RNN为什么会出现梯度消失的现象\n",
      "梯度消失的原因：很难捕捉到长期的依赖关系，因为乘法梯度可以随着层的数量呈指数递减/递增。但需\n",
      "要注意的是RNN中总的梯度是不会消失的。即便梯度越传越弱，那也只是远距离的梯度消失，由于近距离的\n",
      "梯度不会消失，所有梯度之和便不会消失。RNN所谓梯度消失的真正含义是，梯度被近距离梯度主导，导致\n",
      "模型难以学到远距离的依赖关系。\n",
      "2、RNN和LSTM的区别\n",
      "RNN没有细胞状态；LSTM通过细胞状态记忆信息。\n",
      "RNN激活函数只有tanh；LSTM通过输入门、遗忘门、输出门引入sigmoid函数并结合tanh函数，添\n",
      "加求和操作，减少梯度消失和梯度爆炸的可能性。\n",
      "RNN只能够处理短期依赖问题；LSTM既能够处理短期依赖问题，又能够处理长期依赖问题。\n",
      "3、word2vec有几种方式\n",
      "word2vec有CBOW模型和Skip-Gram模型。\n",
      "CBOW模型：输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向\n",
      "量。（ContinuousBag-of-Word）\n",
      "Skip-Gram模型：输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。\n",
      "有两种改进方式：\n",
      "一种是基于HierarchicalSoftmax的，另一种是基于NegativeSampling的。\n",
      "4、greedysearch和beamsearch的区别\n",
      "greedysearch比较简单，就是贪婪式的搜索，每一步都选择概率最大的单词输出，最后组成整个句子输\n",
      "出。这种方法给出的结果一般情况结果比较差，因为只考虑了每一步的最优解，往往里全局最优解差距很大。\n",
      "优点：计算速度快，每次取概率最大的词。缺点：局部最优并不等于全局最好的，而且一旦选错了，后\n",
      "续生成的内容很可能也是错误的，具有错误的累加效果。</paragraph>\n",
      "Prediction: [Relevant]LN 是一种近邻模型，其中每个观测都是一个特征向量，观测之间的相关性由一个特定的函\n",
      "Score: 0.7838164176298077\n",
      "\u001b[0m\u001b[1;3;34m5/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.82 ms /    50 runs   (    0.26 ms per token,  3899.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   65672.09 ms /   879 tokens (   74.71 ms per token,    13.38 tokens per second)\n",
      "llama_print_timings:        eval time =    9847.03 ms /    49 runs   (  200.96 ms per token,     4.98 tokens per second)\n",
      "llama_print_timings:       total time =   77468.04 ms /   928 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第43页共46页好处是这样做大大节约了参数量。\n",
      "5、如何防止过拟合\n",
      "数据的角度：获取和使用更多的数据（数据集增强）；\n",
      "模型角度：降低模型复杂度、L1\\L2\\Dropout正则化、Earlystopping（提前终止）\n",
      "模型融合的角度：使用bagging等模型融合方法。\n",
      "6、BN在训练和测试的时候的区别？可以防止过拟合吗？\n",
      "对于BN，在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。\n",
      "而在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全\n",
      "量训练数据的均值和方差，这个可以通过移动平均法求得。\n",
      "对于BN，当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，gamma和bata。\n",
      "BN算法防止过拟合：在网络的训练中，BN的使用使得一个minibatch中所有样本都被关联在了一起，\n",
      "因此网络不会从某一个训练样本中生成确定的结果，即同样一个样本的输出不再仅仅取决于样本的本身，也取\n",
      "决于跟这个样本同属一个batch的其他样本，而每次网络都是随机取batch，这样就会使得整个网络不会朝\n",
      "这一个方向使劲学习。一定程度上避免了过拟合。\n",
      "7、什么是AUC？\n",
      "AUC：AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。AUC反映的是分\n",
      "类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。\n",
      "8、卷积核计算公式\n",
      "卷积层计算公式如下：\n",
      "其中，W为输入大小，K为卷积核大小，P为padding大小，S为步幅。\n",
      "如果，想保持卷积前后的特征图大小相同，通常会设定padding为：</paragraph>\n",
      "Prediction: [Relevant]卷积核计算公式如下：\n",
      "其中，W为输入大小，K为卷积核大小，P为padding大小，S为\n",
      "Score: 0.6867971081852774\n",
      "\u001b[0m\u001b[1;3;34m6/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.81 ms /    50 runs   (    0.26 ms per token,  3902.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   57682.46 ms /   757 tokens (   76.20 ms per token,    13.12 tokens per second)\n",
      "llama_print_timings:        eval time =    9765.61 ms /    49 runs   (  199.30 ms per token,     5.02 tokens per second)\n",
      "llama_print_timings:       total time =   69237.36 ms /   806 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第21页共46页第二十二篇：2022年5月18日2023届广联达提前批面试题5道\n",
      "1、Transformer中encoder和decoder的区别（结构和功能上）\n",
      "结构上：Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个\n",
      "Block，Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder\n",
      "端的Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一\n",
      "个前馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和Layer\n",
      "Normalization层。\n",
      "功能上：Transformer中encoder是双向的，是编码器；decoder是单向的，是解码器，可以用于生成\n",
      "任务。\n",
      "2、有哪些防止过拟合的方法\n",
      "数据的角度：获取和使用更多的数据（数据集增强）；\n",
      "模型角度：降低模型复杂度、L1\\L2\\Dropout正则化、Earlystopping（提前终止）\n",
      "模型融合的角度：使用bagging等模型融合方法。\n",
      "3、L1和L2正则化为什么可以防止过拟合\n",
      "正规化是防止过拟合的一种重要技巧。正则化通过降低模型的复杂性，达到避免过拟合的问题。这里的\n",
      "降低模型的复杂性可以理解为：\n",
      "L1将很多权重变成0，这样起到作用的因素就会减少。\n",
      "L2使权重都趋于0，这样就不会有某个权重占比特别大。\n",
      "4、传统机器学习方法了解多少\n",
      "这个可以说逻辑回归模型，逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度\n",
      "下降法来求解参数，从而达到将数据二分类的目的。\n",
      "逻辑回归的优缺点\n",
      "优点：</paragraph>\n",
      "Prediction: [Relevant]逻辑回归模型是一种基本的机器学习模型，其中包括了一些基本的机器学习术语\n",
      "Score: 0.6283624230223188\n",
      "\u001b[0m\u001b[1;3;34m7/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.80 ms /    52 runs   (    0.25 ms per token,  4062.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   97929.24 ms /  1244 tokens (   78.72 ms per token,    12.70 tokens per second)\n",
      "llama_print_timings:        eval time =   10602.27 ms /    51 runs   (  207.89 ms per token,     4.81 tokens per second)\n",
      "llama_print_timings:       total time =  111368.23 ms /  1295 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第2页共46页5、生成式模型和判别式模型的区别并举一些例子.22\n",
      "第二十三篇：2022年4月大厂常考Leetcode面试题6道.23\n",
      "1、剑指offer29：顺时针打印矩阵.23\n",
      "2、剑指offer11：旋转数组的最小数字.24\n",
      "3、LeetCode69：x的平方根.24\n",
      "4、LeetCode22：括号生成.24\n",
      "5、LeetCode123：买卖股票的最佳时机III.25\n",
      "6、LeetCode3：无重复字符的最长子串.25\n",
      "第二十四篇：2022年5月10日美团搜索推荐算法面试题10道.27\n",
      "1、介绍下推荐系统的流程.27\n",
      "2、召回和排序的差异？27\n",
      "3、结果f1提升的1%怎么保证有效性，如何保证置信呢？27\n",
      "4、固定随机种子后，多次实验结果相同吗？27\n",
      "5、召回主流的做法.28\n",
      "6、介绍下embedding召回.28\n",
      "7、推荐系统冷启动问题，怎么解决.28\n",
      "8、LeetCode101—对称二叉树.28\n",
      "9、LeetCode3—无重复字符的最长子串.29\n",
      "10、LeetcCode130—被围绕的区域.29\n",
      "第二十五篇：2022年4月字节视频架构暑期实习面试题5道.31\n",
      "1、图像处理的基本知识：直方图均衡化、维纳滤波、锐化的操作.31\n",
      "2、Leetcode912—排序数组.31\n",
      "3、Leetcode102—二叉树层序遍历.32\n",
      "4、Leetcode200—岛屿数量.32\n",
      "5、Leetcode48—旋转图像.33\n",
      "第二十六篇：京东健康NLP实习面试题5道.34\n",
      "1、RNN为什么会出现梯度消失的现象.34\n",
      "2、RNN和LSTM的区别.34\n",
      "3、word2vec有几种方式.34\n",
      "4、greedysearch和beamsearch的区别.34\n",
      "5、你了解的文本表示方法有哪些.35\n",
      "第二十七篇：京东广告NLP实习面试题7道.36\n",
      "1、BeamSearch生成的句子基本都一样，是否有方法扩展生成句子的多样性.36\n",
      "2、LayerNormalization和BatchNormalization的区别，padding对这两者有影响吗，对哪一维有影响.36\n",
      "3、pytorch.Dataloader报错出现数据维度不一样怎么解决。36\n",
      "4、无序数组，找topK，要求比快排快.37\n",
      "5、Bert里面mask的用处。38\n",
      "6、对于两个词怎么算他们的相似度，用基于wordembedding的方式。38\n",
      "7、Leetcode—最大子序列和.38\n",
      "第二十八篇：京东科技NLP实习面试题10道.39\n",
      "1、Bert里面为什么用layernormalization，而不用batchnormalization，分别讲一下这两个啥意思。39\n",
      "2.、Bert里面为什么Q，K，V要用三个不同的矩阵，用一个不是也行吗.39\n",
      "3、Bert和transformer讲一下.39\n",
      "4、AUC指标讲一下.39\n",
      "5、Precision和Recall讲一下.40\n",
      "6、GBDT和Xgboost的区别.40</paragraph>\n",
      "Prediction: [Relevant]LN 是一种常见的训练损失函数，它是一种损失函数，用于训练机器学习\n",
      "Score: 0.7488686516654387\n",
      "\u001b[0m\u001b[1;3;34m8/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.54 ms /    50 runs   (    0.25 ms per token,  3985.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =   62242.99 ms /   831 tokens (   74.90 ms per token,    13.35 tokens per second)\n",
      "llama_print_timings:        eval time =    9565.36 ms /    49 runs   (  195.21 ms per token,     5.12 tokens per second)\n",
      "llama_print_timings:       total time =   73627.74 ms /   880 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第16页共46页4、wide&deep模型为什么要有wide层结构，优缺点，如何改进？\n",
      "wide&deep模型中的wide部分可以通过利用交叉特征引入非线性高效的实现记忆能力，但需要依赖人\n",
      "工特征工程。\n",
      "改进：DeepFM在Wide&Deep的基础上进行改进，不需要预训练FM得到隐向量，不需要人工特征工\n",
      "程，能同时学习低阶和高阶的组合特征；FM模块和Deep模块共享FeatureEmbedding部分，可以更快的\n",
      "训练，以及更精确的训练学习。\n",
      "5、推荐领域GBDT+LR的做法了解吗？\n",
      "GBDT+LR由两部分组成，其中GBDT用来对训练集提取特征作为新的训练输入数据，LR作为新训练输\n",
      "入数据的分类器。GBDT+LR的提出意味着特征工程可以完全交由一个独立的模型来完成，模型的输入可以是\n",
      "原始的特征向量，不必在特征工程上投入过多的人工筛选和模型设计的精力，真正实现了端到端的训练。\n",
      "6、粗排有哪些指标？NDCG了解吗？\n",
      "(1)NDCG，排序相似性的指标，看精排的排序结果和粗排有多相似\n",
      "(2)粗排的召回率/重叠率，粗排的topk和精排的topk有多大占比。\n",
      "计算DCG，计算公式如下：\n",
      "其中，rel为这个排序list，结果i的一个等级得分；i是指结果i的当前位置序号；\n",
      "​在搜索引擎中，rel等级得分，是由人工抽样数据，并且根据一定的规则打出来的等级得分。\n",
      "步骤二：\n",
      "计算IDCG（IdealDCG），即完美序的DCG；计算方式也同步骤1，只是排序序列不是由算法得出，而\n",
      "是由人工对序列根据一定的评估准则排出来的最佳序列。\n",
      "​步骤三：根据前面2个步骤的出来的结果，计算NDCG，计算公式如下：</paragraph>\n",
      "Prediction: [Relevant](3) 排序质量指标（Quality Indicators for Sorting），这个指标是一个排序质量的概括指标\n",
      "Score: 0.7305473359381379\n",
      "\u001b[0m\u001b[1;3;34m9/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.44 ms /    51 runs   (    0.24 ms per token,  4100.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   44850.20 ms /   604 tokens (   74.26 ms per token,    13.47 tokens per second)\n",
      "llama_print_timings:        eval time =    9444.86 ms /    50 runs   (  188.90 ms per token,     5.29 tokens per second)\n",
      "llama_print_timings:       total time =   55795.00 ms /   654 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "BN 和 LN 区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第41页共46页8、LightGBM对于Xgboost有什么改进\n",
      "模型精度：XGBoost和LightGBM相当。\n",
      "训练速度：LightGBM远快于XGBoost。(快百倍以上，跟数据集有关系)\n",
      "内存消耗：LightGBM远小于XGBoost。(大约是xgb的五分之一)\n",
      "缺失值特征：XGBoost和LightGBM都可以自动处理特征缺失值。\n",
      "分类特征：XGBoost不支持类别特征，需要OneHot编码预处理。LightGBM直接支持类别特征。\n",
      "LightGBM在XGBoost上主要有3方面的优化。\n",
      "1，Histogram算法:直方图算法。\n",
      "2，GOSS算法:基于梯度的单边采样算法。\n",
      "3，EFB算法:互斥特征捆绑算法。\n",
      "9、防止过拟合的方式\n",
      "降低模型复杂度\n",
      "增加更多的训练数据：使用更大的数据集训练模型\n",
      "数据增强\n",
      "正则化：L1、L2、添加BN层\n",
      "添加Dropout策略\n",
      "EarlyStopping\n",
      "10、Adam讲一下\n",
      "Adam算法即自适应时刻估计方法（AdaptiveMomentEstimation），能计算每个参数的自适应学习\n",
      "率。这个方法不仅存储了AdaDelta先前平方梯度的指数衰减平均值，而且保持了先前梯度M(t)的指数衰减\n",
      "平均值，这一点与动量类似。\n",
      "Adam实际上就是将Momentum和RMSprop集合在一起，把一阶动量和二阶动量都使用起来了。</paragraph>\n",
      "Prediction: [Relevant]BN是一种常见的神经网络中的一种特征，用来减少梯度下降的梯度衰减\n",
      "Score: 0.5218602448979809\n",
      "\u001b[0m\u001b[1;3;34m10/10 paragraphs done\n",
      "\n",
      "\u001b[0m\u001b[1;3;34mEnd evaluation\n",
      "\u001b[0m\u001b[1;3;34mSelected the best answer: [Relevant]​[No support / Contradictory][Utility:5]\n",
      "\u001b[0m\u001b[1;3;32mFinal answer: ​\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.43 ms /    50 runs   (    0.25 ms per token,  4023.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1107.46 ms /    15 tokens (   73.83 ms per token,    13.54 tokens per second)\n",
      "llama_print_timings:        eval time =    8560.35 ms /    49 runs   (  174.70 ms per token,     5.72 tokens per second)\n",
      "llama_print_timings:       total time =    9850.25 ms /    64 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieval required\n",
      "\u001b[0m\u001b[1;3;34mReceived: 10 documents\n",
      "\u001b[0m\u001b[1;3;34mStart evaluation\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.45 ms /    50 runs   (    0.25 ms per token,  4014.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =   52798.97 ms /   707 tokens (   74.68 ms per token,    13.39 tokens per second)\n",
      "llama_print_timings:        eval time =    9342.08 ms /    49 runs   (  190.65 ms per token,     5.25 tokens per second)\n",
      "llama_print_timings:       total time =   63700.22 ms /   756 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第42页共46页第二十九篇：2022年6月24日字节电商CV实习岗面试题10道\n",
      "1、如何解决类别极度不平衡的问题？\n",
      "2、说下Transformer模型\n",
      "Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个Block，\n",
      "Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder端的\n",
      "Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一个前\n",
      "馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和LayerNormalization\n",
      "层。\n",
      "3、说下FocalLoss\n",
      "Focalloss是目标检测中解决正负样本严重不平衡的方法，在标准交叉熵损失基础上修改得到的。这个函\n",
      "数可以通过减少易分类样本的权重，使得模型在训练时更专注于稀疏的难分类的样本；防止大量易分类负样本\n",
      "在loss中占主导地位。\n",
      "4、介绍下深度可分离卷积和传统卷积的区别\n",
      "传统的卷积是各个通道上采用相不同的卷积核，然后不同的卷积核用于提取不同方面的特征。\n",
      "深度可分离卷积先在各个通道上采用不同的卷积核提取不同的特征，但是这样对于某个通道来说，就只提\n",
      "取了一方面的特征，因此在此基础上加入点卷积，用1*1的卷积对提取特征后的特征图再次提取不同方面的\n",
      "特征，最终产生和普通卷积相同的输出特征图。</paragraph>\n",
      "Prediction: [Relevant]自我注意力（Self Attention）是一种神经语言模型中的一种重要概念，用于模型中的自我参与和内部连接。[Fully supported]\n",
      "Score: 1.491432015330191\n",
      "\u001b[0m\u001b[1;3;34m1/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.88 ms /    50 runs   (    0.26 ms per token,  3881.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   82588.27 ms /  1070 tokens (   77.19 ms per token,    12.96 tokens per second)\n",
      "llama_print_timings:        eval time =   10303.16 ms /    49 runs   (  210.27 ms per token,     4.76 tokens per second)\n",
      "llama_print_timings:       total time =   95250.98 ms /  1119 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第8页共46页第十八篇：2022年4月18日字节跳动机器学习AILab一面面试题6道\n",
      "1、BN和LN区别\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "2、讲讲selfattention\n",
      "SelfAttention与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的\n",
      "隐变量（hiddenstate）计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但\n",
      "SelfAttention不同，它分别在source端和target端进行，仅与sourceinput或者targetinput自身相关\n",
      "的SelfAttention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的\n",
      "selfAttention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系。\n",
      "因此，selfAttentionAttention比传统的Attentionmechanism效果要好，主要原因之一是，传统的\n",
      "Attention机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，selfAttention可以不仅可以得\n",
      "到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系。\n",
      "3、Bert的预训练过程\n",
      "Bert的预训练主要包含两个任务，MLM和NSP，MaskedLanguageModel任务可以理解为完形填\n",
      "空，随机mask每一个句子中15%的词，用其上下文来做预测；NextSentencePrediction任务选择一些句\n",
      "子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中\n",
      "的相关性。BERT预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的Loss相加。\n",
      "4、PreNorm与PostNorm的区别？\n",
      "参考：https://kexue.fm/archives/9009\n",
      "在同一设置下，PreNorm（也就是Normandadd）的效果是要优于PostNorm（AddandNorm）\n",
      "的，但是单独调整的话，PostNorm的效果是更好的，PreNorm结构无形地增加了模型的宽度而降低了模</paragraph>\n",
      "Prediction: [Relevant]以上是对“self attention”的解释，具体如何使用和实现，需要根据自己的需求和使用场景进行选择和\n",
      "Score: 0.5622732080042753\n",
      "\u001b[0m\u001b[1;3;34m2/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.61 ms /    52 runs   (    0.24 ms per token,  4123.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   74091.42 ms /   958 tokens (   77.34 ms per token,    12.93 tokens per second)\n",
      "llama_print_timings:        eval time =   10599.81 ms /    51 runs   (  207.84 ms per token,     4.81 tokens per second)\n",
      "llama_print_timings:       total time =   86846.78 ms /  1009 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第34页共46页第二十六篇：京东健康NLP实习面试题5道\n",
      "1、RNN为什么会出现梯度消失的现象\n",
      "梯度消失的原因：很难捕捉到长期的依赖关系，因为乘法梯度可以随着层的数量呈指数递减/递增。但需\n",
      "要注意的是RNN中总的梯度是不会消失的。即便梯度越传越弱，那也只是远距离的梯度消失，由于近距离的\n",
      "梯度不会消失，所有梯度之和便不会消失。RNN所谓梯度消失的真正含义是，梯度被近距离梯度主导，导致\n",
      "模型难以学到远距离的依赖关系。\n",
      "2、RNN和LSTM的区别\n",
      "RNN没有细胞状态；LSTM通过细胞状态记忆信息。\n",
      "RNN激活函数只有tanh；LSTM通过输入门、遗忘门、输出门引入sigmoid函数并结合tanh函数，添\n",
      "加求和操作，减少梯度消失和梯度爆炸的可能性。\n",
      "RNN只能够处理短期依赖问题；LSTM既能够处理短期依赖问题，又能够处理长期依赖问题。\n",
      "3、word2vec有几种方式\n",
      "word2vec有CBOW模型和Skip-Gram模型。\n",
      "CBOW模型：输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向\n",
      "量。（ContinuousBag-of-Word）\n",
      "Skip-Gram模型：输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。\n",
      "有两种改进方式：\n",
      "一种是基于HierarchicalSoftmax的，另一种是基于NegativeSampling的。\n",
      "4、greedysearch和beamsearch的区别\n",
      "greedysearch比较简单，就是贪婪式的搜索，每一步都选择概率最大的单词输出，最后组成整个句子输\n",
      "出。这种方法给出的结果一般情况结果比较差，因为只考虑了每一步的最优解，往往里全局最优解差距很大。\n",
      "优点：计算速度快，每次取概率最大的词。缺点：局部最优并不等于全局最好的，而且一旦选错了，后\n",
      "续生成的内容很可能也是错误的，具有错误的累加效果。</paragraph>\n",
      "Prediction: [Relevant]beamsearch是一种受限搜索算法，它的思想是每一步都选择概率最大的单词，但\n",
      "Score: 0.7340960660487243\n",
      "\u001b[0m\u001b[1;3;34m3/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.87 ms /    50 runs   (    0.26 ms per token,  3886.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   55755.25 ms /   707 tokens (   78.86 ms per token,    12.68 tokens per second)\n",
      "llama_print_timings:        eval time =    9813.88 ms /    49 runs   (  200.28 ms per token,     4.99 tokens per second)\n",
      "llama_print_timings:       total time =   67309.75 ms /   756 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第28页共46页5、召回主流的做法\n",
      "主流的召回做法包括：规则召回，协同召回，基于内容语义的I2I召回，向量召回（基于\n",
      "embedding），树召回和图召回。\n",
      "6、介绍下embedding召回\n",
      "举一个文本类embedding的例子。\n",
      "文本类的Embedding可以分为两种，一种是比较传统的word2vector、fasttext、glove这些算法的方\n",
      "案，叫做词向量固定表征类算法，这些算法主要是通过分析词的出现频率来进行Embedding生成，不考虑文\n",
      "本上下文。\n",
      "而另一种文本Embedding方法，也是目前最流行的方案是动态词表征算法，比如Bert、ELMo、GPT，\n",
      "这类算法会考虑文本上下文。\n",
      "7、推荐系统冷启动问题，怎么解决\n",
      "1.提供非个性化的推荐\n",
      "最简单的例子就是热门排行榜，我们可以给用户推荐热门排行榜，然后等到用户数据收集到一定的时候，\n",
      "再切换为个性化推荐。\n",
      "2.利用用户注册信息\n",
      "用户注册时提供包括用户的年龄、性别、职业、民族、学历和居住地等数据，做粗粒度的个性化。有一些\n",
      "网站还会让用户用文字描述他们的兴趣。\n",
      "3.利用社交网络信息\n",
      "引导用户通过社交网络账号登录（需要用户授权），导入用户在社交网站上的好友信息，然后给用户推荐\n",
      "其好友喜欢的物品。\n",
      "8、LeetCode101—对称二叉树\n",
      "1.classSolution(object):\n",
      "2. defisSymmetric(self,root):\n",
      "3. \"\"\"\n",
      "4. :typeroot:TreeNode\n",
      "5. :rtype:bool\n",
      "6. \"\"\"\n",
      "7. ifnotroot:</paragraph>\n",
      "Prediction: [Irrelevant]这是一个Python代码，用来实现对称二叉树的Solution。\n",
      "```python\n",
      "class Solution:\n",
      "    def isSymmetric(self, root):\n",
      "        if not root:\n",
      "Score: 0.46780706691910545\n",
      "\u001b[0m\u001b[1;3;34m4/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      13.12 ms /    50 runs   (    0.26 ms per token,  3811.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   80077.67 ms /  1013 tokens (   79.05 ms per token,    12.65 tokens per second)\n",
      "llama_print_timings:        eval time =   10747.48 ms /    49 runs   (  219.34 ms per token,     4.56 tokens per second)\n",
      "llama_print_timings:       total time =   93154.50 ms /  1062 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第15页共46页第二十篇：2022年4月携程暑期实习搜索推荐算法岗面试题8道\n",
      "1、你所理解的推荐系统是什么样的？大致流程？所用模型？\n",
      "推荐系统一方面是为了帮助消费者发现对自己有价值的商品，另一方面是帮助生产者把用户可能感兴趣的\n",
      "商品展现给用户，实现生产者和消费者的双赢。\n",
      "大致流程主要包括：获取用户特征，召回过程，排序过程（粗排、精排）\n",
      "召回模型：\n",
      "·规则召回（兴趣标签top，热门top，新品top等）\n",
      "·协同召回（基于用户的协同过滤，基于商品的协同过滤）\n",
      "·向量召回（FM召回，Item2vec,YoutubeDNN向量召回，GraphEmbedding召回，DSSM双塔召\n",
      "回）\n",
      "排序模型：GBDT+LR、Xgboost、FM/FFM、Wide&Deep、DeepFM、Deep&Cross、DIN、BST\n",
      "等\n",
      "2、双塔模型优势，缺点，如何改进？\n",
      "双塔模型的优势是速度快，但模型精度还有待提升。\n",
      "速度快是因为将所有Item转化成Embedding，并存储进ANN检索系统，比如FAISS，以供查询。类\n",
      "似FAISS这种ANN检索系统对海量数据的查询效率高。\n",
      "而双塔模型为了速度快，必须对用户和Item进行特征分离，而特征分离，又必然导致上述两个原因产生\n",
      "的效果损失。\n",
      "改进：SENet双塔模型，把SENet放在Embedding层之上，目的是通过SENet网络，动态地学习这些\n",
      "特征的重要性：对于每个特征学会一个特征权重，然后再把学习到的权重乘到对应特征的Embedding里，这\n",
      "样就可以动态学习特征权重，通过小权重抑制噪音或者无效低频特征，通过大权重放大重要特征影响的目的。\n",
      "3、粗排的目的是什么？\n",
      "粗排是用来帮精排模型找到那些它本来就要打高分的item，只不过范围更广一些。.按照上面的例子，如\n",
      "果没有粗排，精排模型自己找出来的某top10的item。而粗排的任务就是要找到包含这10个item的一个更\n",
      "小的候选集，既保证了效果，又减少线上预测的负担。</paragraph>\n",
      "Prediction: [Relevant]4、如何理解协同召回？\n",
      "协同召回是基于用户的协同过滤和基于商品的协同过\n",
      "Score: 0.7598202403977503\n",
      "\u001b[0m\u001b[1;3;34m5/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      13.53 ms /    50 runs   (    0.27 ms per token,  3696.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =   81521.54 ms /  1039 tokens (   78.46 ms per token,    12.75 tokens per second)\n",
      "llama_print_timings:        eval time =   11338.90 ms /    49 runs   (  231.41 ms per token,     4.32 tokens per second)\n",
      "llama_print_timings:       total time =   95238.45 ms /  1088 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第27页共46页第二十四篇：2022年5月10日美团搜索推荐算法面试题10道\n",
      "1、介绍下推荐系统的流程\n",
      "推荐系统的流程主要包含一下几个阶段。\n",
      "索引&特征:会根据内容特性提前建立若干种类型的索引。\n",
      "召回阶段:用户请求时会从各种索引种取出千/万条item。\n",
      "粗排阶段:针对这上千/万条item，进行第一遍打分，再筛选出几百条或者千条。这个阶段的排序模型一\n",
      "般都比较简单，能够过滤掉一些与用户兴趣明显不相关的。\n",
      "精排阶段:得到几百条item后，精排阶段会建立相对精细的模型，根据用户的画像，偏好，上下文，结\n",
      "合业务目标进行排序。一般精排后返回50-100条给到engine侧。\n",
      "重排阶段:engine侧拿到精排的50条item。还会做很多的人工干预和产品逻辑，比如item之间的多样\n",
      "性，产品策略逻辑，比如热门，置顶，多种内容之间的位置混合等等。最终会返回5-10条左右的item，曝\n",
      "光给客户端。根据业务特性，在线流程还有许多比较细的模块，比如去重服务，避免给用户推荐重复的内容。\n",
      "特征预处理，特征抽取等模块。\n",
      "2、召回和排序的差异？\n",
      "召回的目的在于减少候选的数量（尽量控制在1000以内），方便后续排序环节使用复杂模型精准排序；\n",
      "因为在短时间内评估海量候选，所以召回的关键点是个快字，受限与此与排序相比，召回的算法模型相对简\n",
      "单，使用的特征比较少。而排序模型相对更加复杂，更追求准确性，使用的特征也会较多。\n",
      "3、结果f1提升的1%怎么保证有效性，如何保证置信呢？\n",
      "实验过程中固定随机种子、多次实验取平均\n",
      "4、固定随机种子后，多次实验结果相同吗？\n",
      "还是会有细微差别，因为在梯度传播过程，梯度（浮点数）精度有差异，随着神经网络层数的增加，梯度\n",
      "差异会从小数后面的位置往前跑。只能设置浮点数精度增加来缓解这个问题。</paragraph>\n",
      "Prediction: [Relevant]这个问题可以通过设置浮点数精度来缓解，但是要注意的是，更高的浮点数精度不一\n",
      "Score: 0.6054243433721611\n",
      "\u001b[0m\u001b[1;3;34m6/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.56 ms /    50 runs   (    0.25 ms per token,  3980.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47534.04 ms /   616 tokens (   77.17 ms per token,    12.96 tokens per second)\n",
      "llama_print_timings:        eval time =    9862.53 ms /    49 runs   (  201.28 ms per token,     4.97 tokens per second)\n",
      "llama_print_timings:       total time =   59017.64 ms /   665 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第35页共46页beamsearch是介于全局搜索和贪婪搜索之间。使用beamsize参数来限制在每一步保留下来的可能性\n",
      "词的数量。beamsearch是在测试阶段为了获得更好准确性而采取的一种策略，在训练阶段无需使用。\n",
      "相比暴力搜索的全局最优解，降低时间复杂度的方法就是寻找次优解，具体就是把搜索空间中的N减下\n",
      "来，每一步计算完只保留K个(beamsize)最大的取值路径，这样时间复杂度降为O(K*N*T)，K取值一般比\n",
      "N小很多。这样得到的虽然不是最优解，但是在seq2seq模型的推理预测中可以兼顾时间和效果。\n",
      "5、你了解的文本表示方法有哪些\n",
      "基于one-hot、tf-idf、textrank等的bag-of-words；\n",
      "主题模型：LSA（SVD）、pLSA、LDA；\n",
      "基于词向量的固定表征：word2vec、fastText、glove\n",
      "基于词向量的动态表征：elmo、GPT、bert\n",
      "对比如下：\n",
      "One-hot表示：维度灾难、语义鸿沟；\n",
      "矩阵分解（LSA）：利用全局语料特征，但SVD求解计算复杂度大；\n",
      "基于NNLM/RNNLM的词向量：词向量为副产物，存在效率不高等问题；\n",
      "word2vec、fastText：优化效率高，但是基于局部语料；\n",
      "glove：基于全局预料，结合了LSA和word2vec的优点；\n",
      "elmo、GPT、bert：动态特征。</paragraph>\n",
      "Prediction: [Irrelevant]你可以在这里获取更多信息：https://www.alibabacloud.com/blog/self-attention-in-transformer-models_594292?spm\n",
      "Score: 0.4853524170170509\n",
      "\u001b[0m\u001b[1;3;34m7/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      13.14 ms /    50 runs   (    0.26 ms per token,  3804.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =   58649.10 ms /   757 tokens (   77.48 ms per token,    12.91 tokens per second)\n",
      "llama_print_timings:        eval time =    9990.36 ms /    49 runs   (  203.88 ms per token,     4.90 tokens per second)\n",
      "llama_print_timings:       total time =   70426.45 ms /   806 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第21页共46页第二十二篇：2022年5月18日2023届广联达提前批面试题5道\n",
      "1、Transformer中encoder和decoder的区别（结构和功能上）\n",
      "结构上：Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个\n",
      "Block，Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder\n",
      "端的Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一\n",
      "个前馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和Layer\n",
      "Normalization层。\n",
      "功能上：Transformer中encoder是双向的，是编码器；decoder是单向的，是解码器，可以用于生成\n",
      "任务。\n",
      "2、有哪些防止过拟合的方法\n",
      "数据的角度：获取和使用更多的数据（数据集增强）；\n",
      "模型角度：降低模型复杂度、L1\\L2\\Dropout正则化、Earlystopping（提前终止）\n",
      "模型融合的角度：使用bagging等模型融合方法。\n",
      "3、L1和L2正则化为什么可以防止过拟合\n",
      "正规化是防止过拟合的一种重要技巧。正则化通过降低模型的复杂性，达到避免过拟合的问题。这里的\n",
      "降低模型的复杂性可以理解为：\n",
      "L1将很多权重变成0，这样起到作用的因素就会减少。\n",
      "L2使权重都趋于0，这样就不会有某个权重占比特别大。\n",
      "4、传统机器学习方法了解多少\n",
      "这个可以说逻辑回归模型，逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度\n",
      "下降法来求解参数，从而达到将数据二分类的目的。\n",
      "逻辑回归的优缺点\n",
      "优点：</paragraph>\n",
      "Prediction: [Relevant]逻辑回归模型是一种基本的机器学习模型，其中，输出为一个分类器，输入为一个特征向\n",
      "Score: 0.6522245571265552\n",
      "\u001b[0m\u001b[1;3;34m8/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      13.59 ms /    50 runs   (    0.27 ms per token,  3679.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =   70375.90 ms /   831 tokens (   84.69 ms per token,    11.81 tokens per second)\n",
      "llama_print_timings:        eval time =   10091.93 ms /    49 runs   (  205.96 ms per token,     4.86 tokens per second)\n",
      "llama_print_timings:       total time =   82794.10 ms /   880 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第16页共46页4、wide&deep模型为什么要有wide层结构，优缺点，如何改进？\n",
      "wide&deep模型中的wide部分可以通过利用交叉特征引入非线性高效的实现记忆能力，但需要依赖人\n",
      "工特征工程。\n",
      "改进：DeepFM在Wide&Deep的基础上进行改进，不需要预训练FM得到隐向量，不需要人工特征工\n",
      "程，能同时学习低阶和高阶的组合特征；FM模块和Deep模块共享FeatureEmbedding部分，可以更快的\n",
      "训练，以及更精确的训练学习。\n",
      "5、推荐领域GBDT+LR的做法了解吗？\n",
      "GBDT+LR由两部分组成，其中GBDT用来对训练集提取特征作为新的训练输入数据，LR作为新训练输\n",
      "入数据的分类器。GBDT+LR的提出意味着特征工程可以完全交由一个独立的模型来完成，模型的输入可以是\n",
      "原始的特征向量，不必在特征工程上投入过多的人工筛选和模型设计的精力，真正实现了端到端的训练。\n",
      "6、粗排有哪些指标？NDCG了解吗？\n",
      "(1)NDCG，排序相似性的指标，看精排的排序结果和粗排有多相似\n",
      "(2)粗排的召回率/重叠率，粗排的topk和精排的topk有多大占比。\n",
      "计算DCG，计算公式如下：\n",
      "其中，rel为这个排序list，结果i的一个等级得分；i是指结果i的当前位置序号；\n",
      "​在搜索引擎中，rel等级得分，是由人工抽样数据，并且根据一定的规则打出来的等级得分。\n",
      "步骤二：\n",
      "计算IDCG（IdealDCG），即完美序的DCG；计算方式也同步骤1，只是排序序列不是由算法得出，而\n",
      "是由人工对序列根据一定的评估准则排出来的最佳序列。\n",
      "​步骤三：根据前面2个步骤的出来的结果，计算NDCG，计算公式如下：</paragraph>\n",
      "Prediction: [Relevant](3) 排序的时间，排序的时间越长越好。\n",
      "(4) 排序的空间，排序的空间越少越好。\n",
      "Score: 0.7340953330576359\n",
      "\u001b[0m\u001b[1;3;34m9/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       8.57 ms /    33 runs   (    0.26 ms per token,  3848.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =   79170.12 ms /   978 tokens (   80.95 ms per token,    12.35 tokens per second)\n",
      "llama_print_timings:        eval time =    6731.35 ms /    32 runs   (  210.35 ms per token,     4.75 tokens per second)\n",
      "llama_print_timings:       total time =   88402.55 ms /  1010 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "讲讲self attention\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第36页共46页第二十七篇：京东广告NLP实习面试题7道\n",
      "1、BeamSearch生成的句子基本都一样，是否有方法扩展生成句子的多样性\n",
      "解决方法：通过分组加入相似性惩罚，具体可以参考论文Diversebeamsearch：\n",
      "https://arxiv.org/pdf/1610.02424.pdf。\n",
      "具体方法：选择Beamsize为B，然后将其分为G组，每一组就有B/G个beam，每个单独的组内跟\n",
      "beamsearch很像，不断延展序列，同时引入一个dissimilarity项来保证组与组之间有差异。\n",
      "组内与beamsearch很像：从t-1到t时刻，不断的减少搜索空间（如同beamsearch一样）。\n",
      "组间差异：对于t=4时刻，我们先对第一组输出y（t=4），然后我们开始对第二组输出y（t=4），但\n",
      "是第二组y（t=4）的score不仅取决于第二组之前的y（t=3），也取决于其与第一组的相似程度。以此类\n",
      "推，在t=4时刻对于第三组的输出，我们从上图可以看到其score的打分标准。这儿对于其dissimilarity项\n",
      "的计算采用的办法是hammingdiversity，这个理解起来很简单，比如这个时刻可能输出的词在上面的组出\n",
      "现过，我们就对这个词的分数-1，如果这个时刻可能输出的词在上面组没有出现过，我们就对这个词的分数不\n",
      "惩罚。\n",
      "2、LayerNormalization和BatchNormalization的区别，padding对这两者有影响吗，\n",
      "对哪一维有影响\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样\n",
      "本的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "padding会对BatchNormalization的seq_len这个维度有影响，计算的时候会把padding也算进去。\n",
      "3、pytorch.Dataloader报错出现数据维度不一样怎么解决。\n",
      "在构建dataset重写的__getitem__方法中要返回相同长度的tensor。\n",
      "可以使用向量补全的方法来解决这个问题，把不同长度的向量补全成等长的。</paragraph>\n",
      "Prediction: [Relevant]I'm sorry, but I'm not sure what you're asking.[No support / Contradictory][No Retrieval]Could you please provide more context or clarify your question?[Utility:4]\n",
      "Score: 1.019263046241107\n",
      "\u001b[0m\u001b[1;3;34m10/10 paragraphs done\n",
      "\n",
      "\u001b[0m\u001b[1;3;34mEnd evaluation\n",
      "\u001b[0m\u001b[1;3;34mSelected the best answer: [Relevant]自我注意力（Self Attention）是一种神经语言模型中的一种重要概念，用于模型中的自我参与和内部连接。[Fully supported]\n",
      "\u001b[0m\u001b[1;3;32mFinal answer: 自我注意力（Self Attention）是一种神经语言模型中的一种重要概念，用于模型中的自我参与和内部连接。\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       8.06 ms /    32 runs   (    0.25 ms per token,  3969.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1814.17 ms /    22 tokens (   82.46 ms per token,    12.13 tokens per second)\n",
      "llama_print_timings:        eval time =    5542.71 ms /    31 runs   (  178.80 ms per token,     5.59 tokens per second)\n",
      "llama_print_timings:       total time =    7493.82 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieval required\n",
      "\u001b[0m\u001b[1;3;34mReceived: 10 documents\n",
      "\u001b[0m\u001b[1;3;34mStart evaluation\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.82 ms /    50 runs   (    0.26 ms per token,  3900.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =   51572.05 ms /   666 tokens (   77.44 ms per token,    12.91 tokens per second)\n",
      "llama_print_timings:        eval time =    9812.95 ms /    49 runs   (  200.26 ms per token,     4.99 tokens per second)\n",
      "llama_print_timings:       total time =   62909.45 ms /   715 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第9页共46页型的深度，PostNorm每Norm一次就削弱一次恒等分支的权重，所以PostNorm反而是更突出残差分支\n",
      "的。\n",
      "参考：https://zhuanlan.zhihu.com/p/474988236\n",
      "post-norm在残差之后做归一化，对参数正则化的效果更强，进而模型的鲁棒性也会更好；pre-norm\n",
      "相对于post-norm，因为有一部分参数直接加在了后面，不需要对这部分参数进行正则化，正好可以防止模\n",
      "型的梯度爆炸或者梯度消失，因此，如果层数少post-norm的效果其实要好一些，如果要把层数加大，为了\n",
      "保证模型的训练，pre-norm显然更好一些。\n",
      "5、GPT与Bert的区别\n",
      "1）GPT是单向模型，无法利用上下文信息，只能利用上文；而BERT是双向模型。\n",
      "2）GPT是基于自回归模型，可以应用在NLU和NLG两大任务，而原生的BERT采用的基于自编码模\n",
      "型，只能完成NLU任务，无法直接应用在文本生成上面。\n",
      "6、如何加速Bert模型的训练\n",
      "BERT基线模型的训练使用Adamwithweightdecay（Adam优化器的变体）作为优化器，LAMB是一\n",
      "款通用优化器，它适用于小批量和大批量，且除了学习率以外其他超参数均无需调整。LAMB优化器支持自\n",
      "适应元素级更新（adaptiveelement-wiseupdating）和准确的逐层修正（layer-wisecorrection）。\n",
      "LAMB可将BERT预训练的批量大小扩展到64K，且不会造成准确率损失，76分钟就可以完成BERT的训\n",
      "练。</paragraph>\n",
      "Prediction: [Relevant]参考：https://zhuanlan.zhihu.com/p/474988236\n",
      "LAMB优化器可以将BERT的训练\n",
      "Score: 0.7654733645318321\n",
      "\u001b[0m\u001b[1;3;34m1/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      13.01 ms /    50 runs   (    0.26 ms per token,  3842.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =   83512.51 ms /  1070 tokens (   78.05 ms per token,    12.81 tokens per second)\n",
      "llama_print_timings:        eval time =   10408.16 ms /    49 runs   (  212.41 ms per token,     4.71 tokens per second)\n",
      "llama_print_timings:       total time =   96318.50 ms /  1119 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第8页共46页第十八篇：2022年4月18日字节跳动机器学习AILab一面面试题6道\n",
      "1、BN和LN区别\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "2、讲讲selfattention\n",
      "SelfAttention与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的\n",
      "隐变量（hiddenstate）计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但\n",
      "SelfAttention不同，它分别在source端和target端进行，仅与sourceinput或者targetinput自身相关\n",
      "的SelfAttention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的\n",
      "selfAttention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系。\n",
      "因此，selfAttentionAttention比传统的Attentionmechanism效果要好，主要原因之一是，传统的\n",
      "Attention机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，selfAttention可以不仅可以得\n",
      "到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系。\n",
      "3、Bert的预训练过程\n",
      "Bert的预训练主要包含两个任务，MLM和NSP，MaskedLanguageModel任务可以理解为完形填\n",
      "空，随机mask每一个句子中15%的词，用其上下文来做预测；NextSentencePrediction任务选择一些句\n",
      "子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中\n",
      "的相关性。BERT预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的Loss相加。\n",
      "4、PreNorm与PostNorm的区别？\n",
      "参考：https://kexue.fm/archives/9009\n",
      "在同一设置下，PreNorm（也就是Normandadd）的效果是要优于PostNorm（AddandNorm）\n",
      "的，但是单独调整的话，PostNorm的效果是更好的，PreNorm结构无形地增加了模型的宽度而降低了模</paragraph>\n",
      "Prediction: [Relevant]参考：https://kexue.fm/archives/9009\n",
      "在同一设置下，PreNorm（也就是Normandadd）的效果是要优于\n",
      "Score: 0.5427709817942077\n",
      "\u001b[0m\u001b[1;3;34m2/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.54 ms /    50 runs   (    0.23 ms per token,  4330.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   66041.01 ms /   865 tokens (   76.35 ms per token,    13.10 tokens per second)\n",
      "llama_print_timings:        eval time =    9512.64 ms /    49 runs   (  194.14 ms per token,     5.15 tokens per second)\n",
      "llama_print_timings:       total time =   77551.51 ms /   914 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第39页共46页第二十八篇：京东科技NLP实习面试题10道\n",
      "1、Bert里面为什么用layernormalization，而不用batchnormalization，分别讲一下这\n",
      "两个啥意思。\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "2.、Bert里面为什么Q，K，V要用三个不同的矩阵，用一个不是也行吗\n",
      "如果使用相同的矩阵，相同量级的情况下，q和k进行点积的值会是最大的，进行softmax的加权平均\n",
      "后，该词所占的比重会最大，使得其他词的比重很少，无法有效利用上下文信息来增强当前词的语义表示，而\n",
      "使用不同的QKV后，会很大程度减轻上述的影响。\n",
      "3、Bert和transformer讲一下\n",
      "1bert只有transformer的encode结构，是生成语言模型\n",
      "2bert加入了输入句子的mask机制，在输入的时候会随机mask\n",
      "3模型接收两个句子作为输入，并且预测其中第二个句子是否在原始文档中也是后续句子可以做对话机\n",
      "制的应答。\n",
      "4在训练BERT模型时，MaskedLM和NextSentencePrediction是一起训练的，目标就是要最小化\n",
      "两种策略的组合损失函数。\n",
      "4、AUC指标讲一下\n",
      "AUC：AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。AUC反映的是分\n",
      "类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。</paragraph>\n",
      "Prediction: [Relevant]BERT模型的预训练过程是在句子上进行了卷积和激活函数，以获取句子中的\n",
      "Score: 0.8266907574176545\n",
      "\u001b[0m\u001b[1;3;34m3/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.61 ms /    50 runs   (    0.25 ms per token,  3966.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   76748.86 ms /  1013 tokens (   75.76 ms per token,    13.20 tokens per second)\n",
      "llama_print_timings:        eval time =    9894.06 ms /    49 runs   (  201.92 ms per token,     4.95 tokens per second)\n",
      "llama_print_timings:       total time =   88787.49 ms /  1062 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第15页共46页第二十篇：2022年4月携程暑期实习搜索推荐算法岗面试题8道\n",
      "1、你所理解的推荐系统是什么样的？大致流程？所用模型？\n",
      "推荐系统一方面是为了帮助消费者发现对自己有价值的商品，另一方面是帮助生产者把用户可能感兴趣的\n",
      "商品展现给用户，实现生产者和消费者的双赢。\n",
      "大致流程主要包括：获取用户特征，召回过程，排序过程（粗排、精排）\n",
      "召回模型：\n",
      "·规则召回（兴趣标签top，热门top，新品top等）\n",
      "·协同召回（基于用户的协同过滤，基于商品的协同过滤）\n",
      "·向量召回（FM召回，Item2vec,YoutubeDNN向量召回，GraphEmbedding召回，DSSM双塔召\n",
      "回）\n",
      "排序模型：GBDT+LR、Xgboost、FM/FFM、Wide&Deep、DeepFM、Deep&Cross、DIN、BST\n",
      "等\n",
      "2、双塔模型优势，缺点，如何改进？\n",
      "双塔模型的优势是速度快，但模型精度还有待提升。\n",
      "速度快是因为将所有Item转化成Embedding，并存储进ANN检索系统，比如FAISS，以供查询。类\n",
      "似FAISS这种ANN检索系统对海量数据的查询效率高。\n",
      "而双塔模型为了速度快，必须对用户和Item进行特征分离，而特征分离，又必然导致上述两个原因产生\n",
      "的效果损失。\n",
      "改进：SENet双塔模型，把SENet放在Embedding层之上，目的是通过SENet网络，动态地学习这些\n",
      "特征的重要性：对于每个特征学会一个特征权重，然后再把学习到的权重乘到对应特征的Embedding里，这\n",
      "样就可以动态学习特征权重，通过小权重抑制噪音或者无效低频特征，通过大权重放大重要特征影响的目的。\n",
      "3、粗排的目的是什么？\n",
      "粗排是用来帮精排模型找到那些它本来就要打高分的item，只不过范围更广一些。.按照上面的例子，如\n",
      "果没有粗排，精排模型自己找出来的某top10的item。而粗排的任务就是要找到包含这10个item的一个更\n",
      "小的候选集，既保证了效果，又减少线上预测的负担。</paragraph>\n",
      "Prediction: [Relevant]粗排模型主要是基于协同召回模型，协同召回模型主要是基于协同召回算法\n",
      "Score: 0.7957932645418677\n",
      "\u001b[0m\u001b[1;3;34m4/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.17 ms /    50 runs   (    0.24 ms per token,  4108.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   56925.42 ms /   757 tokens (   75.20 ms per token,    13.30 tokens per second)\n",
      "llama_print_timings:        eval time =    9660.27 ms /    49 runs   (  197.15 ms per token,     5.07 tokens per second)\n",
      "llama_print_timings:       total time =   68296.89 ms /   806 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第21页共46页第二十二篇：2022年5月18日2023届广联达提前批面试题5道\n",
      "1、Transformer中encoder和decoder的区别（结构和功能上）\n",
      "结构上：Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个\n",
      "Block，Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder\n",
      "端的Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一\n",
      "个前馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和Layer\n",
      "Normalization层。\n",
      "功能上：Transformer中encoder是双向的，是编码器；decoder是单向的，是解码器，可以用于生成\n",
      "任务。\n",
      "2、有哪些防止过拟合的方法\n",
      "数据的角度：获取和使用更多的数据（数据集增强）；\n",
      "模型角度：降低模型复杂度、L1\\L2\\Dropout正则化、Earlystopping（提前终止）\n",
      "模型融合的角度：使用bagging等模型融合方法。\n",
      "3、L1和L2正则化为什么可以防止过拟合\n",
      "正规化是防止过拟合的一种重要技巧。正则化通过降低模型的复杂性，达到避免过拟合的问题。这里的\n",
      "降低模型的复杂性可以理解为：\n",
      "L1将很多权重变成0，这样起到作用的因素就会减少。\n",
      "L2使权重都趋于0，这样就不会有某个权重占比特别大。\n",
      "4、传统机器学习方法了解多少\n",
      "这个可以说逻辑回归模型，逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度\n",
      "下降法来求解参数，从而达到将数据二分类的目的。\n",
      "逻辑回归的优缺点\n",
      "优点：</paragraph>\n",
      "Prediction: [Relevant]这里的“逻辑回归”可以理解为一种基本的机器学习模型，其中“极大似然”可以理\n",
      "Score: 0.6839272548977428\n",
      "\u001b[0m\u001b[1;3;34m5/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.89 ms /    50 runs   (    0.26 ms per token,  3877.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47888.26 ms /   616 tokens (   77.74 ms per token,    12.86 tokens per second)\n",
      "llama_print_timings:        eval time =    9587.14 ms /    49 runs   (  195.66 ms per token,     5.11 tokens per second)\n",
      "llama_print_timings:       total time =   58939.28 ms /   665 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第35页共46页beamsearch是介于全局搜索和贪婪搜索之间。使用beamsize参数来限制在每一步保留下来的可能性\n",
      "词的数量。beamsearch是在测试阶段为了获得更好准确性而采取的一种策略，在训练阶段无需使用。\n",
      "相比暴力搜索的全局最优解，降低时间复杂度的方法就是寻找次优解，具体就是把搜索空间中的N减下\n",
      "来，每一步计算完只保留K个(beamsize)最大的取值路径，这样时间复杂度降为O(K*N*T)，K取值一般比\n",
      "N小很多。这样得到的虽然不是最优解，但是在seq2seq模型的推理预测中可以兼顾时间和效果。\n",
      "5、你了解的文本表示方法有哪些\n",
      "基于one-hot、tf-idf、textrank等的bag-of-words；\n",
      "主题模型：LSA（SVD）、pLSA、LDA；\n",
      "基于词向量的固定表征：word2vec、fastText、glove\n",
      "基于词向量的动态表征：elmo、GPT、bert\n",
      "对比如下：\n",
      "One-hot表示：维度灾难、语义鸿沟；\n",
      "矩阵分解（LSA）：利用全局语料特征，但SVD求解计算复杂度大；\n",
      "基于NNLM/RNNLM的词向量：词向量为副产物，存在效率不高等问题；\n",
      "word2vec、fastText：优化效率高，但是基于局部语料；\n",
      "glove：基于全局预料，结合了LSA和word2vec的优点；\n",
      "elmo、GPT、bert：动态特征。</paragraph>\n",
      "Prediction: [Relevant]GPT-2、BERT等模型基于词向量动态表示，其中BERT使用了Transformer模型，可以实现语义相关性，语义相关性\n",
      "Score: 0.703241653366312\n",
      "\u001b[0m\u001b[1;3;34m6/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.34 ms /    49 runs   (    0.25 ms per token,  3972.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   79042.70 ms /  1039 tokens (   76.08 ms per token,    13.14 tokens per second)\n",
      "llama_print_timings:        eval time =    9871.49 ms /    48 runs   (  205.66 ms per token,     4.86 tokens per second)\n",
      "llama_print_timings:       total time =   91288.58 ms /  1087 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第27页共46页第二十四篇：2022年5月10日美团搜索推荐算法面试题10道\n",
      "1、介绍下推荐系统的流程\n",
      "推荐系统的流程主要包含一下几个阶段。\n",
      "索引&特征:会根据内容特性提前建立若干种类型的索引。\n",
      "召回阶段:用户请求时会从各种索引种取出千/万条item。\n",
      "粗排阶段:针对这上千/万条item，进行第一遍打分，再筛选出几百条或者千条。这个阶段的排序模型一\n",
      "般都比较简单，能够过滤掉一些与用户兴趣明显不相关的。\n",
      "精排阶段:得到几百条item后，精排阶段会建立相对精细的模型，根据用户的画像，偏好，上下文，结\n",
      "合业务目标进行排序。一般精排后返回50-100条给到engine侧。\n",
      "重排阶段:engine侧拿到精排的50条item。还会做很多的人工干预和产品逻辑，比如item之间的多样\n",
      "性，产品策略逻辑，比如热门，置顶，多种内容之间的位置混合等等。最终会返回5-10条左右的item，曝\n",
      "光给客户端。根据业务特性，在线流程还有许多比较细的模块，比如去重服务，避免给用户推荐重复的内容。\n",
      "特征预处理，特征抽取等模块。\n",
      "2、召回和排序的差异？\n",
      "召回的目的在于减少候选的数量（尽量控制在1000以内），方便后续排序环节使用复杂模型精准排序；\n",
      "因为在短时间内评估海量候选，所以召回的关键点是个快字，受限与此与排序相比，召回的算法模型相对简\n",
      "单，使用的特征比较少。而排序模型相对更加复杂，更追求准确性，使用的特征也会较多。\n",
      "3、结果f1提升的1%怎么保证有效性，如何保证置信呢？\n",
      "实验过程中固定随机种子、多次实验取平均\n",
      "4、固定随机种子后，多次实验结果相同吗？\n",
      "还是会有细微差别，因为在梯度传播过程，梯度（浮点数）精度有差异，随着神经网络层数的增加，梯度\n",
      "差异会从小数后面的位置往前跑。只能设置浮点数精度增加来缓解这个问题。</paragraph>\n",
      "Prediction: [Relevant]因此，在实验中，我们需要确保我们的实验数据是一致的，以确保我们的结果是可信的。[No support / Contradictory][Utility:5]\n",
      "Score: 1.2328546356547072\n",
      "\u001b[0m\u001b[1;3;34m7/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.63 ms /    50 runs   (    0.25 ms per token,  3958.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   87705.56 ms /  1149 tokens (   76.33 ms per token,    13.10 tokens per second)\n",
      "llama_print_timings:        eval time =   10363.05 ms /    49 runs   (  211.49 ms per token,     4.73 tokens per second)\n",
      "llama_print_timings:       total time =  100599.21 ms /  1198 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第1页共46页目录\n",
      "第十七篇：2022年4月10日百度机器学习方向暑期实习面试题6道.4\n",
      "1、介绍下SVM算法.4\n",
      "2、介绍下逻辑回归算法.4\n",
      "3、介绍下决策树算法.5\n",
      "4、用通俗的语言介绍下强化学习（ReinforcementLearning）.5\n",
      "5、Leetcode34在排序数组中查找元素的第一个和最后一个位置.5\n",
      "6、Letcode102层序遍历.6\n",
      "第十八篇：2022年4月18日字节跳动机器学习AILab一面面试题6道.8\n",
      "1、BN和LN区别.8\n",
      "2、讲讲selfattention.8\n",
      "3、Bert的预训练过程.8\n",
      "4、PreNorm与PostNorm的区别？8\n",
      "5、GPT与Bert的区别.9\n",
      "6、如何加速Bert模型的训练.9\n",
      "第十九篇：2022年4月8日字节跳动机抖音APP推荐实习面试题8道.11\n",
      "1、AUC是什么？如何计算AUC？11\n",
      "2、AUC线上线下不一致怎么办.11\n",
      "3、召回阶段的负采样是怎么做的？12\n",
      "4、FM，DeepFM跟FFM的对比.12\n",
      "5、手撕FM的训练过程.12\n",
      "6、Leetcode—64.最小路径和.13\n",
      "7、剑指Offer10-I.斐波那契数列.14\n",
      "8、Leetcode—215.数组中的第K个最大元素.14\n",
      "第二十篇：2022年4月携程暑期实习搜索推荐算法岗面试题8道.15\n",
      "1、你所理解的推荐系统是什么样的？大致流程？所用模型？15\n",
      "2、双塔模型优势，缺点，如何改进？15\n",
      "3、粗排的目的是什么？15\n",
      "4、wide&deep模型为什么要有wide层结构，优缺点，如何改进？16\n",
      "5、推荐领域GBDT+LR的做法了解吗？16\n",
      "6、粗排有哪些指标？NDCG了解吗？16\n",
      "7、ROC，PR曲线含义，坐标轴代表什么？17\n",
      "8、AUC怎么求，实际意义？17\n",
      "第二十一篇：2022年4月14日美团计算机视觉算法暑期实习面试题6道.18\n",
      "1、简单介绍gbdt算法的原理.18\n",
      "2、pca属于有监督还是无监督.18\n",
      "3、介绍svm算法.18\n",
      "4、介绍transformer算法.19\n",
      "5、layernorm和batchnorm的比较.19\n",
      "6、Leetcode—两数之和.19\n",
      "第二十二篇：2022年5月18日2023届广联达提前批面试题5道.21\n",
      "1、Transformer中encoder和decoder的区别（结构和功能上）.21\n",
      "2、有哪些防止过拟合的方法.21\n",
      "3、L1和L2正则化为什么可以防止过拟合.21\n",
      "4、传统机器学习方法了解多少.21</paragraph>\n",
      "Prediction: [Relevant]这些内容主要包括了BERT模型的基本概念和预训练过程，以及其他相关知识。[No support / Contradictory]\n",
      "Score: 0.8003372310175744\n",
      "\u001b[0m\u001b[1;3;34m8/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.80 ms /    51 runs   (    0.25 ms per token,  3982.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   55412.06 ms /   704 tokens (   78.71 ms per token,    12.70 tokens per second)\n",
      "llama_print_timings:        eval time =    9905.00 ms /    50 runs   (  198.10 ms per token,     5.05 tokens per second)\n",
      "llama_print_timings:       total time =   67093.09 ms /   754 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第42页共46页第二十九篇：2022年6月24日字节电商CV实习岗面试题10道\n",
      "1、如何解决类别极度不平衡的问题？\n",
      "2、说下Transformer模型\n",
      "Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个Block，\n",
      "Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder端的\n",
      "Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一个前\n",
      "馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和LayerNormalization\n",
      "层。\n",
      "3、说下FocalLoss\n",
      "Focalloss是目标检测中解决正负样本严重不平衡的方法，在标准交叉熵损失基础上修改得到的。这个函\n",
      "数可以通过减少易分类样本的权重，使得模型在训练时更专注于稀疏的难分类的样本；防止大量易分类负样本\n",
      "在loss中占主导地位。\n",
      "4、介绍下深度可分离卷积和传统卷积的区别\n",
      "传统的卷积是各个通道上采用相不同的卷积核，然后不同的卷积核用于提取不同方面的特征。\n",
      "深度可分离卷积先在各个通道上采用不同的卷积核提取不同的特征，但是这样对于某个通道来说，就只提\n",
      "取了一方面的特征，因此在此基础上加入点卷积，用1*1的卷积对提取特征后的特征图再次提取不同方面的\n",
      "特征，最终产生和普通卷积相同的输出特征图。</paragraph>\n",
      "Prediction: [Relevant]这里是Bert的预训练过程：\n",
      "\n",
      "1.[No support / Contradictory][No Retrieval]数据集选择和数据准备\n",
      "2.[No Retrieval]模型选择\n",
      "Score: 0.5538953334975258\n",
      "\u001b[0m\u001b[1;3;34m9/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.61 ms /    50 runs   (    0.25 ms per token,  3966.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =   69612.06 ms /   878 tokens (   79.28 ms per token,    12.61 tokens per second)\n",
      "llama_print_timings:        eval time =    9874.28 ms /    49 runs   (  201.52 ms per token,     4.96 tokens per second)\n",
      "llama_print_timings:       total time =   81595.12 ms /   927 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "Bert 的预训练过程\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第43页共46页好处是这样做大大节约了参数量。\n",
      "5、如何防止过拟合\n",
      "数据的角度：获取和使用更多的数据（数据集增强）；\n",
      "模型角度：降低模型复杂度、L1\\L2\\Dropout正则化、Earlystopping（提前终止）\n",
      "模型融合的角度：使用bagging等模型融合方法。\n",
      "6、BN在训练和测试的时候的区别？可以防止过拟合吗？\n",
      "对于BN，在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。\n",
      "而在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全\n",
      "量训练数据的均值和方差，这个可以通过移动平均法求得。\n",
      "对于BN，当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，gamma和bata。\n",
      "BN算法防止过拟合：在网络的训练中，BN的使用使得一个minibatch中所有样本都被关联在了一起，\n",
      "因此网络不会从某一个训练样本中生成确定的结果，即同样一个样本的输出不再仅仅取决于样本的本身，也取\n",
      "决于跟这个样本同属一个batch的其他样本，而每次网络都是随机取batch，这样就会使得整个网络不会朝\n",
      "这一个方向使劲学习。一定程度上避免了过拟合。\n",
      "7、什么是AUC？\n",
      "AUC：AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。AUC反映的是分\n",
      "类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。\n",
      "8、卷积核计算公式\n",
      "卷积层计算公式如下：\n",
      "其中，W为输入大小，K为卷积核大小，P为padding大小，S为步幅。\n",
      "如果，想保持卷积前后的特征图大小相同，通常会设定padding为：</paragraph>\n",
      "Prediction: [Relevant]卷积核计算公式如下：\n",
      "其中，W为输入大小，K为卷积核大小，P为padding大小，S为\n",
      "Score: 0.7518601301343627\n",
      "\u001b[0m\u001b[1;3;34m10/10 paragraphs done\n",
      "\n",
      "\u001b[0m\u001b[1;3;34mEnd evaluation\n",
      "\u001b[0m\u001b[1;3;34mSelected the best answer: [Relevant]因此，在实验中，我们需要确保我们的实验数据是一致的，以确保我们的结果是可信的。[No support / Contradictory][Utility:5]\n",
      "\u001b[0m\u001b[1;3;32mFinal answer: 因此，在实验中，我们需要确保我们的实验数据是一致的，以确保我们的结果是可信的。\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       8.42 ms /    33 runs   (    0.26 ms per token,  3921.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1420.97 ms /    18 tokens (   78.94 ms per token,    12.67 tokens per second)\n",
      "llama_print_timings:        eval time =    5801.54 ms /    32 runs   (  181.30 ms per token,     5.52 tokens per second)\n",
      "llama_print_timings:       total time =    7358.00 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieval required\n",
      "\u001b[0m\u001b[1;3;34mReceived: 10 documents\n",
      "\u001b[0m\u001b[1;3;34mStart evaluation\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.29 ms /    50 runs   (    0.25 ms per token,  4070.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   50071.12 ms /   666 tokens (   75.18 ms per token,    13.30 tokens per second)\n",
      "llama_print_timings:        eval time =    9549.31 ms /    49 runs   (  194.88 ms per token,     5.13 tokens per second)\n",
      "llama_print_timings:       total time =   61148.81 ms /   715 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第9页共46页型的深度，PostNorm每Norm一次就削弱一次恒等分支的权重，所以PostNorm反而是更突出残差分支\n",
      "的。\n",
      "参考：https://zhuanlan.zhihu.com/p/474988236\n",
      "post-norm在残差之后做归一化，对参数正则化的效果更强，进而模型的鲁棒性也会更好；pre-norm\n",
      "相对于post-norm，因为有一部分参数直接加在了后面，不需要对这部分参数进行正则化，正好可以防止模\n",
      "型的梯度爆炸或者梯度消失，因此，如果层数少post-norm的效果其实要好一些，如果要把层数加大，为了\n",
      "保证模型的训练，pre-norm显然更好一些。\n",
      "5、GPT与Bert的区别\n",
      "1）GPT是单向模型，无法利用上下文信息，只能利用上文；而BERT是双向模型。\n",
      "2）GPT是基于自回归模型，可以应用在NLU和NLG两大任务，而原生的BERT采用的基于自编码模\n",
      "型，只能完成NLU任务，无法直接应用在文本生成上面。\n",
      "6、如何加速Bert模型的训练\n",
      "BERT基线模型的训练使用Adamwithweightdecay（Adam优化器的变体）作为优化器，LAMB是一\n",
      "款通用优化器，它适用于小批量和大批量，且除了学习率以外其他超参数均无需调整。LAMB优化器支持自\n",
      "适应元素级更新（adaptiveelement-wiseupdating）和准确的逐层修正（layer-wisecorrection）。\n",
      "LAMB可将BERT预训练的批量大小扩展到64K，且不会造成准确率损失，76分钟就可以完成BERT的训\n",
      "练。</paragraph>\n",
      "Prediction: [Relevant]1) GPT is a unidirectional model, unable to utilize context information, only able to utilize previous text.[No support / Contradictory][No Retrieval]2) GPT is based on a recurrent model, able to be used for both NL\n",
      "Score: 0.8096211372466848\n",
      "\u001b[0m\u001b[1;3;34m1/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       6.78 ms /    28 runs   (    0.24 ms per token,  4130.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   54356.72 ms /   723 tokens (   75.18 ms per token,    13.30 tokens per second)\n",
      "llama_print_timings:        eval time =    5290.65 ms /    27 runs   (  195.95 ms per token,     5.10 tokens per second)\n",
      "llama_print_timings:       total time =   61162.30 ms /   750 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第40页共46页5、Precision和Recall讲一下\n",
      "精确度（precision）/查准率：TP/（TP+FP）=TP/P 预测为真中，实际为正样本的概率\n",
      "召回率（recall）/查全率：TP/（TP+FN）正样本中，被识别为真的概率。\n",
      "6、GBDT和Xgboost的区别\n",
      "1）GBDT是机器学习算法，XGBoost是该算法的一种工程实现\n",
      "2）XGBoost在使用CART作为基学习器时，加入了正则项来控制模型的复杂度，有利于防止过拟合，从\n",
      "而提高模型的泛化能力\n",
      "3）GBDT在模型训练时只使用了损失函数的一阶导数信息，XGBoost对损失函数进行二阶泰勒展开，可\n",
      "以同时使用一阶和二阶导数\n",
      "4）XGBoost支持自定义损失函数，增强了模型的扩展性\n",
      "5）传统的GBDT采用CART作为基学习器（也叫基分类器），XGBoost支持多种类型的基学习器，包\n",
      "括树模型（gbtree和dart，dart为一种引入dropout的树模型）和线性模型（gblinear），默认为gbtree\n",
      "6）传统的GBDT在每轮迭代时使用全部的数据，XGBoost支持对数据进列采样，即特征采样，有利于\n",
      "防止过拟合，同时可以减少计算量，提高训练的效率\n",
      "7）传统的GBDT不能支持缺失值的处理（必须填充），XGBoost支持缺失值的处理，能够自动学习出\n",
      "缺失值的分裂方向（无需填充）\n",
      "7、Xgboost叶子结点的值怎么计算的\n",
      "XGBoost目标函数最终推导形式如下：\n",
      "利用一元二次函数求最值的知识，当目标函数达到最小值Obj*时，每个叶子结点的权重为wj*。\n",
      "具体公式如下：</paragraph>\n",
      "Prediction: [Relevant]Bert 是一种语言模型，其中 GPT 是一种模型类型。[No support / Contradictory][Utility:5]\n",
      "Score: 1.3977941486621372\n",
      "\u001b[0m\u001b[1;3;34m2/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.49 ms /    50 runs   (    0.25 ms per token,  4004.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =   65597.55 ms /   861 tokens (   76.19 ms per token,    13.13 tokens per second)\n",
      "llama_print_timings:        eval time =    9776.02 ms /    49 runs   (  199.51 ms per token,     5.01 tokens per second)\n",
      "llama_print_timings:       total time =   77342.80 ms /   910 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第18页共46页第二十一篇：2022年4月14日美团计算机视觉算法暑期实习面试题6道\n",
      "1、简单介绍gbdt算法的原理\n",
      "GBDT是梯度提升决策树，是一种基于Boosting的算法，采用以决策树为基学习器的加法模型，通过不\n",
      "断拟合上一个弱学习器的残差，最终实现分类或回归的模型。关键在于利用损失函数的负梯度在当前模型的值\n",
      "作为残差的近似值，从而拟合一个回归树。对于分类问题：常使用指数损失函数；对于回归问题：常使用平方\n",
      "误差损失函数（此时，其负梯度就是通常意义的残差），对于一般损失函数来说就是残差的近似。\n",
      "无论损失函数是什么形式，每个决策树拟合的都是负梯度。准确的说，不是用负梯度代替残差，而是当损\n",
      "失函数是均方损失时，负梯度刚好是残差，残差只是特例。\n",
      "2、pca属于有监督还是无监督\n",
      "PCA按有监督和无监督划分应该属于无监督学习，所以数据集有无y并不重要，只是改变样本X的属性\n",
      "(特征)维度。\n",
      "3、介绍svm算法\n",
      "是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感\n",
      "知机。\n",
      "SVM可分为三种：\n",
      "线性可分SVM\n",
      "当训练数据线性可分时，通过最大化硬间隔（hardmargin）可以学习得到一个线性分类器，即硬间隔\n",
      "SVM。\n",
      "线性SVM\n",
      "当训练数据不能线性可分但是近似线性可分时，通过最大化软间隔（softmargin）也可以学习到一个线\n",
      "性分类器，即软间隔SVM。\n",
      "非线性SVM\n",
      "当训练数据线性不可分时，通过使用核技巧（kerneltrick）和最大化软间隔，可以学习到一个非线性\n",
      "SVM。</paragraph>\n",
      "Prediction: [Irrelevant]GBDT是梯度提升决策树，是一种基于Boosting的算法，采用以决策树为基\n",
      "Score: 0.48440533202222463\n",
      "\u001b[0m\u001b[1;3;34m3/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.79 ms /    50 runs   (    0.26 ms per token,  3908.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   81088.55 ms /  1070 tokens (   75.78 ms per token,    13.20 tokens per second)\n",
      "llama_print_timings:        eval time =   10249.40 ms /    49 runs   (  209.17 ms per token,     4.78 tokens per second)\n",
      "llama_print_timings:       total time =   93655.99 ms /  1119 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第8页共46页第十八篇：2022年4月18日字节跳动机器学习AILab一面面试题6道\n",
      "1、BN和LN区别\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "2、讲讲selfattention\n",
      "SelfAttention与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的\n",
      "隐变量（hiddenstate）计算Attention的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但\n",
      "SelfAttention不同，它分别在source端和target端进行，仅与sourceinput或者targetinput自身相关\n",
      "的SelfAttention，捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的\n",
      "selfAttention加入到target端得到的Attention中，捕捉source端和target端词与词之间的依赖关系。\n",
      "因此，selfAttentionAttention比传统的Attentionmechanism效果要好，主要原因之一是，传统的\n",
      "Attention机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，selfAttention可以不仅可以得\n",
      "到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系。\n",
      "3、Bert的预训练过程\n",
      "Bert的预训练主要包含两个任务，MLM和NSP，MaskedLanguageModel任务可以理解为完形填\n",
      "空，随机mask每一个句子中15%的词，用其上下文来做预测；NextSentencePrediction任务选择一些句\n",
      "子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中\n",
      "的相关性。BERT预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的Loss相加。\n",
      "4、PreNorm与PostNorm的区别？\n",
      "参考：https://kexue.fm/archives/9009\n",
      "在同一设置下，PreNorm（也就是Normandadd）的效果是要优于PostNorm（AddandNorm）\n",
      "的，但是单独调整的话，PostNorm的效果是更好的，PreNorm结构无形地增加了模型的宽度而降低了模</paragraph>\n",
      "Prediction: [Relevant]Bert是一种基于Transformer语言模型的模型，其中使用了一种特殊的预处理技术，通过将输入序列中一部\n",
      "Score: 0.5673533465102469\n",
      "\u001b[0m\u001b[1;3;34m4/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.40 ms /    50 runs   (    0.25 ms per token,  4032.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =   68036.69 ms /   865 tokens (   78.66 ms per token,    12.71 tokens per second)\n",
      "llama_print_timings:        eval time =    9523.69 ms /    49 runs   (  194.36 ms per token,     5.15 tokens per second)\n",
      "llama_print_timings:       total time =   79452.98 ms /   914 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第39页共46页第二十八篇：京东科技NLP实习面试题10道\n",
      "1、Bert里面为什么用layernormalization，而不用batchnormalization，分别讲一下这\n",
      "两个啥意思。\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "2.、Bert里面为什么Q，K，V要用三个不同的矩阵，用一个不是也行吗\n",
      "如果使用相同的矩阵，相同量级的情况下，q和k进行点积的值会是最大的，进行softmax的加权平均\n",
      "后，该词所占的比重会最大，使得其他词的比重很少，无法有效利用上下文信息来增强当前词的语义表示，而\n",
      "使用不同的QKV后，会很大程度减轻上述的影响。\n",
      "3、Bert和transformer讲一下\n",
      "1bert只有transformer的encode结构，是生成语言模型\n",
      "2bert加入了输入句子的mask机制，在输入的时候会随机mask\n",
      "3模型接收两个句子作为输入，并且预测其中第二个句子是否在原始文档中也是后续句子可以做对话机\n",
      "制的应答。\n",
      "4在训练BERT模型时，MaskedLM和NextSentencePrediction是一起训练的，目标就是要最小化\n",
      "两种策略的组合损失函数。\n",
      "4、AUC指标讲一下\n",
      "AUC：AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。AUC反映的是分\n",
      "类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。</paragraph>\n",
      "Prediction: [Relevant]AUC指标讲一下\n",
      "AUC：AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取\n",
      "Score: 0.8266546761612655\n",
      "\u001b[0m\u001b[1;3;34m5/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.94 ms /    50 runs   (    0.24 ms per token,  4188.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   74242.79 ms /   957 tokens (   77.58 ms per token,    12.89 tokens per second)\n",
      "llama_print_timings:        eval time =    9810.98 ms /    49 runs   (  200.22 ms per token,     4.99 tokens per second)\n",
      "llama_print_timings:       total time =   86214.38 ms /  1006 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第34页共46页第二十六篇：京东健康NLP实习面试题5道\n",
      "1、RNN为什么会出现梯度消失的现象\n",
      "梯度消失的原因：很难捕捉到长期的依赖关系，因为乘法梯度可以随着层的数量呈指数递减/递增。但需\n",
      "要注意的是RNN中总的梯度是不会消失的。即便梯度越传越弱，那也只是远距离的梯度消失，由于近距离的\n",
      "梯度不会消失，所有梯度之和便不会消失。RNN所谓梯度消失的真正含义是，梯度被近距离梯度主导，导致\n",
      "模型难以学到远距离的依赖关系。\n",
      "2、RNN和LSTM的区别\n",
      "RNN没有细胞状态；LSTM通过细胞状态记忆信息。\n",
      "RNN激活函数只有tanh；LSTM通过输入门、遗忘门、输出门引入sigmoid函数并结合tanh函数，添\n",
      "加求和操作，减少梯度消失和梯度爆炸的可能性。\n",
      "RNN只能够处理短期依赖问题；LSTM既能够处理短期依赖问题，又能够处理长期依赖问题。\n",
      "3、word2vec有几种方式\n",
      "word2vec有CBOW模型和Skip-Gram模型。\n",
      "CBOW模型：输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向\n",
      "量。（ContinuousBag-of-Word）\n",
      "Skip-Gram模型：输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。\n",
      "有两种改进方式：\n",
      "一种是基于HierarchicalSoftmax的，另一种是基于NegativeSampling的。\n",
      "4、greedysearch和beamsearch的区别\n",
      "greedysearch比较简单，就是贪婪式的搜索，每一步都选择概率最大的单词输出，最后组成整个句子输\n",
      "出。这种方法给出的结果一般情况结果比较差，因为只考虑了每一步的最优解，往往里全局最优解差距很大。\n",
      "优点：计算速度快，每次取概率最大的词。缺点：局部最优并不等于全局最好的，而且一旦选错了，后\n",
      "续生成的内容很可能也是错误的，具有错误的累加效果。</paragraph>\n",
      "Prediction: [Relevant]Bert 是一种基于 Transformer 模型的语言模型，其中 GPT 是一种基于 Recurrent Neural Networks (RNN) 的语言模型。[Partially supported][Utility:5]\n",
      "Score: 1.6567812233689607\n",
      "\u001b[0m\u001b[1;3;34m6/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.62 ms /    50 runs   (    0.23 ms per token,  4304.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   47042.24 ms /   615 tokens (   76.49 ms per token,    13.07 tokens per second)\n",
      "llama_print_timings:        eval time =    9367.69 ms /    49 runs   (  191.18 ms per token,     5.23 tokens per second)\n",
      "llama_print_timings:       total time =   57898.84 ms /   664 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第35页共46页beamsearch是介于全局搜索和贪婪搜索之间。使用beamsize参数来限制在每一步保留下来的可能性\n",
      "词的数量。beamsearch是在测试阶段为了获得更好准确性而采取的一种策略，在训练阶段无需使用。\n",
      "相比暴力搜索的全局最优解，降低时间复杂度的方法就是寻找次优解，具体就是把搜索空间中的N减下\n",
      "来，每一步计算完只保留K个(beamsize)最大的取值路径，这样时间复杂度降为O(K*N*T)，K取值一般比\n",
      "N小很多。这样得到的虽然不是最优解，但是在seq2seq模型的推理预测中可以兼顾时间和效果。\n",
      "5、你了解的文本表示方法有哪些\n",
      "基于one-hot、tf-idf、textrank等的bag-of-words；\n",
      "主题模型：LSA（SVD）、pLSA、LDA；\n",
      "基于词向量的固定表征：word2vec、fastText、glove\n",
      "基于词向量的动态表征：elmo、GPT、bert\n",
      "对比如下：\n",
      "One-hot表示：维度灾难、语义鸿沟；\n",
      "矩阵分解（LSA）：利用全局语料特征，但SVD求解计算复杂度大；\n",
      "基于NNLM/RNNLM的词向量：词向量为副产物，存在效率不高等问题；\n",
      "word2vec、fastText：优化效率高，但是基于局部语料；\n",
      "glove：基于全局预料，结合了LSA和word2vec的优点；\n",
      "elmo、GPT、bert：动态特征。</paragraph>\n",
      "Prediction: [Relevant]GPT 是一种基于神经网络的模型，其中一个主要特点是它可以在任何时间进行更新，因此可以在任何\n",
      "Score: 0.6970225760683694\n",
      "\u001b[0m\u001b[1;3;34m7/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.68 ms /    50 runs   (    0.23 ms per token,  4281.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   78922.37 ms /  1013 tokens (   77.91 ms per token,    12.84 tokens per second)\n",
      "llama_print_timings:        eval time =   10233.62 ms /    49 runs   (  208.85 ms per token,     4.79 tokens per second)\n",
      "llama_print_timings:       total time =   91537.28 ms /  1062 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第15页共46页第二十篇：2022年4月携程暑期实习搜索推荐算法岗面试题8道\n",
      "1、你所理解的推荐系统是什么样的？大致流程？所用模型？\n",
      "推荐系统一方面是为了帮助消费者发现对自己有价值的商品，另一方面是帮助生产者把用户可能感兴趣的\n",
      "商品展现给用户，实现生产者和消费者的双赢。\n",
      "大致流程主要包括：获取用户特征，召回过程，排序过程（粗排、精排）\n",
      "召回模型：\n",
      "·规则召回（兴趣标签top，热门top，新品top等）\n",
      "·协同召回（基于用户的协同过滤，基于商品的协同过滤）\n",
      "·向量召回（FM召回，Item2vec,YoutubeDNN向量召回，GraphEmbedding召回，DSSM双塔召\n",
      "回）\n",
      "排序模型：GBDT+LR、Xgboost、FM/FFM、Wide&Deep、DeepFM、Deep&Cross、DIN、BST\n",
      "等\n",
      "2、双塔模型优势，缺点，如何改进？\n",
      "双塔模型的优势是速度快，但模型精度还有待提升。\n",
      "速度快是因为将所有Item转化成Embedding，并存储进ANN检索系统，比如FAISS，以供查询。类\n",
      "似FAISS这种ANN检索系统对海量数据的查询效率高。\n",
      "而双塔模型为了速度快，必须对用户和Item进行特征分离，而特征分离，又必然导致上述两个原因产生\n",
      "的效果损失。\n",
      "改进：SENet双塔模型，把SENet放在Embedding层之上，目的是通过SENet网络，动态地学习这些\n",
      "特征的重要性：对于每个特征学会一个特征权重，然后再把学习到的权重乘到对应特征的Embedding里，这\n",
      "样就可以动态学习特征权重，通过小权重抑制噪音或者无效低频特征，通过大权重放大重要特征影响的目的。\n",
      "3、粗排的目的是什么？\n",
      "粗排是用来帮精排模型找到那些它本来就要打高分的item，只不过范围更广一些。.按照上面的例子，如\n",
      "果没有粗排，精排模型自己找出来的某top10的item。而粗排的任务就是要找到包含这10个item的一个更\n",
      "小的候选集，既保证了效果，又减少线上预测的负担。</paragraph>\n",
      "Prediction: [Relevant]粗排模型主要是基于用户的协同过滤，基于商品的协同过滤，以及基于用户和商品的\n",
      "Score: 0.7860490517079531\n",
      "\u001b[0m\u001b[1;3;34m8/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       8.25 ms /    36 runs   (    0.23 ms per token,  4366.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   61339.77 ms /   800 tokens (   76.67 ms per token,    13.04 tokens per second)\n",
      "llama_print_timings:        eval time =    6873.96 ms /    35 runs   (  196.40 ms per token,     5.09 tokens per second)\n",
      "llama_print_timings:       total time =   70119.39 ms /   835 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第22页共46页形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权\n",
      "重值比较高，那么这个特征最后对结果的影响会比较大。\n",
      "模型效果不错。在工程上是可以接受的（作为baseline），如果特征工程做的好，效果不会太差，并且\n",
      "特征工程可以并行开发，大大加快开发的速度。\n",
      "训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。\n",
      "缺点：\n",
      "准确率欠佳。因为形式非常的简单，而现实中的数据非常复杂，因此，很难达到很高的准确性。很难处理\n",
      "数据不平衡的问题。\n",
      "5、生成式模型和判别式模型的区别并举一些例子\n",
      "生成模型：学习得到联合概率分布P(x,y)，即特征x，共同出现的概率。\n",
      "常见的生成模型：朴素贝叶斯模型，混合高斯模型，HMM模型。\n",
      "判别模型：学习得到条件概率分布P(y|x)，即在特征x出现的情况下标记y出现的概率。\n",
      "常见的判别模型：感知机，决策树，逻辑回归，SVM，CRF等。\n",
      "判别式模型：要确定一个羊是山羊还是绵羊，用判别式模型的方法是从历史数据中学习到模型，然后通过\n",
      "提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。\n",
      "生成式模型：是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模\n",
      "型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，再放到绵羊模型中看概率是多少，哪个大就是\n",
      "哪个。</paragraph>\n",
      "Prediction: [Relevant]Bert 是一种语言模型，其中包含了一个语言模型和一个位置模型。[Partially supported][Utility:5]\n",
      "Score: 1.4693892065910208\n",
      "\u001b[0m\u001b[1;3;34m9/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.20 ms /    50 runs   (    0.22 ms per token,  4462.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   74968.58 ms /   978 tokens (   76.65 ms per token,    13.05 tokens per second)\n",
      "llama_print_timings:        eval time =    9890.43 ms /    49 runs   (  201.85 ms per token,     4.95 tokens per second)\n",
      "llama_print_timings:       total time =   87065.56 ms /  1027 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "GPT 与 Bert 的区别\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第36页共46页第二十七篇：京东广告NLP实习面试题7道\n",
      "1、BeamSearch生成的句子基本都一样，是否有方法扩展生成句子的多样性\n",
      "解决方法：通过分组加入相似性惩罚，具体可以参考论文Diversebeamsearch：\n",
      "https://arxiv.org/pdf/1610.02424.pdf。\n",
      "具体方法：选择Beamsize为B，然后将其分为G组，每一组就有B/G个beam，每个单独的组内跟\n",
      "beamsearch很像，不断延展序列，同时引入一个dissimilarity项来保证组与组之间有差异。\n",
      "组内与beamsearch很像：从t-1到t时刻，不断的减少搜索空间（如同beamsearch一样）。\n",
      "组间差异：对于t=4时刻，我们先对第一组输出y（t=4），然后我们开始对第二组输出y（t=4），但\n",
      "是第二组y（t=4）的score不仅取决于第二组之前的y（t=3），也取决于其与第一组的相似程度。以此类\n",
      "推，在t=4时刻对于第三组的输出，我们从上图可以看到其score的打分标准。这儿对于其dissimilarity项\n",
      "的计算采用的办法是hammingdiversity，这个理解起来很简单，比如这个时刻可能输出的词在上面的组出\n",
      "现过，我们就对这个词的分数-1，如果这个时刻可能输出的词在上面组没有出现过，我们就对这个词的分数不\n",
      "惩罚。\n",
      "2、LayerNormalization和BatchNormalization的区别，padding对这两者有影响吗，\n",
      "对哪一维有影响\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样\n",
      "本的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "padding会对BatchNormalization的seq_len这个维度有影响，计算的时候会把padding也算进去。\n",
      "3、pytorch.Dataloader报错出现数据维度不一样怎么解决。\n",
      "在构建dataset重写的__getitem__方法中要返回相同长度的tensor。\n",
      "可以使用向量补全的方法来解决这个问题，把不同长度的向量补全成等长的。</paragraph>\n",
      "Prediction: [Relevant]Bert 是一种语言模型，其中包含了一个位于输入序列之后的句子检测器（sentence boundary detector），这个模\n",
      "Score: 0.7345150908417325\n",
      "\u001b[0m\u001b[1;3;34m10/10 paragraphs done\n",
      "\n",
      "\u001b[0m\u001b[1;3;34mEnd evaluation\n",
      "\u001b[0m\u001b[1;3;34mSelected the best answer: [Relevant]Bert 是一种基于 Transformer 模型的语言模型，其中 GPT 是一种基于 Recurrent Neural Networks (RNN) 的语言模型。[Partially supported][Utility:5]\n",
      "\u001b[0m\u001b[1;3;32mFinal answer: Bert 是一种基于 Transformer 模型的语言模型，其中 GPT 是一种基于 Recurrent Neural Networks (RNN) 的语言模型。\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.90 ms /    50 runs   (    0.24 ms per token,  4202.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2222.18 ms /    28 tokens (   79.36 ms per token,    12.60 tokens per second)\n",
      "llama_print_timings:        eval time =    8659.21 ms /    49 runs   (  176.72 ms per token,     5.66 tokens per second)\n",
      "llama_print_timings:       total time =   11073.73 ms /    77 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieval required\n",
      "\u001b[0m\u001b[1;3;34mReceived: 10 documents\n",
      "\u001b[0m\u001b[1;3;34mStart evaluation\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.57 ms /    50 runs   (    0.23 ms per token,  4322.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =   64918.38 ms /   864 tokens (   75.14 ms per token,    13.31 tokens per second)\n",
      "llama_print_timings:        eval time =    9557.72 ms /    49 runs   (  195.06 ms per token,     5.13 tokens per second)\n",
      "llama_print_timings:       total time =   76319.60 ms /   913 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第18页共46页第二十一篇：2022年4月14日美团计算机视觉算法暑期实习面试题6道\n",
      "1、简单介绍gbdt算法的原理\n",
      "GBDT是梯度提升决策树，是一种基于Boosting的算法，采用以决策树为基学习器的加法模型，通过不\n",
      "断拟合上一个弱学习器的残差，最终实现分类或回归的模型。关键在于利用损失函数的负梯度在当前模型的值\n",
      "作为残差的近似值，从而拟合一个回归树。对于分类问题：常使用指数损失函数；对于回归问题：常使用平方\n",
      "误差损失函数（此时，其负梯度就是通常意义的残差），对于一般损失函数来说就是残差的近似。\n",
      "无论损失函数是什么形式，每个决策树拟合的都是负梯度。准确的说，不是用负梯度代替残差，而是当损\n",
      "失函数是均方损失时，负梯度刚好是残差，残差只是特例。\n",
      "2、pca属于有监督还是无监督\n",
      "PCA按有监督和无监督划分应该属于无监督学习，所以数据集有无y并不重要，只是改变样本X的属性\n",
      "(特征)维度。\n",
      "3、介绍svm算法\n",
      "是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感\n",
      "知机。\n",
      "SVM可分为三种：\n",
      "线性可分SVM\n",
      "当训练数据线性可分时，通过最大化硬间隔（hardmargin）可以学习得到一个线性分类器，即硬间隔\n",
      "SVM。\n",
      "线性SVM\n",
      "当训练数据不能线性可分但是近似线性可分时，通过最大化软间隔（softmargin）也可以学习到一个线\n",
      "性分类器，即软间隔SVM。\n",
      "非线性SVM\n",
      "当训练数据线性不可分时，通过使用核技巧（kerneltrick）和最大化软间隔，可以学习到一个非线性\n",
      "SVM。</paragraph>\n",
      "Prediction: [Irrelevant]PCA的基本思想是通过对数据集中的特征向量进行变换，使其规范化，从而对数据集中的样本进行\n",
      "Score: 0.45433171584768933\n",
      "\u001b[0m\u001b[1;3;34m1/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.34 ms /    50 runs   (    0.23 ms per token,  4411.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =   56254.25 ms /   740 tokens (   76.02 ms per token,    13.15 tokens per second)\n",
      "llama_print_timings:        eval time =    9504.96 ms /    49 runs   (  193.98 ms per token,     5.16 tokens per second)\n",
      "llama_print_timings:       total time =   67349.50 ms /   789 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第4页共46页第十七篇：2022年4月10日百度机器学习方向暑期实习面试题6道\n",
      "1、介绍下SVM算法\n",
      "是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感\n",
      "知机。\n",
      "SVM可分为三种：\n",
      "线性可分SVM\n",
      "当训练数据线性可分时，通过最大化硬间隔（hardmargin）可以学习得到一个线性分类器，即硬间隔\n",
      "SVM。\n",
      "线性SVM\n",
      "当训练数据不能线性可分但是近似线性可分时，通过最大化软间隔（softmargin）也可以学习到一个线\n",
      "性分类器，即软间隔SVM。\n",
      "非线性SVM\n",
      "当训练数据线性不可分时，通过使用核技巧（kerneltrick）和最大化软间隔，可以学习到一个非线性\n",
      "SVM。\n",
      "SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损\n",
      "失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。\n",
      "SVM如何选择核函数\n",
      "Linear核：主要用于线性可分的情形。参数少，速度快，对于一般数据，分类效果已经很理想了。\n",
      "RBF核：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交\n",
      "叉验证来寻找合适的参数，不过这个过程比较耗时。\n",
      "2、介绍下逻辑回归算法\n",
      "逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度下降法来求解参数，从而达\n",
      "到将数据二分类的目的。\n",
      "逻辑回归的优缺点</paragraph>\n",
      "Prediction: [Relevant]PCA (Principal Component Analysis) is a technique for dimensionality reduction and data visualization.[No support / Contradictory][Continue to Use Evidence]It is a supervised learning algorithm that seeks to find a lower-dimensional representation of a given dataset, by identifying the principal\n",
      "Score: 0.9902063263956218\n",
      "\u001b[0m\u001b[1;3;34m2/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.20 ms /    50 runs   (    0.22 ms per token,  4462.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   88706.87 ms /  1149 tokens (   77.20 ms per token,    12.95 tokens per second)\n",
      "llama_print_timings:        eval time =   10114.33 ms /    49 runs   (  206.41 ms per token,     4.84 tokens per second)\n",
      "llama_print_timings:       total time =  101563.15 ms /  1198 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第1页共46页目录\n",
      "第十七篇：2022年4月10日百度机器学习方向暑期实习面试题6道.4\n",
      "1、介绍下SVM算法.4\n",
      "2、介绍下逻辑回归算法.4\n",
      "3、介绍下决策树算法.5\n",
      "4、用通俗的语言介绍下强化学习（ReinforcementLearning）.5\n",
      "5、Leetcode34在排序数组中查找元素的第一个和最后一个位置.5\n",
      "6、Letcode102层序遍历.6\n",
      "第十八篇：2022年4月18日字节跳动机器学习AILab一面面试题6道.8\n",
      "1、BN和LN区别.8\n",
      "2、讲讲selfattention.8\n",
      "3、Bert的预训练过程.8\n",
      "4、PreNorm与PostNorm的区别？8\n",
      "5、GPT与Bert的区别.9\n",
      "6、如何加速Bert模型的训练.9\n",
      "第十九篇：2022年4月8日字节跳动机抖音APP推荐实习面试题8道.11\n",
      "1、AUC是什么？如何计算AUC？11\n",
      "2、AUC线上线下不一致怎么办.11\n",
      "3、召回阶段的负采样是怎么做的？12\n",
      "4、FM，DeepFM跟FFM的对比.12\n",
      "5、手撕FM的训练过程.12\n",
      "6、Leetcode—64.最小路径和.13\n",
      "7、剑指Offer10-I.斐波那契数列.14\n",
      "8、Leetcode—215.数组中的第K个最大元素.14\n",
      "第二十篇：2022年4月携程暑期实习搜索推荐算法岗面试题8道.15\n",
      "1、你所理解的推荐系统是什么样的？大致流程？所用模型？15\n",
      "2、双塔模型优势，缺点，如何改进？15\n",
      "3、粗排的目的是什么？15\n",
      "4、wide&deep模型为什么要有wide层结构，优缺点，如何改进？16\n",
      "5、推荐领域GBDT+LR的做法了解吗？16\n",
      "6、粗排有哪些指标？NDCG了解吗？16\n",
      "7、ROC，PR曲线含义，坐标轴代表什么？17\n",
      "8、AUC怎么求，实际意义？17\n",
      "第二十一篇：2022年4月14日美团计算机视觉算法暑期实习面试题6道.18\n",
      "1、简单介绍gbdt算法的原理.18\n",
      "2、pca属于有监督还是无监督.18\n",
      "3、介绍svm算法.18\n",
      "4、介绍transformer算法.19\n",
      "5、layernorm和batchnorm的比较.19\n",
      "6、Leetcode—两数之和.19\n",
      "第二十二篇：2022年5月18日2023届广联达提前批面试题5道.21\n",
      "1、Transformer中encoder和decoder的区别（结构和功能上）.21\n",
      "2、有哪些防止过拟合的方法.21\n",
      "3、L1和L2正则化为什么可以防止过拟合.21\n",
      "4、传统机器学习方法了解多少.21</paragraph>\n",
      "Prediction: [Relevant]2、哪些卷积神经网络中的卷积层可以防止过拟合.21\n",
      "5、哪\n",
      "Score: 0.5551161221012553\n",
      "\u001b[0m\u001b[1;3;34m3/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       7.27 ms /    31 runs   (    0.23 ms per token,  4266.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =   32805.50 ms /   436 tokens (   75.24 ms per token,    13.29 tokens per second)\n",
      "llama_print_timings:        eval time =    5791.85 ms /    30 runs   (  193.06 ms per token,     5.18 tokens per second)\n",
      "llama_print_timings:       total time =   39625.70 ms /   466 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第17页共46页7、ROC，PR曲线含义，坐标轴代表什么？\n",
      "ROC曲线以真正例率TPR为纵轴，以假正例率FPR为横轴，在不同的阈值下获得坐标点，并连接各个坐\n",
      "标点，得到ROC曲线。\n",
      "PR曲线中的P代表的是Precision（精准率），R代表的是Recall（召回率），其代表的是精准率与召\n",
      "回率的关系，一般情况下，Precision设置为纵坐标，将Recall设置为横坐标。\n",
      "8、AUC怎么求，实际意义？\n",
      "AUC：随机取一个正样本和一个负样本，正样本的预测值大于负样本预测值的概率。\n",
      "AUC计算的关键是找到所有正样本预测值大于负样本预测值的正负样本对。\n",
      "首先，需要将样本按照预测值进行从小到大排序（最小score对应的sample的rank为1，第二小\n",
      "score对应sample的rank为2，以此类推）；\n",
      "其次，把所有的正类样本的rank相加，再减去两个正样本组合的情况。</paragraph>\n",
      "Prediction: [Relevant]PCA 的概念和算法基本上都是无监督的。[No support / Contradictory][Utility:5]\n",
      "Score: 1.3116679501535715\n",
      "\u001b[0m\u001b[1;3;34m4/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.30 ms /    50 runs   (    0.23 ms per token,  4424.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   57690.18 ms /   757 tokens (   76.21 ms per token,    13.12 tokens per second)\n",
      "llama_print_timings:        eval time =    9546.87 ms /    49 runs   (  194.83 ms per token,     5.13 tokens per second)\n",
      "llama_print_timings:       total time =   68877.98 ms /   806 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第21页共46页第二十二篇：2022年5月18日2023届广联达提前批面试题5道\n",
      "1、Transformer中encoder和decoder的区别（结构和功能上）\n",
      "结构上：Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个\n",
      "Block，Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder\n",
      "端的Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一\n",
      "个前馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和Layer\n",
      "Normalization层。\n",
      "功能上：Transformer中encoder是双向的，是编码器；decoder是单向的，是解码器，可以用于生成\n",
      "任务。\n",
      "2、有哪些防止过拟合的方法\n",
      "数据的角度：获取和使用更多的数据（数据集增强）；\n",
      "模型角度：降低模型复杂度、L1\\L2\\Dropout正则化、Earlystopping（提前终止）\n",
      "模型融合的角度：使用bagging等模型融合方法。\n",
      "3、L1和L2正则化为什么可以防止过拟合\n",
      "正规化是防止过拟合的一种重要技巧。正则化通过降低模型的复杂性，达到避免过拟合的问题。这里的\n",
      "降低模型的复杂性可以理解为：\n",
      "L1将很多权重变成0，这样起到作用的因素就会减少。\n",
      "L2使权重都趋于0，这样就不会有某个权重占比特别大。\n",
      "4、传统机器学习方法了解多少\n",
      "这个可以说逻辑回归模型，逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度\n",
      "下降法来求解参数，从而达到将数据二分类的目的。\n",
      "逻辑回归的优缺点\n",
      "优点：</paragraph>\n",
      "Prediction: [Relevant]逻辑回归模型的优点是它很容易实现，很快地可以得出结论，因此\n",
      "Score: 0.5602655761931419\n",
      "\u001b[0m\u001b[1;3;34m5/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      13.76 ms /    40 runs   (    0.34 ms per token,  2907.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   67604.30 ms /   879 tokens (   76.91 ms per token,    13.00 tokens per second)\n",
      "llama_print_timings:        eval time =    7614.41 ms /    39 runs   (  195.24 ms per token,     5.12 tokens per second)\n",
      "llama_print_timings:       total time =   77101.55 ms /   918 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第43页共46页好处是这样做大大节约了参数量。\n",
      "5、如何防止过拟合\n",
      "数据的角度：获取和使用更多的数据（数据集增强）；\n",
      "模型角度：降低模型复杂度、L1\\L2\\Dropout正则化、Earlystopping（提前终止）\n",
      "模型融合的角度：使用bagging等模型融合方法。\n",
      "6、BN在训练和测试的时候的区别？可以防止过拟合吗？\n",
      "对于BN，在训练时，是对每一批的训练数据进行归一化，也即用每一批数据的均值和方差。\n",
      "而在测试时，比如进行一个样本的预测，就并没有batch的概念，因此，这个时候用的均值和方差是全\n",
      "量训练数据的均值和方差，这个可以通过移动平均法求得。\n",
      "对于BN，当一个模型训练完成之后，它的所有参数都确定了，包括均值和方差，gamma和bata。\n",
      "BN算法防止过拟合：在网络的训练中，BN的使用使得一个minibatch中所有样本都被关联在了一起，\n",
      "因此网络不会从某一个训练样本中生成确定的结果，即同样一个样本的输出不再仅仅取决于样本的本身，也取\n",
      "决于跟这个样本同属一个batch的其他样本，而每次网络都是随机取batch，这样就会使得整个网络不会朝\n",
      "这一个方向使劲学习。一定程度上避免了过拟合。\n",
      "7、什么是AUC？\n",
      "AUC：AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。AUC反映的是分\n",
      "类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。\n",
      "8、卷积核计算公式\n",
      "卷积层计算公式如下：\n",
      "其中，W为输入大小，K为卷积核大小，P为padding大小，S为步幅。\n",
      "如果，想保持卷积前后的特征图大小相同，通常会设定padding为：</paragraph>\n",
      "Prediction: [Relevant]W为输入大小，K为卷积核大小，P为padding大小，S为步幅。[Fully supported][Utility:5]\n",
      "Score: 1.940682781989481\n",
      "\u001b[0m\u001b[1;3;34m6/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       2.34 ms /    10 runs   (    0.23 ms per token,  4273.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =   70947.42 ms /   920 tokens (   77.12 ms per token,    12.97 tokens per second)\n",
      "llama_print_timings:        eval time =    1798.89 ms /     9 runs   (  199.88 ms per token,     5.00 tokens per second)\n",
      "llama_print_timings:       total time =   74658.89 ms /   929 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第11页共46页第十九篇：2022年4月8日字节跳动机抖音APP推荐实习面试题8道\n",
      "1、AUC是什么？如何计算AUC？\n",
      "AUC：随机取一个正样本和一个负样本，正样本的预测值大于负样本预测值的概率。\n",
      "AUC计算的关键是找到所有正样本预测值大于负样本预测值的正负样本对。\n",
      "首先，需要将样本按照预测值进行从小到大排序（最小score对应的sample的rank为1，第二小\n",
      "score对应sample的rank为2，以此类推）；\n",
      "其次，把所有的正类样本的rank相加，再减去两个正样本组合的情况。\n",
      "2、AUC线上线下不一致怎么办\n",
      "线上线下效果不一致，大概率是由线上线下预估环境不一致引起。预估环境，一般涉及2个要素：模型\n",
      "和特征。\n",
      "模型是否一致\n",
      "主要包括校验离线模型格式转换、serving部署，线上模型加载、预估等接口是否有问题\n",
      "·特征是否一致\n",
      "准确是指，线上线下喂给模型的特征是否一致。\n",
      "与模型一致性检验一样，首先需要校验线上线下特征处理逻辑是否一致等；\n",
      "其次，与线上真实预估环境相比，离线环境更容易获取到特征，当离线使用线上获取不到的特征时，就会\n",
      "造成离线效果虚高的假象。\n",
      "严重点的，如特征穿越，即特征中包含标签信息，会造成训练和评估时数据泄露，导致离线评估时AUC\n",
      "虚高；轻一点的，如离线使用的特征比线上实时性高，同样会导致线上效果不符合预期。\n",
      "这就要求离线阶段构造样本时，需要参考线上真实预估环境获取特征时的延迟，通过严格控制离线特征拼\n",
      "接时的咬合时间，保证线下线下喂给模型的特征的一致性。\n",
      "更好地做法是，落地线上特征日志，直接用于离线训练。\n",
      "AUC指标不能有效刻画模型表现。可以尝试GAUC代替。</paragraph>\n",
      "Prediction: [Irrelevant]无监督[Utility:5]\n",
      "Score: 0.7301676245897185\n",
      "\u001b[0m\u001b[1;3;34m7/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       2.74 ms /    12 runs   (    0.23 ms per token,  4381.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   54067.57 ms /   704 tokens (   76.80 ms per token,    13.02 tokens per second)\n",
      "llama_print_timings:        eval time =    2110.82 ms /    11 runs   (  191.89 ms per token,     5.21 tokens per second)\n",
      "llama_print_timings:       total time =   57550.67 ms /   715 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第42页共46页第二十九篇：2022年6月24日字节电商CV实习岗面试题10道\n",
      "1、如何解决类别极度不平衡的问题？\n",
      "2、说下Transformer模型\n",
      "Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个Block，\n",
      "Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder端的\n",
      "Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一个前\n",
      "馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和LayerNormalization\n",
      "层。\n",
      "3、说下FocalLoss\n",
      "Focalloss是目标检测中解决正负样本严重不平衡的方法，在标准交叉熵损失基础上修改得到的。这个函\n",
      "数可以通过减少易分类样本的权重，使得模型在训练时更专注于稀疏的难分类的样本；防止大量易分类负样本\n",
      "在loss中占主导地位。\n",
      "4、介绍下深度可分离卷积和传统卷积的区别\n",
      "传统的卷积是各个通道上采用相不同的卷积核，然后不同的卷积核用于提取不同方面的特征。\n",
      "深度可分离卷积先在各个通道上采用不同的卷积核提取不同的特征，但是这样对于某个通道来说，就只提\n",
      "取了一方面的特征，因此在此基础上加入点卷积，用1*1的卷积对提取特征后的特征图再次提取不同方面的\n",
      "特征，最终产生和普通卷积相同的输出特征图。</paragraph>\n",
      "Prediction: [Irrelevant]无监督模型[Utility:5]\n",
      "Score: 0.7283484868777735\n",
      "\u001b[0m\u001b[1;3;34m8/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       1.07 ms /     5 runs   (    0.21 ms per token,  4651.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   79272.31 ms /  1039 tokens (   76.30 ms per token,    13.11 tokens per second)\n",
      "llama_print_timings:        eval time =     815.67 ms /     4 runs   (  203.92 ms per token,     4.90 tokens per second)\n",
      "llama_print_timings:       total time =   82094.84 ms /  1043 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第27页共46页第二十四篇：2022年5月10日美团搜索推荐算法面试题10道\n",
      "1、介绍下推荐系统的流程\n",
      "推荐系统的流程主要包含一下几个阶段。\n",
      "索引&特征:会根据内容特性提前建立若干种类型的索引。\n",
      "召回阶段:用户请求时会从各种索引种取出千/万条item。\n",
      "粗排阶段:针对这上千/万条item，进行第一遍打分，再筛选出几百条或者千条。这个阶段的排序模型一\n",
      "般都比较简单，能够过滤掉一些与用户兴趣明显不相关的。\n",
      "精排阶段:得到几百条item后，精排阶段会建立相对精细的模型，根据用户的画像，偏好，上下文，结\n",
      "合业务目标进行排序。一般精排后返回50-100条给到engine侧。\n",
      "重排阶段:engine侧拿到精排的50条item。还会做很多的人工干预和产品逻辑，比如item之间的多样\n",
      "性，产品策略逻辑，比如热门，置顶，多种内容之间的位置混合等等。最终会返回5-10条左右的item，曝\n",
      "光给客户端。根据业务特性，在线流程还有许多比较细的模块，比如去重服务，避免给用户推荐重复的内容。\n",
      "特征预处理，特征抽取等模块。\n",
      "2、召回和排序的差异？\n",
      "召回的目的在于减少候选的数量（尽量控制在1000以内），方便后续排序环节使用复杂模型精准排序；\n",
      "因为在短时间内评估海量候选，所以召回的关键点是个快字，受限与此与排序相比，召回的算法模型相对简\n",
      "单，使用的特征比较少。而排序模型相对更加复杂，更追求准确性，使用的特征也会较多。\n",
      "3、结果f1提升的1%怎么保证有效性，如何保证置信呢？\n",
      "实验过程中固定随机种子、多次实验取平均\n",
      "4、固定随机种子后，多次实验结果相同吗？\n",
      "还是会有细微差别，因为在梯度传播过程，梯度（浮点数）精度有差异，随着神经网络层数的增加，梯度\n",
      "差异会从小数后面的位置往前跑。只能设置浮点数精度增加来缓解这个问题。</paragraph>\n",
      "Prediction: [Relevant]​[No support / Contradictory][Utility:5]\n",
      "Score: 0.980478530338437\n",
      "\u001b[0m\u001b[1;3;34m9/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.50 ms /    50 runs   (    0.23 ms per token,  4347.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =   60757.97 ms /   799 tokens (   76.04 ms per token,    13.15 tokens per second)\n",
      "llama_print_timings:        eval time =    9748.64 ms /    49 runs   (  198.95 ms per token,     5.03 tokens per second)\n",
      "llama_print_timings:       total time =   72266.55 ms /   848 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "pca 属于有监督还是无监督\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第22页共46页形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权\n",
      "重值比较高，那么这个特征最后对结果的影响会比较大。\n",
      "模型效果不错。在工程上是可以接受的（作为baseline），如果特征工程做的好，效果不会太差，并且\n",
      "特征工程可以并行开发，大大加快开发的速度。\n",
      "训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。\n",
      "缺点：\n",
      "准确率欠佳。因为形式非常的简单，而现实中的数据非常复杂，因此，很难达到很高的准确性。很难处理\n",
      "数据不平衡的问题。\n",
      "5、生成式模型和判别式模型的区别并举一些例子\n",
      "生成模型：学习得到联合概率分布P(x,y)，即特征x，共同出现的概率。\n",
      "常见的生成模型：朴素贝叶斯模型，混合高斯模型，HMM模型。\n",
      "判别模型：学习得到条件概率分布P(y|x)，即在特征x出现的情况下标记y出现的概率。\n",
      "常见的判别模型：感知机，决策树，逻辑回归，SVM，CRF等。\n",
      "判别式模型：要确定一个羊是山羊还是绵羊，用判别式模型的方法是从历史数据中学习到模型，然后通过\n",
      "提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。\n",
      "生成式模型：是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模\n",
      "型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，再放到绵羊模型中看概率是多少，哪个大就是\n",
      "哪个。</paragraph>\n",
      "Prediction: [Relevant]PCA属于生成式模型，即学习得到联合概率分布P(x,y)，即特征x和标记y的\n",
      "Score: 0.5920095905613563\n",
      "\u001b[0m\u001b[1;3;34m10/10 paragraphs done\n",
      "\n",
      "\u001b[0m\u001b[1;3;34mEnd evaluation\n",
      "\u001b[0m\u001b[1;3;34mSelected the best answer: [Relevant]W为输入大小，K为卷积核大小，P为padding大小，S为步幅。[Fully supported][Utility:5]\n",
      "\u001b[0m\u001b[1;3;32mFinal answer: W为输入大小，K为卷积核大小，P为padding大小，S为步幅。\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       1.32 ms /     6 runs   (    0.22 ms per token,  4559.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1290.52 ms /    16 tokens (   80.66 ms per token,    12.40 tokens per second)\n",
      "llama_print_timings:        eval time =     848.36 ms /     5 runs   (  169.67 ms per token,     5.89 tokens per second)\n",
      "llama_print_timings:       total time =    2185.72 ms /    21 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieval required\n",
      "\u001b[0m\u001b[1;3;34mReceived: 10 documents\n",
      "\u001b[0m\u001b[1;3;34mStart evaluation\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.86 ms /    50 runs   (    0.24 ms per token,  4216.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   57871.21 ms /   760 tokens (   76.15 ms per token,    13.13 tokens per second)\n",
      "llama_print_timings:        eval time =    9681.51 ms /    49 runs   (  197.58 ms per token,     5.06 tokens per second)\n",
      "llama_print_timings:       total time =   69169.96 ms /   809 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第21页共46页第二十二篇：2022年5月18日2023届广联达提前批面试题5道\n",
      "1、Transformer中encoder和decoder的区别（结构和功能上）\n",
      "结构上：Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个\n",
      "Block，Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder\n",
      "端的Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一\n",
      "个前馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和Layer\n",
      "Normalization层。\n",
      "功能上：Transformer中encoder是双向的，是编码器；decoder是单向的，是解码器，可以用于生成\n",
      "任务。\n",
      "2、有哪些防止过拟合的方法\n",
      "数据的角度：获取和使用更多的数据（数据集增强）；\n",
      "模型角度：降低模型复杂度、L1\\L2\\Dropout正则化、Earlystopping（提前终止）\n",
      "模型融合的角度：使用bagging等模型融合方法。\n",
      "3、L1和L2正则化为什么可以防止过拟合\n",
      "正规化是防止过拟合的一种重要技巧。正则化通过降低模型的复杂性，达到避免过拟合的问题。这里的\n",
      "降低模型的复杂性可以理解为：\n",
      "L1将很多权重变成0，这样起到作用的因素就会减少。\n",
      "L2使权重都趋于0，这样就不会有某个权重占比特别大。\n",
      "4、传统机器学习方法了解多少\n",
      "这个可以说逻辑回归模型，逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度\n",
      "下降法来求解参数，从而达到将数据二分类的目的。\n",
      "逻辑回归的优缺点\n",
      "优点：</paragraph>\n",
      "Prediction: [Relevant]逻辑回归模型的优点是它可以很好地处理二分类问题，因为它的参数只有两个，\n",
      "Score: 0.7015084317750376\n",
      "\u001b[0m\u001b[1;3;34m1/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       9.77 ms /    41 runs   (    0.24 ms per token,  4198.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   54112.46 ms /   704 tokens (   76.86 ms per token,    13.01 tokens per second)\n",
      "llama_print_timings:        eval time =    7797.66 ms /    40 runs   (  194.94 ms per token,     5.13 tokens per second)\n",
      "llama_print_timings:       total time =   63426.76 ms /   744 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第42页共46页第二十九篇：2022年6月24日字节电商CV实习岗面试题10道\n",
      "1、如何解决类别极度不平衡的问题？\n",
      "2、说下Transformer模型\n",
      "Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个Block，\n",
      "Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder端的\n",
      "Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一个前\n",
      "馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和LayerNormalization\n",
      "层。\n",
      "3、说下FocalLoss\n",
      "Focalloss是目标检测中解决正负样本严重不平衡的方法，在标准交叉熵损失基础上修改得到的。这个函\n",
      "数可以通过减少易分类样本的权重，使得模型在训练时更专注于稀疏的难分类的样本；防止大量易分类负样本\n",
      "在loss中占主导地位。\n",
      "4、介绍下深度可分离卷积和传统卷积的区别\n",
      "传统的卷积是各个通道上采用相不同的卷积核，然后不同的卷积核用于提取不同方面的特征。\n",
      "深度可分离卷积先在各个通道上采用不同的卷积核提取不同的特征，但是这样对于某个通道来说，就只提\n",
      "取了一方面的特征，因此在此基础上加入点卷积，用1*1的卷积对提取特征后的特征图再次提取不同方面的\n",
      "特征，最终产生和普通卷积相同的输出特征图。</paragraph>\n",
      "Prediction: [Relevant]Transformer 是一种深度学习模型，主要用于机器翻译和自然语言处理任务。[Fully supported][Utility:5]\n",
      "Score: 2.0576465028671422\n",
      "\u001b[0m\u001b[1;3;34m2/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.06 ms /    50 runs   (    0.24 ms per token,  4146.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =   79518.85 ms /  1039 tokens (   76.53 ms per token,    13.07 tokens per second)\n",
      "llama_print_timings:        eval time =   10065.27 ms /    49 runs   (  205.41 ms per token,     4.87 tokens per second)\n",
      "llama_print_timings:       total time =   91772.68 ms /  1088 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第27页共46页第二十四篇：2022年5月10日美团搜索推荐算法面试题10道\n",
      "1、介绍下推荐系统的流程\n",
      "推荐系统的流程主要包含一下几个阶段。\n",
      "索引&特征:会根据内容特性提前建立若干种类型的索引。\n",
      "召回阶段:用户请求时会从各种索引种取出千/万条item。\n",
      "粗排阶段:针对这上千/万条item，进行第一遍打分，再筛选出几百条或者千条。这个阶段的排序模型一\n",
      "般都比较简单，能够过滤掉一些与用户兴趣明显不相关的。\n",
      "精排阶段:得到几百条item后，精排阶段会建立相对精细的模型，根据用户的画像，偏好，上下文，结\n",
      "合业务目标进行排序。一般精排后返回50-100条给到engine侧。\n",
      "重排阶段:engine侧拿到精排的50条item。还会做很多的人工干预和产品逻辑，比如item之间的多样\n",
      "性，产品策略逻辑，比如热门，置顶，多种内容之间的位置混合等等。最终会返回5-10条左右的item，曝\n",
      "光给客户端。根据业务特性，在线流程还有许多比较细的模块，比如去重服务，避免给用户推荐重复的内容。\n",
      "特征预处理，特征抽取等模块。\n",
      "2、召回和排序的差异？\n",
      "召回的目的在于减少候选的数量（尽量控制在1000以内），方便后续排序环节使用复杂模型精准排序；\n",
      "因为在短时间内评估海量候选，所以召回的关键点是个快字，受限与此与排序相比，召回的算法模型相对简\n",
      "单，使用的特征比较少。而排序模型相对更加复杂，更追求准确性，使用的特征也会较多。\n",
      "3、结果f1提升的1%怎么保证有效性，如何保证置信呢？\n",
      "实验过程中固定随机种子、多次实验取平均\n",
      "4、固定随机种子后，多次实验结果相同吗？\n",
      "还是会有细微差别，因为在梯度传播过程，梯度（浮点数）精度有差异，随着神经网络层数的增加，梯度\n",
      "差异会从小数后面的位置往前跑。只能设置浮点数精度增加来缓解这个问题。</paragraph>\n",
      "Prediction: [Relevant]因此，使用固定随机种子后，多次实验结果可能会相同，但是因为浮点数的不确定性，实\n",
      "Score: 0.6843746630167201\n",
      "\u001b[0m\u001b[1;3;34m3/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.67 ms /    50 runs   (    0.23 ms per token,  4286.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   65426.78 ms /   861 tokens (   75.99 ms per token,    13.16 tokens per second)\n",
      "llama_print_timings:        eval time =    9691.30 ms /    49 runs   (  197.78 ms per token,     5.06 tokens per second)\n",
      "llama_print_timings:       total time =   77189.42 ms /   910 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第18页共46页第二十一篇：2022年4月14日美团计算机视觉算法暑期实习面试题6道\n",
      "1、简单介绍gbdt算法的原理\n",
      "GBDT是梯度提升决策树，是一种基于Boosting的算法，采用以决策树为基学习器的加法模型，通过不\n",
      "断拟合上一个弱学习器的残差，最终实现分类或回归的模型。关键在于利用损失函数的负梯度在当前模型的值\n",
      "作为残差的近似值，从而拟合一个回归树。对于分类问题：常使用指数损失函数；对于回归问题：常使用平方\n",
      "误差损失函数（此时，其负梯度就是通常意义的残差），对于一般损失函数来说就是残差的近似。\n",
      "无论损失函数是什么形式，每个决策树拟合的都是负梯度。准确的说，不是用负梯度代替残差，而是当损\n",
      "失函数是均方损失时，负梯度刚好是残差，残差只是特例。\n",
      "2、pca属于有监督还是无监督\n",
      "PCA按有监督和无监督划分应该属于无监督学习，所以数据集有无y并不重要，只是改变样本X的属性\n",
      "(特征)维度。\n",
      "3、介绍svm算法\n",
      "是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感\n",
      "知机。\n",
      "SVM可分为三种：\n",
      "线性可分SVM\n",
      "当训练数据线性可分时，通过最大化硬间隔（hardmargin）可以学习得到一个线性分类器，即硬间隔\n",
      "SVM。\n",
      "线性SVM\n",
      "当训练数据不能线性可分但是近似线性可分时，通过最大化软间隔（softmargin）也可以学习到一个线\n",
      "性分类器，即软间隔SVM。\n",
      "非线性SVM\n",
      "当训练数据线性不可分时，通过使用核技巧（kerneltrick）和最大化软间隔，可以学习到一个非线性\n",
      "SVM。</paragraph>\n",
      "Prediction: [Relevant]这里介绍了Transformer算法的基本概念和原理，以及如何使用Transformer算法进行语言模型的建模和预测。\n",
      "Score: 0.8059199448300844\n",
      "\u001b[0m\u001b[1;3;34m4/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.31 ms /    52 runs   (    0.24 ms per token,  4225.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   78044.06 ms /  1012 tokens (   77.12 ms per token,    12.97 tokens per second)\n",
      "llama_print_timings:        eval time =   10261.64 ms /    51 runs   (  201.21 ms per token,     4.97 tokens per second)\n",
      "llama_print_timings:       total time =   90539.52 ms /  1063 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第15页共46页第二十篇：2022年4月携程暑期实习搜索推荐算法岗面试题8道\n",
      "1、你所理解的推荐系统是什么样的？大致流程？所用模型？\n",
      "推荐系统一方面是为了帮助消费者发现对自己有价值的商品，另一方面是帮助生产者把用户可能感兴趣的\n",
      "商品展现给用户，实现生产者和消费者的双赢。\n",
      "大致流程主要包括：获取用户特征，召回过程，排序过程（粗排、精排）\n",
      "召回模型：\n",
      "·规则召回（兴趣标签top，热门top，新品top等）\n",
      "·协同召回（基于用户的协同过滤，基于商品的协同过滤）\n",
      "·向量召回（FM召回，Item2vec,YoutubeDNN向量召回，GraphEmbedding召回，DSSM双塔召\n",
      "回）\n",
      "排序模型：GBDT+LR、Xgboost、FM/FFM、Wide&Deep、DeepFM、Deep&Cross、DIN、BST\n",
      "等\n",
      "2、双塔模型优势，缺点，如何改进？\n",
      "双塔模型的优势是速度快，但模型精度还有待提升。\n",
      "速度快是因为将所有Item转化成Embedding，并存储进ANN检索系统，比如FAISS，以供查询。类\n",
      "似FAISS这种ANN检索系统对海量数据的查询效率高。\n",
      "而双塔模型为了速度快，必须对用户和Item进行特征分离，而特征分离，又必然导致上述两个原因产生\n",
      "的效果损失。\n",
      "改进：SENet双塔模型，把SENet放在Embedding层之上，目的是通过SENet网络，动态地学习这些\n",
      "特征的重要性：对于每个特征学会一个特征权重，然后再把学习到的权重乘到对应特征的Embedding里，这\n",
      "样就可以动态学习特征权重，通过小权重抑制噪音或者无效低频特征，通过大权重放大重要特征影响的目的。\n",
      "3、粗排的目的是什么？\n",
      "粗排是用来帮精排模型找到那些它本来就要打高分的item，只不过范围更广一些。.按照上面的例子，如\n",
      "果没有粗排，精排模型自己找出来的某top10的item。而粗排的任务就是要找到包含这10个item的一个更\n",
      "小的候选集，既保证了效果，又减少线上预测的负担。</paragraph>\n",
      "Prediction: [Relevant]粗排模型的目的是帮助精排模型尽量减少负载，减少线上预测的负\n",
      "Score: 0.8329282868390154\n",
      "\u001b[0m\u001b[1;3;34m5/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =       1.06 ms /     4 runs   (    0.26 ms per token,  3784.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =   65398.68 ms /   837 tokens (   78.13 ms per token,    12.80 tokens per second)\n",
      "llama_print_timings:        eval time =     598.27 ms /     3 runs   (  199.42 ms per token,     5.01 tokens per second)\n",
      "llama_print_timings:       total time =   67718.74 ms /   840 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第19页共46页SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损\n",
      "失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。\n",
      "SVM如何选择核函数\n",
      "Linear核：主要用于线性可分的情形。参数少，速度快，对于一般数据，分类效果已经很理想了。\n",
      "RBF核：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交\n",
      "叉验证来寻找合适的参数，不过这个过程比较耗时。\n",
      "4、介绍transformer算法\n",
      "Transformer本身是一个典型的encoder-decoder模型，Encoder端和Decoder端均有6个Block，\n",
      "Encoder端的Block包括两个模块，多头self-attention模块以及一个前馈神经网络模块；Decoder端的\n",
      "Block包括三个模块，多头self-attention模块，多头Encoder-Decoderattention交互模块，以及一个前\n",
      "馈神经网络模块；需要注意：Encoder端和Decoder端中的每个模块都有残差层和LayerNormalization\n",
      "层。\n",
      "5、layernorm和batchnorm的比较\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "6、Leetcode—两数之和\n",
      "该题较为简单。\n",
      "1.classSolution(object):\n",
      "2. deftwoSum(self,nums,target):\n",
      "3. \"\"\"\n",
      "4. :typenums:List[int]\n",
      "5. :typetarget:int\n",
      "6. :rtype:List[int]\n",
      "7. \"\"\"\n",
      "8. dic={}\n",
      "9. fori,numinenumerate(nums):\n",
      "10. iftarget-numindic:</paragraph>\n",
      "Prediction: [Irrelevant]​[Utility:5]\n",
      "Score: 0.7745346473996275\n",
      "\u001b[0m\u001b[1;3;34m6/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      12.08 ms /    50 runs   (    0.24 ms per token,  4137.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =   57953.99 ms /   740 tokens (   78.32 ms per token,    12.77 tokens per second)\n",
      "llama_print_timings:        eval time =    9731.99 ms /    49 runs   (  198.61 ms per token,     5.03 tokens per second)\n",
      "llama_print_timings:       total time =   69378.99 ms /   789 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第4页共46页第十七篇：2022年4月10日百度机器学习方向暑期实习面试题6道\n",
      "1、介绍下SVM算法\n",
      "是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感\n",
      "知机。\n",
      "SVM可分为三种：\n",
      "线性可分SVM\n",
      "当训练数据线性可分时，通过最大化硬间隔（hardmargin）可以学习得到一个线性分类器，即硬间隔\n",
      "SVM。\n",
      "线性SVM\n",
      "当训练数据不能线性可分但是近似线性可分时，通过最大化软间隔（softmargin）也可以学习到一个线\n",
      "性分类器，即软间隔SVM。\n",
      "非线性SVM\n",
      "当训练数据线性不可分时，通过使用核技巧（kerneltrick）和最大化软间隔，可以学习到一个非线性\n",
      "SVM。\n",
      "SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损\n",
      "失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。\n",
      "SVM如何选择核函数\n",
      "Linear核：主要用于线性可分的情形。参数少，速度快，对于一般数据，分类效果已经很理想了。\n",
      "RBF核：主要用于线性不可分的情形。参数多，分类结果非常依赖于参数。有很多人是通过训练数据的交\n",
      "叉验证来寻找合适的参数，不过这个过程比较耗时。\n",
      "2、介绍下逻辑回归算法\n",
      "逻辑回归是在数据服从伯努利分布的假设下，通过极大似然的方法，运用梯度下降法来求解参数，从而达\n",
      "到将数据二分类的目的。\n",
      "逻辑回归的优缺点</paragraph>\n",
      "Prediction: [Relevant]逻辑回归是一种常见的分类模型，它假设数据服从伯努利分布，通过极\n",
      "Score: 0.9323531103241683\n",
      "\u001b[0m\u001b[1;3;34m7/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      11.67 ms /    50 runs   (    0.23 ms per token,  4284.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   66934.58 ms /   865 tokens (   77.38 ms per token,    12.92 tokens per second)\n",
      "llama_print_timings:        eval time =    9799.45 ms /    49 runs   (  199.99 ms per token,     5.00 tokens per second)\n",
      "llama_print_timings:       total time =   78569.83 ms /   914 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第39页共46页第二十八篇：京东科技NLP实习面试题10道\n",
      "1、Bert里面为什么用layernormalization，而不用batchnormalization，分别讲一下这\n",
      "两个啥意思。\n",
      "BatchNormalization是对这批样本的同一维度特征做归一化，LayerNormalization是对这单个样本\n",
      "的所有维度特征做归一化。\n",
      "区别：LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN中则针\n",
      "对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。所以，LN不依赖于\n",
      "batch的大小和输入sequence的长度，因此可以用于batchsize为1和RNN中sequence的normalize操\n",
      "作。\n",
      "2.、Bert里面为什么Q，K，V要用三个不同的矩阵，用一个不是也行吗\n",
      "如果使用相同的矩阵，相同量级的情况下，q和k进行点积的值会是最大的，进行softmax的加权平均\n",
      "后，该词所占的比重会最大，使得其他词的比重很少，无法有效利用上下文信息来增强当前词的语义表示，而\n",
      "使用不同的QKV后，会很大程度减轻上述的影响。\n",
      "3、Bert和transformer讲一下\n",
      "1bert只有transformer的encode结构，是生成语言模型\n",
      "2bert加入了输入句子的mask机制，在输入的时候会随机mask\n",
      "3模型接收两个句子作为输入，并且预测其中第二个句子是否在原始文档中也是后续句子可以做对话机\n",
      "制的应答。\n",
      "4在训练BERT模型时，MaskedLM和NextSentencePrediction是一起训练的，目标就是要最小化\n",
      "两种策略的组合损失函数。\n",
      "4、AUC指标讲一下\n",
      "AUC：AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。AUC反映的是分\n",
      "类器对样本的排序能力。AUC越大，自然排序能力越好，即分类器将越多的正例排在负例之前。</paragraph>\n",
      "Prediction: [Relevant]AUC是ROC曲线下面的面积，AUC可以解读为从所有正例中随机选取一个样本A，再从所有负\n",
      "Score: 0.8405405122724328\n",
      "\u001b[0m\u001b[1;3;34m8/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      10.06 ms /    23 runs   (    0.44 ms per token,  2286.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =  465203.92 ms /   831 tokens (  559.81 ms per token,     1.79 tokens per second)\n",
      "llama_print_timings:        eval time =    7131.03 ms /    22 runs   (  324.14 ms per token,     3.09 tokens per second)\n",
      "llama_print_timings:       total time =  476835.26 ms /   853 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第16页共46页4、wide&deep模型为什么要有wide层结构，优缺点，如何改进？\n",
      "wide&deep模型中的wide部分可以通过利用交叉特征引入非线性高效的实现记忆能力，但需要依赖人\n",
      "工特征工程。\n",
      "改进：DeepFM在Wide&Deep的基础上进行改进，不需要预训练FM得到隐向量，不需要人工特征工\n",
      "程，能同时学习低阶和高阶的组合特征；FM模块和Deep模块共享FeatureEmbedding部分，可以更快的\n",
      "训练，以及更精确的训练学习。\n",
      "5、推荐领域GBDT+LR的做法了解吗？\n",
      "GBDT+LR由两部分组成，其中GBDT用来对训练集提取特征作为新的训练输入数据，LR作为新训练输\n",
      "入数据的分类器。GBDT+LR的提出意味着特征工程可以完全交由一个独立的模型来完成，模型的输入可以是\n",
      "原始的特征向量，不必在特征工程上投入过多的人工筛选和模型设计的精力，真正实现了端到端的训练。\n",
      "6、粗排有哪些指标？NDCG了解吗？\n",
      "(1)NDCG，排序相似性的指标，看精排的排序结果和粗排有多相似\n",
      "(2)粗排的召回率/重叠率，粗排的topk和精排的topk有多大占比。\n",
      "计算DCG，计算公式如下：\n",
      "其中，rel为这个排序list，结果i的一个等级得分；i是指结果i的当前位置序号；\n",
      "​在搜索引擎中，rel等级得分，是由人工抽样数据，并且根据一定的规则打出来的等级得分。\n",
      "步骤二：\n",
      "计算IDCG（IdealDCG），即完美序的DCG；计算方式也同步骤1，只是排序序列不是由算法得出，而\n",
      "是由人工对序列根据一定的评估准则排出来的最佳序列。\n",
      "​步骤三：根据前面2个步骤的出来的结果，计算NDCG，计算公式如下：</paragraph>\n",
      "Prediction: [Relevant]NDCG = DCG / (DCG + 0.5 * IIDCG)[No support / Contradictory][Utility:5]\n",
      "Score: 1.4900611161061235\n",
      "\u001b[0m\u001b[1;3;34m9/10 paragraphs done\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2904.69 ms\n",
      "llama_print_timings:      sample time =      23.32 ms /    50 runs   (    0.47 ms per token,  2144.45 tokens per second)\n",
      "llama_print_timings: prompt eval time = 1916958.72 ms /   616 tokens ( 3111.95 ms per token,     0.32 tokens per second)\n",
      "llama_print_timings:        eval time =   12368.01 ms /    49 runs   (  252.41 ms per token,     3.96 tokens per second)\n",
      "llama_print_timings:       total time = 1933642.38 ms /   665 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mInput: ### Instruction:\n",
      "介绍 transformer 算法\n",
      "\n",
      "### Response:\n",
      "[Retrieval]<paragraph>第35页共46页beamsearch是介于全局搜索和贪婪搜索之间。使用beamsize参数来限制在每一步保留下来的可能性\n",
      "词的数量。beamsearch是在测试阶段为了获得更好准确性而采取的一种策略，在训练阶段无需使用。\n",
      "相比暴力搜索的全局最优解，降低时间复杂度的方法就是寻找次优解，具体就是把搜索空间中的N减下\n",
      "来，每一步计算完只保留K个(beamsize)最大的取值路径，这样时间复杂度降为O(K*N*T)，K取值一般比\n",
      "N小很多。这样得到的虽然不是最优解，但是在seq2seq模型的推理预测中可以兼顾时间和效果。\n",
      "5、你了解的文本表示方法有哪些\n",
      "基于one-hot、tf-idf、textrank等的bag-of-words；\n",
      "主题模型：LSA（SVD）、pLSA、LDA；\n",
      "基于词向量的固定表征：word2vec、fastText、glove\n",
      "基于词向量的动态表征：elmo、GPT、bert\n",
      "对比如下：\n",
      "One-hot表示：维度灾难、语义鸿沟；\n",
      "矩阵分解（LSA）：利用全局语料特征，但SVD求解计算复杂度大；\n",
      "基于NNLM/RNNLM的词向量：词向量为副产物，存在效率不高等问题；\n",
      "word2vec、fastText：优化效率高，但是基于局部语料；\n",
      "glove：基于全局预料，结合了LSA和word2vec的优点；\n",
      "elmo、GPT、bert：动态特征。</paragraph>\n",
      "Prediction: [Relevant]以上文本表示方法中，基于一-hot表示的方式，每个单词都被看作一个独立的特征，因此对应的特\n",
      "Score: 0.5503470700438311\n",
      "\u001b[0m\u001b[1;3;34m10/10 paragraphs done\n",
      "\n",
      "\u001b[0m\u001b[1;3;34mEnd evaluation\n",
      "\u001b[0m\u001b[1;3;34mSelected the best answer: [Relevant]Transformer 是一种深度学习模型，主要用于机器翻译和自然语言处理任务。[Fully supported][Utility:5]\n",
      "\u001b[0m\u001b[1;3;32mFinal answer: Transformer 是一种深度学习模型，主要用于机器翻译和自然语言处理任务。\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "answers = {'question':[], 'llm_answer': [], 'expected_answer': []}\n",
    "\n",
    "for question in qna_dict.keys():\n",
    "    expected_answer = qna_dict[question]\n",
    "    # result\n",
    "    response = agent_pack.run(question)\n",
    "    answers['question'].append(question)\n",
    "    answers['llm_answer'].append(response.response)\n",
    "    answers['expected_answer'].append(expected_answer)\n",
    "\n",
    "answers_df = pd.DataFrame(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6af5b26c-71c1-490c-b792-1ea3e5a36e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>llm_answer</th>\n",
       "      <th>expected_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>介绍下SVM算法</td>\n",
       "      <td>这里介绍的是线性可分SVM。</td>\n",
       "      <td>是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>用通俗的语言介绍下强化学习</td>\n",
       "      <td>好的</td>\n",
       "      <td>监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BN 和 LN 区别</td>\n",
       "      <td>​</td>\n",
       "      <td>Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>讲讲self attention</td>\n",
       "      <td>自我注意力（Self Attention）是一种神经语言模型中的一种重要概念，用于模型中的自我参与和内部连接。</td>\n",
       "      <td>Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bert 的预训练过程</td>\n",
       "      <td>因此，在实验中，我们需要确保我们的实验数据是一致的，以确保我们的结果是可信的。</td>\n",
       "      <td>Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GPT 与 Bert 的区别</td>\n",
       "      <td>Bert 是一种基于 Transformer 模型的语言模型，其中 GPT 是一种基于 Recurrent Neural Networks (RNN) 的语言模型。</td>\n",
       "      <td>GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pca 属于有监督还是无监督</td>\n",
       "      <td>W为输入大小，K为卷积核大小，P为padding大小，S为步幅。</td>\n",
       "      <td>PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>介绍 transformer 算法</td>\n",
       "      <td>Transformer 是一种深度学习模型，主要用于机器翻译和自然语言处理任务。</td>\n",
       "      <td>Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            question  \\\n",
       "0           介绍下SVM算法   \n",
       "1      用通俗的语言介绍下强化学习   \n",
       "2         BN 和 LN 区别   \n",
       "3   讲讲self attention   \n",
       "4        Bert 的预训练过程   \n",
       "5     GPT 与 Bert 的区别   \n",
       "6     pca 属于有监督还是无监督   \n",
       "7  介绍 transformer 算法   \n",
       "\n",
       "                                                                           llm_answer  \\\n",
       "0                                                                      这里介绍的是线性可分SVM。   \n",
       "1                                                                                  好的   \n",
       "2                                                                                   ​   \n",
       "3                             自我注意力（Self Attention）是一种神经语言模型中的一种重要概念，用于模型中的自我参与和内部连接。   \n",
       "4                                             因此，在实验中，我们需要确保我们的实验数据是一致的，以确保我们的结果是可信的。   \n",
       "5  Bert 是一种基于 Transformer 模型的语言模型，其中 GPT 是一种基于 Recurrent Neural Networks (RNN) 的语言模型。   \n",
       "6                                                    W为输入大小，K为卷积核大小，P为padding大小，S为步幅。   \n",
       "7                                            Transformer 是一种深度学习模型，主要用于机器翻译和自然语言处理任务。   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          expected_answer  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机  \n",
       "1                                                                                                                                                               监督学习的特点是有一个“老师”来“监督”我们，告诉我们正确的结果是什么。在我们在小的时候，会有老师来教我们，本质上监督学习是一种知识的传递，但不能发现新的知识。对于人类整体而言，真正（甚至唯一）的知识来源是实践——也就是强化学习。比如神农尝百草，最早人类并不知道哪些草能治病，但是通\\过尝试，就能学到新的知识。学习与决策者被称为智能体，与智能体交互的部分则称为环境。智能体与环境不断进行交互，具体而言，这一交互的过程可以看做是多个时刻，每一时刻，智能体根据环境的状态，依据一定的策略选择一个动作（这里的策略指的是从环境状态到智能体动作或者动作概率之间的映射），然后环境依据一定的状态转移概率转移到下一个状态，与此同时根据此时状态的好坏反馈给智能体一个奖励。智能体可以根据环境的反馈调整其策略，然后继续在环境中探索，最终学习到一个能够获得最多奖励的最优策略  \n",
       "2                                                                                                                                                                                                                                                                                        Batch Normalization 是对这批样本的同一维度特征做归一化， Layer Normalization 是对这单个样本的所有维度特征做归一化。区别：LN 中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；BN 中则针对不同神经元输入计算均值和方差，同一个 batch 中的输入拥有相同的均值和方差。所以，LN 不依赖于batch 的大小和输入 sequence 的长度，因此可以用于 batchsize 为 1 和 RNN 中 sequence 的 normalize 操作。  \n",
       "3  Self Attention 与传统的 Attention 机制非常的不同：传统的 Attention 是基于 source 端和 target 端的隐变量（hidden state）计算 Attention 的，得到的结果是源端的每个词与目标端每个词之间的依赖关系。但Self Attention 不同，它分别在 source 端和 target 端进行，仅与 source input 或者 target input 自身相关的 Self Attention，捕捉 source 端或 target 端自身的词与词之间的依赖关系；然后再把 source 端的得到的self Attention 加入到 target 端得到的 Attention 中，捕捉 source 端和 target 端词与词之间的依赖关系。因此，self Attention Attention 比传统的 Attention mechanism 效果要好，主要原因之一是，传统的Attention 机制忽略了源端或目标端句子中词与词之间的依赖关系，相对比，self Attention 可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系  \n",
       "4                                                                                                                                                                                                                                                                                                             Bert 的预训练主要包含两个任务，MLM 和 NSP，Masked Language Model 任务可以理解为完形填空，随机 mask 每一个句子中 15%的词，用其上下文来做预测；Next Sentence Prediction 任务选择一些句子对 A 与 B，其中 50%的数据 B 是 A 的下一条句子，剩余 50%的数据 B 是语料库中随机选择的，学习其中的相关性。BERT 预训练阶段实际上是将上述两个任务结合起来，同时进行，然后将所有的 Loss 相加。  \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                               GPT 是单向模型，无法利用上下文信息，只能利用上文；而 BERT 是双向模型。GPT 是基于自回归模型，可以应用在 NLU 和 NLG 两大任务，而原生的 BERT 采用的基于自编码模型，只能完成 NLU 任务，无法直接应用在文本生成上面。  \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              PCA 按有监督和无监督划分应该属于无监督学习，所以数据集有无 y 并不重要，只是改变样本 X 的属性(特征)维度。  \n",
       "7                                                                                                                                                                                                                                                                Transformer 本身是一个典型的 encoder-decoder 模型，Encoder 端和 Decoder 端均有 6 个 Block，Encoder 端的 Block 包括两个模块，多头 self-attention 模块以及一个前馈神经网络模块；Decoder 端的Block 包括三个模块，多头 self-attention 模块，多头 Encoder-Decoder attention 交互模块，以及一个前馈神经网络模块；需要注意：Encoder 端和 Decoder 端中的每个模块都有残差层和 Layer Normalization层。  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(answers_df)\n",
    "answers_df.to_excel('self_rag_pack_results.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645435c7",
   "metadata": {},
   "source": [
    "参考链接：https://docs.llamaindex.ai/en/v0.10.33/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
